{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Network Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step0: Import Package & Hyperparameter Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 清空所有變數\n",
    "%reset -f  \n",
    "# 強制 Python 回收記憶體\n",
    "import gc\n",
    "gc.collect()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Package\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Unified Hyperparameter Configuration\n",
    "class Config:\n",
    "    SEED = 1\n",
    "    NUM_EPOCHS = 3000\n",
    "    BATCH_SIZE = 128\n",
    "    LEARNING_RATE = 0.002  #論文提供\n",
    "    LR_SCHEDULER_GAMMA = 0.99  #論文提供\n",
    "    DECAY_EPOCH = 200\n",
    "    DECAY_RATIO = 0.5\n",
    "    EARLY_STOPPING_PATIENCE = 500\n",
    "    HIDDEN_SIZE = 30\n",
    "    OPERATOR_SIZE = 30\n",
    "\n",
    "\n",
    "# Reproducibility\n",
    "random.seed(Config.SEED)\n",
    "np.random.seed(Config.SEED)\n",
    "torch.manual_seed(Config.SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Material & Number of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "material = \"CH467160_Buck\"\n",
    "down_sample_way = \"linspace_n_init2\"\n",
    "downsample = 128\n",
    "\n",
    "# 訓練情況況\n",
    "plot_interval = 300\n",
    "train_show_sample = 1\n",
    "\n",
    "# 定義保存模型的路徑\n",
    "model_save_dir = f\"./Model/{down_sample_way}/{downsample}/\"\n",
    "os.makedirs(model_save_dir, exist_ok=True)  # 如果路徑不存在，創建路徑\n",
    "model_save_path = os.path.join(model_save_dir,\n",
    "                               f\"{material}_n_init2.pt\")  # 定義模型保存檔名\n",
    "\n",
    "figure_save_base_path = f\"./figure/{down_sample_way}/{downsample}/\"\n",
    "os.makedirs(figure_save_base_path, exist_ok=True)  # 如果路徑不存在，創建路徑\n",
    "\n",
    "# Select device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1: Data processing and data loader generate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Preprocess data into a data loader\n",
    "def get_dataloader(data_B,\n",
    "                   data_F,\n",
    "                   data_T,\n",
    "                   data_H,\n",
    "                   data_N,\n",
    "                   data_Hdc,\n",
    "                   data_Pcv,\n",
    "                   n_init=16):\n",
    "    \"\"\" #*(Date:250105)\n",
    "    Process data and return DataLoader for training, validation, and testing.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_B : np.array\n",
    "        Magnetic flux density data.\n",
    "    data_F : np.array\n",
    "        Frequency data.\n",
    "    data_T : np.array\n",
    "        Temperature data.\n",
    "    data_N : np.array\n",
    "        Turns data.\n",
    "    data_Hdc : np.array\n",
    "        DC Magnetic field strength data.\n",
    "    data_H : np.array\n",
    "        AC Magnetic field strength data.\n",
    "    data_Pcv : np.array\n",
    "        Core loss data.\n",
    "    norm : list\n",
    "        Normalization parameters for the features.\n",
    "    n_init : int\n",
    "        Number of initial data points for magnetization.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        train_loader, valid_loader : DataLoader\n",
    "        Dataloaders for training, validation\n",
    "        norm\n",
    "    \"\"\"\n",
    "\n",
    "    # Data pre-process\n",
    "\n",
    "    # 1. Down-sample to 128 points\n",
    "    #*  Add  Down-sample: 8192 to 128 points (Date:241213)\n",
    "    #*  Add H Down-sample to 128 points (Date:241213)\n",
    "    seq_length = downsample\n",
    "    #range(start, stop, step)\n",
    "    # cols = range(0, 8192, int(8192 / seq_length))\n",
    "    cols = np.linspace(0, 1024, seq_length, dtype=int)\n",
    "    data_B = data_B[:, cols]\n",
    "    data_H = data_H[:, cols]\n",
    "\n",
    "    # 2. Add extra points for initial magnetization calculation\n",
    "    #*(Date:241216) MMINN output似乎是128點\n",
    "    #*(Date:250130) 原始MMINN H有包含n_init\n",
    "    data_length = seq_length + n_init\n",
    "    data_B = np.hstack((data_B[:, -n_init:], data_B))\n",
    "    data_H = np.hstack((data_H[:, -n_init:], data_H))\n",
    "\n",
    "    # 3. Format data into tensors  #*(Date:241216) seq_length=128, data_length=144\n",
    "    B = torch.from_numpy(data_B).view(-1, data_length, 1).float()\n",
    "    H = torch.from_numpy(data_H).view(-1, data_length, 1).float()\n",
    "    F = torch.log10(torch.from_numpy(data_F).view(-1, 1).float())\n",
    "    T = torch.from_numpy(data_T).view(-1, 1).float()\n",
    "    Hdc = torch.from_numpy(data_Hdc).view(-1, 1).float()\n",
    "    N = torch.from_numpy(data_N).view(-1, 1).float()\n",
    "    Pcv = torch.log10(torch.from_numpy(data_Pcv).view(-1, 1).float())\n",
    "\n",
    "    # 原本在6. 因要先計算標準化故移至這\n",
    "    dB = torch.diff(B, dim=1)\n",
    "    dB = torch.cat((dB[:, 0:1], dB), dim=1)\n",
    "    dB_dt = dB * (seq_length * F.view(-1, 1, 1))\n",
    "\n",
    "    # # 4. Compute normalization parameters (均值 & 標準差)**\n",
    "    # norm = [\n",
    "    #     [torch.mean(B).item(), torch.std(B).item()],  # B\n",
    "    #     [torch.mean(H).item(), torch.std(H).item()],  # H\n",
    "    #     [torch.mean(F).item(), torch.std(F).item()],  # F\n",
    "    #     [torch.mean(T).item(), torch.std(T).item()],  # T\n",
    "    #     [torch.mean(Hdc).item(), torch.std(Hdc).item()],  # Hdc #*(250317新加入)\n",
    "    #     [torch.mean(N).item(), torch.std(N).item()],  # N #*(250317新加入)\n",
    "    #     [torch.mean(Pcv).item(), torch.std(Pcv).item()],  # Pv\n",
    "    # ]\n",
    "\n",
    "    #  4. Compute normalization parameters (均值 & 標準差)**\n",
    "    # ! 溫度頻率不變加入微小的 epsilon\n",
    "    norm = [\n",
    "        safe_mean_std(B),  # 0: B\n",
    "        safe_mean_std(H),  # 1: H\n",
    "        safe_mean_std(F),  # 2: F\n",
    "        safe_mean_std(T),  # 3: T\n",
    "        safe_mean_std(dB_dt),  # 4: dB/dt\n",
    "        safe_mean_std(Pcv),  # 5: Pcv\n",
    "        safe_mean_std(Hdc),  # 6: Hdc\n",
    "        safe_mean_std(N)  # 7: N\n",
    "    ]\n",
    "\n",
    "    # 用來做test固定標準化參數的\n",
    "    material_name = f\"{material}\"\n",
    "    print(f'\"{material_name}\": [')\n",
    "    for param in norm:\n",
    "        print(f\"    {param},\")\n",
    "    print(\"]\")\n",
    "\n",
    "    # 5. Data Normalization\n",
    "    in_B = (B - norm[0][0]) / norm[0][1]  # B\n",
    "    out_H = (H - norm[1][0]) / norm[1][1]  # H\n",
    "    in_F = (F - norm[2][0]) / norm[2][1]  # F\n",
    "    in_T = (T - norm[3][0]) / norm[3][1]  # T\n",
    "\n",
    "    in_Pcv = (Pcv - norm[5][0]) / norm[5][1]  # Pcv\n",
    "    in_Hdc = (Hdc - norm[6][0]) / norm[6][1]  # Hdc\n",
    "    in_N = (N - norm[7][0]) / norm[7][1]  # N\n",
    "\n",
    "    # 6. Extra features\n",
    "\n",
    "    in_dB = torch.diff(B, dim=1)\n",
    "    in_dB = torch.cat((in_dB[:, 0:1], in_dB), dim=1)\n",
    "\n",
    "    in_dB_dt = (dB_dt - norm[4][0]) / norm[4][1]\n",
    "\n",
    "    max_B, _ = torch.max(in_B, dim=1)\n",
    "    min_B, _ = torch.min(in_B, dim=1)\n",
    "\n",
    "    s0 = get_operator_init(in_B[:, 0] - in_dB[:, 0], in_dB, max_B, min_B)\n",
    "\n",
    "    # 7. Create dataloader to speed up data processing\n",
    "    full_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.cat((in_B, in_dB, in_dB_dt), dim=2),  # B 部分（144 點）\n",
    "        torch.cat((in_F, in_T, in_Hdc, in_N, in_Pcv), dim=1),  # 輔助變量\n",
    "        s0,  # 初始狀態\n",
    "        out_H)\n",
    "\n",
    "    # Split dataset into train, validation, and test sets (60:20:20)\n",
    "    train_size = int(0.8 * len(full_dataset))\n",
    "    valid_size = len(full_dataset) - train_size\n",
    "\n",
    "    train_dataset, valid_dataset = torch.utils.data.random_split(\n",
    "        full_dataset, [train_size, valid_size],\n",
    "        generator=torch.Generator().manual_seed(Config.SEED))\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                               batch_size=Config.BATCH_SIZE,\n",
    "                                               shuffle=True,\n",
    "                                               num_workers=0,\n",
    "                                               collate_fn=filter_input)\n",
    "\n",
    "    valid_loader = torch.utils.data.DataLoader(valid_dataset,\n",
    "                                               batch_size=Config.BATCH_SIZE,\n",
    "                                               shuffle=False,\n",
    "                                               num_workers=0,\n",
    "                                               collate_fn=filter_input)\n",
    "\n",
    "    return train_loader, valid_loader, norm\n",
    "\n",
    "\n",
    "# %% Predict the operator state at t0\n",
    "def get_operator_init(B1,\n",
    "                      dB,\n",
    "                      Bmax,\n",
    "                      Bmin,\n",
    "                      max_out_H=5,\n",
    "                      operator_size=Config.OPERATOR_SIZE):\n",
    "    \"\"\"Compute the initial state of hysteresis operators\"\"\"\n",
    "    s0 = torch.zeros((dB.shape[0], operator_size))\n",
    "    operator_thre = torch.from_numpy(\n",
    "        np.linspace(max_out_H / operator_size, max_out_H,\n",
    "                    operator_size)).view(1, -1)\n",
    "\n",
    "    for i in range(dB.shape[0]):\n",
    "        for j in range(operator_size):\n",
    "            r = operator_thre[0, j]\n",
    "            if (Bmax[i] >= r) or (Bmin[i] <= -r):\n",
    "                if dB[i, 0] >= 0:\n",
    "                    if B1[i] > Bmin[i] + 2 * r:\n",
    "                        s0[i, j] = r\n",
    "                    else:\n",
    "                        s0[i, j] = B1[i] - (r + Bmin[i])\n",
    "                else:\n",
    "                    if B1[i] < Bmax[i] - 2 * r:\n",
    "                        s0[i, j] = -r\n",
    "                    else:\n",
    "                        s0[i, j] = B1[i] + (r - Bmax[i])\n",
    "    return s0\n",
    "\n",
    "\n",
    "def filter_input(batch):\n",
    "    inputs, features, s0, target_H = zip(*batch)\n",
    "\n",
    "    # 如果 inputs 是 tuple，先堆疊成張量\n",
    "    inputs = torch.stack(inputs)  # B 的所有輸入部分（144 點）\n",
    "\n",
    "    # 保留 in_B, in_dB, in_dB_dt 作為模型輸入\n",
    "    inputs = inputs[:, :, :3]\n",
    "\n",
    "    # 保留 features（包括 in_F 和 in_T）\n",
    "    features = torch.stack(\n",
    "        features\n",
    "    )[:, :4]  #!(250317)保留 in_F, in_T, in_Hdc, in_N (排除 in_Pcv，in_Pcv要放在最面)\n",
    "\n",
    "    # 保留目標值 H\n",
    "    target_H = torch.stack(\n",
    "        target_H)[:, -downsample:, :]  # ?只取最後 128 點 (改1024看狀況有無變好)\n",
    "\n",
    "    s0 = torch.stack(s0)  # 初始狀態\n",
    "\n",
    "    return inputs, features, s0, target_H\n",
    "\n",
    "\n",
    "# ! 溫度頻率不變加入微小的 epsilon\n",
    "def safe_mean_std(tensor, eps=1e-8):\n",
    "    m_tensor = torch.mean(tensor)  # 還是 Tensor\n",
    "    s_tensor = torch.std(tensor)  # 還是 Tensor\n",
    "\n",
    "    m_val = m_tensor.item()  # 第一次轉成 float\n",
    "    s_val = s_tensor.item()\n",
    "    if s_val < eps:\n",
    "        s_val = 1.0\n",
    "\n",
    "    return [m_val, s_val]  # 直接回傳 float\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2: Define Network Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Magnetization mechansim-determined neural network\n",
    "\"\"\"\n",
    "    Parameters:\n",
    "    - hidden_size: number of eddy current slices (RNN neuron)\n",
    "    - operator_size: number of operators\n",
    "    - input_size: number of inputs (1.B 2.dB 3.dB/dt)\n",
    "# ! - var_size: number of supplenmentary variables (1.F 2.T 3.Hdc 4.N)        \n",
    "    - output_size: number of outputs (1.H)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class MMINet(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            norm,  #*這裡改成從外部傳入 norm(250203)\n",
    "            hidden_size=Config.HIDDEN_SIZE,\n",
    "            operator_size=Config.OPERATOR_SIZE,\n",
    "            input_size=3,\n",
    "            var_size=4,\n",
    "            output_size=1):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.var_size = var_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.operator_size = operator_size\n",
    "        self.norm = norm  #*這裡改成從外部傳入 norm(250203)\n",
    "\n",
    "        self.rnn1 = StopOperatorCell(self.operator_size)\n",
    "        self.dnn1 = nn.Linear(self.operator_size + 4,\n",
    "                              1)  #!250317更新：operator_size + 4\n",
    "        self.rnn2 = EddyCell(\n",
    "            6, self.hidden_size,\n",
    "            output_size)  #!250317更新：4 (F, T, B, dB/dt ) + 2 (Hdc, N)\n",
    "        self.dnn2 = nn.Linear(self.hidden_size, 1)\n",
    "\n",
    "        self.rnn2_hx = None\n",
    "\n",
    "    def forward(self, x, var, s0, n_init=16):\n",
    "        \"\"\"\n",
    "         Parameters: \n",
    "          - x(batch,seq,input_size): Input features (1.B, 2.dB, 3.dB/dt)  \n",
    "# !       - var(batch,var_size): Supplementary inputs (1.F 2.T 3.Hdc 4.N) \n",
    "          - s0(batch,1): Operator inital states\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)  # Batch size\n",
    "        seq_size = x.size(1)  # Ser\n",
    "        self.rnn1_hx = s0\n",
    "\n",
    "        # Initialize DNN2 input (1.B 2.dB/dt)\n",
    "        x2 = torch.cat((x[:, :, 0:1], x[:, :, 2:3]), dim=2)\n",
    "\n",
    "        for t in range(seq_size):\n",
    "            # RNN1 input (dB,state)\n",
    "            self.rnn1_hx = self.rnn1(x[:, t, 1:2], self.rnn1_hx)\n",
    "\n",
    "            # DNN1 input (rnn1_hx,F,T,Hdc,N)\n",
    "            dnn1_in = torch.cat((self.rnn1_hx, var), dim=1)\n",
    "\n",
    "            # H hysteresis prediction\n",
    "            H_hyst_pred = self.dnn1(dnn1_in)\n",
    "\n",
    "            # DNN2 input (B,dB/dt,T,F)\n",
    "            rnn2_in = torch.cat((x2[:, t, :], var), dim=1)\n",
    "\n",
    "            # Initialize second rnn state\n",
    "            if t == 0:\n",
    "                H_eddy_init = x[:, t, 0:1] - H_hyst_pred\n",
    "                buffer = x.new_ones(x.size(0), self.hidden_size)\n",
    "                self.rnn2_hx = Variable(\n",
    "                    (buffer / torch.sum(self.dnn2.weight, dim=1)) *\n",
    "                    H_eddy_init)\n",
    "\n",
    "            #rnn2_in = torch.cat((rnn2_in,H_hyst_pred),dim=1)\n",
    "            self.rnn2_hx = self.rnn2(rnn2_in, self.rnn2_hx)\n",
    "\n",
    "            # H eddy prediction\n",
    "            H_eddy = self.dnn2(self.rnn2_hx)\n",
    "\n",
    "            # H total\n",
    "            H_total = (H_hyst_pred + H_eddy).view(batch_size, 1,\n",
    "                                                  self.output_size)\n",
    "            if t == 0:\n",
    "                output = H_total\n",
    "            else:\n",
    "                output = torch.cat((output, H_total), dim=1)\n",
    "\n",
    "        H = (output[:, n_init:, :])\n",
    "\n",
    "        return H\n",
    "\n",
    "\n",
    "class StopOperatorCell():\n",
    "    def __init__(self, operator_size):\n",
    "        self.operator_thre = torch.from_numpy(\n",
    "            np.linspace(5 / operator_size, 5, operator_size)).view(1, -1)\n",
    "\n",
    "    def sslu(self, X):\n",
    "        a = torch.ones_like(X)\n",
    "        return torch.max(-a, torch.min(a, X))\n",
    "\n",
    "    def __call__(self, dB, state):\n",
    "        r = self.operator_thre.to(dB.device)\n",
    "        output = self.sslu((dB + state) / r) * r\n",
    "        return output.float()\n",
    "\n",
    "\n",
    "class EddyCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size=1):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.x2h = nn.Linear(input_size, hidden_size, bias=False)\n",
    "        self.h2h = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        hidden = self.x2h(x) + self.h2h(hidden)\n",
    "        hidden = torch.sigmoid(hidden)\n",
    "        return hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3: Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def load_dataset(material, base_path=\"./Data/\"):\n",
    "\n",
    "    in_file1 = f\"{base_path}{material}/train/B_Field.csv\"\n",
    "    in_file2 = f\"{base_path}{material}/train/Frequency.csv\"\n",
    "    in_file3 = f\"{base_path}{material}/train/Temperature.csv\"\n",
    "    in_file4 = f\"{base_path}{material}/train/H_Field.csv\"\n",
    "    in_file5 = f\"{base_path}{material}/train/Volumetric_Loss.csv\"\n",
    "    in_file6 = f\"{base_path}{material}/train/Hdc.csv\"  # *250317新增：直流偏置磁場\n",
    "    in_file7 = f\"{base_path}{material}/train/Turns.csv\"  # *250317新增：匝數\n",
    "\n",
    "    data_B = np.genfromtxt(in_file1, delimiter=',')  # N x 1024\n",
    "    data_F = np.genfromtxt(in_file2, delimiter=',')  # N x 1\n",
    "    data_T = np.genfromtxt(in_file3, delimiter=',')  # N x 1\n",
    "    data_H = np.genfromtxt(in_file4, delimiter=',')  # N x 1024  # *250317新增\n",
    "    data_Pcv = np.genfromtxt(in_file5, delimiter=',')  # N x 1\n",
    "    data_Hdc = np.genfromtxt(in_file6, delimiter=',')  # N x 1  # *250317新增\n",
    "    data_N = np.genfromtxt(in_file7, delimiter=',')  # N x 1\n",
    "\n",
    "    return data_B, data_F, data_T, data_H, data_Pcv, data_Hdc, data_N\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def train_model(norm, train_loader, valid_loader):\n",
    "\n",
    "    model = MMINet(norm=norm).to(device)\n",
    "    print(\"Number of parameters: \", count_parameters(model))\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=Config.LEARNING_RATE)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    # **新增 Loss 記錄**\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    fixed_idx = None\n",
    "\n",
    "    for epoch in range(Config.NUM_EPOCHS):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "\n",
    "        for inputs, features, s0, target_H in train_loader:\n",
    "            inputs, features, s0, target_H = inputs.to(device), features.to(\n",
    "                device), s0.to(device), target_H.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs, features, s0)  # 模型的輸出\n",
    "            loss = criterion(outputs, target_H)  # 使用真實的 H(t) 計算損失\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)  # **記錄 Train Loss**\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, features, s0, target_H in valid_loader:\n",
    "                inputs, features, s0, target_H = inputs.to(\n",
    "                    device), features.to(device), s0.to(device), target_H.to(\n",
    "                        device)\n",
    "                outputs = model(inputs, features, s0)\n",
    "                loss = criterion(outputs, target_H)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(valid_loader)\n",
    "        val_losses.append(val_loss)  # **記錄 Validation Loss**\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch+1}, Train Loss: {train_loss:.6f}, Validation Loss: {val_loss:.6f}\"\n",
    "        )\n",
    "\n",
    "        # ======================================================繪製訓練情況======================================================\n",
    "\n",
    "        if (epoch + 1) % plot_interval == 0:\n",
    "\n",
    "            # 第一次產生固定的隨機索引\n",
    "            if fixed_idx is None:\n",
    "                batch_size_fix = 3\n",
    "                fixed_idx = torch.randperm(batch_size_fix)[:train_show_sample]\n",
    "\n",
    "            # # -------------------------設定圖表H(t)比較---------------------------------------\n",
    "\n",
    "            # outputs = [fixed_idx, :downsample,\n",
    "            #  0].detach().cpu().numpy()\n",
    "            # targets_np = target_H[fixed_idx, :downsample,\n",
    "            #                       0].detach().cpu().numpy()\n",
    "\n",
    "            # plt.figure(figsize=(12, 6))\n",
    "\n",
    "            # for i in range(outputs.shape[0]):  # 每一批數據繪製一個圖表\n",
    "            #     plt.plot(outputs[i, :, 0],\n",
    "            #              label=f\"Pred: Sample {i+1}\",\n",
    "            #              linestyle='--',\n",
    "            #              marker='o')\n",
    "            #     plt.plot(targets[i, :, 0],\n",
    "            #              label=f\"Target: Sample {i+1}\",\n",
    "            #              linestyle='-',\n",
    "            #              marker='x')\n",
    "\n",
    "            # # 添加標題和標籤\n",
    "            # plt.title(f\"Compare - Epoch {epoch + 1}\", fontsize=16)\n",
    "            # plt.xlabel(\"Index\", fontsize=14)\n",
    "            # plt.ylabel(\"Value\", fontsize=14)\n",
    "            # plt.legend(loc=\"upper right\", fontsize=12)\n",
    "            # plt.grid(alpha=0.5)\n",
    "\n",
    "            # # 顯示圖表\n",
    "            # plt.show()\n",
    "            # # -------------------------設定圖表H(t)比較 結束---------------------------------------\n",
    "\n",
    "            # # -------------------------設定圖表B-H比較---------------------------------------\n",
    "            # 取對應 sample\n",
    "            outputs_np = outputs[fixed_idx, -downsample:,\n",
    "                                 0].detach().cpu().numpy()\n",
    "            targets_np = target_H[fixed_idx, -downsample:,\n",
    "                                  0].detach().cpu().numpy()\n",
    "            B_seq_np = inputs[fixed_idx, -downsample:,\n",
    "                              0].detach().cpu().numpy()\n",
    "\n",
    "            # 設定圖表\n",
    "            plt.figure()\n",
    "\n",
    "            for i in range(train_show_sample):  # 每一批數據繪製一個圖表\n",
    "                plt.plot(outputs_np[i],\n",
    "                         B_seq_np[i],\n",
    "                         label=f\"Pred: Sample {i+1}\",\n",
    "                         markersize=1)\n",
    "\n",
    "                plt.plot(targets_np[i],\n",
    "                         B_seq_np[i],\n",
    "                         label=f\"Target: Sample {i+1}\",\n",
    "                         alpha=0.5)\n",
    "\n",
    "            # 添加標題和標籤\n",
    "            plt.title(f\"Compare - Epoch {epoch + 1}\")\n",
    "            plt.xlabel(\"Index\")\n",
    "            plt.ylabel(\"Value\")\n",
    "            plt.grid(alpha=0.5)\n",
    "            plt.legend()\n",
    "            figure_save_path1 = os.path.join(\n",
    "                figure_save_base_path,\n",
    "                f\"Compare_Epoch {epoch + 1}.svg\")  # 定義模型保存檔名\n",
    "\n",
    "            plt.savefig(figure_save_path1)\n",
    "            # 顯示圖表\n",
    "            plt.show()\n",
    "            # # -------------------------設定圖表B-H比較 END---------------------------------------\n",
    "        # ======================================================繪製訓練情況  END ======================================================\n",
    "\n",
    "        # ======================================================Early stop======================================================\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), model_save_path)  # 保存最佳模型\n",
    "            print(\n",
    "                f\"Saving model at epoch {epoch+1} with validation loss {val_loss:.6f}...\"\n",
    "            )\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= Config.EARLY_STOPPING_PATIENCE:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "        # ======================================================Early stop======================================================\n",
    "\n",
    "    print(f\"Training complete. Best model saved at {model_save_path}.\")\n",
    "\n",
    "    # ==============================繪製 Train Loss 與 Validation Loss 圖==============================\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(\n",
    "        range(1,\n",
    "              len(train_losses) + 1),\n",
    "        train_losses,\n",
    "        label=\"Train Loss\",\n",
    "    )\n",
    "    plt.plot(range(1,\n",
    "                   len(val_losses) + 1),\n",
    "             val_losses,\n",
    "             label=\"Validation Loss\")\n",
    "\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training & Validation Loss Curve\")\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.5)\n",
    "    figure_save_path2 = os.path.join(figure_save_base_path,\n",
    "                                     \"Training_Loss_Curve.svg\")  # 定義模型保存檔名\n",
    "    plt.savefig(figure_save_path2)\n",
    "    plt.show()\n",
    "    # ==============================繪製 Train Loss 與 Validation Loss 圖 END==============================\n",
    "\n",
    "    # ===================================使用最佳模型來產生驗證結果=============================\n",
    "    model.load_state_dict(torch.load(model_save_path))  # 載入最佳模型\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, features, s0, target_H in valid_loader:\n",
    "            inputs, features, s0, target_H = inputs.to(device), features.to(\n",
    "                device), s0.to(device), target_H.to(device)\n",
    "\n",
    "            outputs = model(inputs, features, s0)  # 使用最佳模型產生預測值\n",
    "            break  # 只使用一批驗證數據進行可視化\n",
    "\n",
    "    # 選取對應資料（index tensor 要先轉 list 才能 index numpy）\n",
    "    outputs_np = outputs[fixed_idx, -downsample:, 0].detach().cpu().numpy()\n",
    "    targets_np = target_H[fixed_idx, -downsample:, 0].detach().cpu().numpy()\n",
    "    B_seq_np = inputs[fixed_idx, -downsample:, 0].detach().cpu().numpy()\n",
    "\n",
    "    # 設定圖表\n",
    "    plt.figure()\n",
    "\n",
    "    for i in range(train_show_sample):  # 每一批數據繪製一個圖表\n",
    "        plt.plot(outputs_np[i], B_seq_np[i], label=f\"Pred: Sample {i+1}\")\n",
    "\n",
    "        plt.plot(targets_np[i],\n",
    "                 B_seq_np[i],\n",
    "                 label=f\"Target: Sample {i+1}\",\n",
    "                 alpha=0.7)\n",
    "\n",
    "    # 添加標題和標籤\n",
    "    plt.title(f\"Best Model - Predicted vs Target\")\n",
    "    plt.xlabel(\"Index\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.grid(alpha=0.5)\n",
    "    plt.legend()\n",
    "    figure_save_path3 = os.path.join(\n",
    "        figure_save_base_path,\n",
    "        \"Best Model_Predicted vs Target.svg\")  # 定義模型保存檔名\n",
    "    plt.savefig(figure_save_path3)\n",
    "\n",
    "    # ===================================使用最佳模型來產生驗證結果 END============================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Train!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1024 is out of bounds for axis 1 with size 1024",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m      3\u001b[0m     data_B, data_F, data_T, data_H, data_Pcv, data_Hdc, data_N \u001b[38;5;241m=\u001b[39m load_dataset(\n\u001b[0;32m      4\u001b[0m         material)\n\u001b[1;32m----> 6\u001b[0m     train_loader, valid_loader, norm \u001b[38;5;241m=\u001b[39m \u001b[43mget_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_B\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_F\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_T\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m                                                      \u001b[49m\u001b[43mdata_H\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_N\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_Hdc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m                                                      \u001b[49m\u001b[43mdata_Pcv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     train_model(norm, train_loader, valid_loader)\n",
      "Cell \u001b[1;32mIn[5], line 50\u001b[0m, in \u001b[0;36mget_dataloader\u001b[1;34m(data_B, data_F, data_T, data_H, data_N, data_Hdc, data_Pcv, n_init)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m#range(start, stop, step)\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# cols = range(0, 8192, int(8192 / seq_length))\u001b[39;00m\n\u001b[0;32m     49\u001b[0m cols \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1024\u001b[39m, seq_length, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m---> 50\u001b[0m data_B \u001b[38;5;241m=\u001b[39m \u001b[43mdata_B\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcols\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     51\u001b[0m data_H \u001b[38;5;241m=\u001b[39m data_H[:, cols]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# 2. Add extra points for initial magnetization calculation\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m#*(Date:241216) MMINN output似乎是128點\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m#*(Date:250130) 原始MMINN H有包含n_init\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 1024 is out of bounds for axis 1 with size 1024"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    data_B, data_F, data_T, data_H, data_Pcv, data_Hdc, data_N = load_dataset(\n",
    "        material)\n",
    "\n",
    "    train_loader, valid_loader, norm = get_dataloader(data_B, data_F, data_T,\n",
    "                                                      data_H, data_N, data_Hdc,\n",
    "                                                      data_Pcv)\n",
    "\n",
    "    train_model(norm, train_loader, valid_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
