{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Network Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step0: Import Package & Hyperparameter Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n2025/02/03: 1.新增normDict計算 #!注意normDict是有先經過資料前處理計算的\\n            2.資料集中任意取n筆去訓練(n:自填數) \\n            \\n2025/03/17: 1.修改增加Hdc,N輸入參數\\n            2.Mdoel修正兩個子迴路接考慮Hdc與N            \\n            \\n'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "2025/02/03: 1.新增normDict計算 #!注意normDict是有先經過資料前處理計算的\n",
    "            2.資料集中任意取n筆去訓練(n:自填數) \n",
    "            \n",
    "2025/03/17: 1.修改增加Hdc,N輸入參數\n",
    "            2.Mdoel修正兩個子迴路接考慮Hdc與N            \n",
    "            \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Don't know how to reset  #, please run `%reset?` for details\n",
      "Don't know how to reset  清空所有變數, please run `%reset?` for details\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reset -f  # 清空所有變數\n",
    "\n",
    "import gc\n",
    "gc.collect()  # 強制 Python 回收記憶體\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Package\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Unified Hyperparameter Configuration\n",
    "class Config:\n",
    "    SEED = 1\n",
    "    NUM_EPOCHS = 3000\n",
    "    BATCH_SIZE = 128\n",
    "    LEARNING_RATE = 0.002  #論文提供\n",
    "    LR_SCHEDULER_GAMMA = 0.99  #論文提供\n",
    "    DECAY_EPOCH = 200\n",
    "    DECAY_RATIO = 0.5\n",
    "    EARLY_STOPPING_PATIENCE = 500\n",
    "    HIDDEN_SIZE = 30\n",
    "    OPERATOR_SIZE = 30\n",
    "\n",
    "\n",
    "# Reproducibility\n",
    "random.seed(Config.SEED)\n",
    "np.random.seed(Config.SEED)\n",
    "torch.manual_seed(Config.SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Material & Number of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "material = \"CH467160\"\n",
    "down_sample_way = \"range_n_init2\"\n",
    "downsample = 128\n",
    "\n",
    "# 訓練情況況\n",
    "plot_interval = 300\n",
    "train_show_sample = 1\n",
    "\n",
    "# 定義保存模型的路徑\n",
    "model_save_dir = f\"./Model/{down_sample_way}/{downsample}/\"\n",
    "os.makedirs(model_save_dir, exist_ok=True)  # 如果路徑不存在，創建路徑\n",
    "model_save_path = os.path.join(model_save_dir,\n",
    "                               f\"{material}_n_init2.pt\")  # 定義模型保存檔名\n",
    "\n",
    "figure_save_base_path = f\"./figure/{down_sample_way}/{downsample}/\"\n",
    "os.makedirs(figure_save_base_path, exist_ok=True)  # 如果路徑不存在，創建路徑\n",
    "\n",
    "# Select device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1: Data processing and data loader generate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Preprocess data into a data loader\n",
    "def get_dataloader(data_B,\n",
    "                   data_F,\n",
    "                   data_T,\n",
    "                   data_H,\n",
    "                   data_N,\n",
    "                   data_Hdc,\n",
    "                   data_Pcv,\n",
    "                   n_init=16):\n",
    "    \"\"\" #*(Date:250105)\n",
    "    Process data and return DataLoader for training, validation, and testing.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_B : np.array\n",
    "        Magnetic flux density data.\n",
    "    data_F : np.array\n",
    "        Frequency data.\n",
    "    data_T : np.array\n",
    "        Temperature data.\n",
    "    data_N : np.array\n",
    "        Turns data.\n",
    "    data_Hdc : np.array\n",
    "        DC Magnetic field strength data.\n",
    "    data_H : np.array\n",
    "        AC Magnetic field strength data.\n",
    "    data_Pcv : np.array\n",
    "        Core loss data.\n",
    "    norm : list\n",
    "        Normalization parameters for the features.\n",
    "    n_init : int\n",
    "        Number of initial data points for magnetization.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        train_loader, valid_loader : DataLoader\n",
    "        Dataloaders for training, validation\n",
    "        norm\n",
    "    \"\"\"\n",
    "\n",
    "    # Data pre-process\n",
    "    # 1. Down-sample to 128 points\n",
    "    seq_length = downsample\n",
    "    cols = range(\n",
    "        0, 8192, int(8192 / seq_length)\n",
    "    )  #range(start, stop, step) #*  Add  Down-sample: 8192 to 128 points (Date:241213)\n",
    "    data_B = data_B[:, cols]\n",
    "    data_H = data_H[:, cols]  #*  Add H Down-sample to 128 points (Date:241213)\n",
    "\n",
    "    # data_B = data_B - np.mean(data_B, axis=1,\n",
    "    #                           keepdims=True)  #*  移除降階影響 (Date:250325)\n",
    "    # data_H = data_H - np.mean(data_H, axis=1, keepdims=True)\n",
    "\n",
    "    # 2. Add extra points for initial magnetization calculation\n",
    "    data_length = seq_length + n_init\n",
    "    data_B = np.hstack((data_B[:, -n_init:], data_B))\n",
    "    data_H = np.hstack((data_H[:, -n_init:], data_H))\n",
    "\n",
    "    print(\"Shape of data_H:\", data_H.shape)\n",
    "    print(\"Shape of data_B:\", data_B.shape)\n",
    "\n",
    "    #*(Date:241216) MMINN output似乎是128點\n",
    "    #*(Date:250130) 原始MMINN H有包含n_init\n",
    "\n",
    "    # 3. Format data into tensors  #*(Date:241216) seq_length=128, data_length=144\n",
    "    B = torch.from_numpy(data_B).view(-1, data_length, 1).float()\n",
    "    H = torch.from_numpy(data_H).view(-1, data_length, 1).float()\n",
    "    F = torch.log10(torch.from_numpy(data_F).view(-1, 1).float())\n",
    "    T = torch.from_numpy(data_T).view(-1, 1).float()\n",
    "    Hdc = torch.from_numpy(data_Hdc).view(-1, 1).float()\n",
    "    N = torch.from_numpy(data_N).view(-1, 1).float()\n",
    "    Pcv = torch.log10(torch.from_numpy(data_Pcv).view(-1, 1).float())\n",
    "\n",
    "    # 原本在6. 因要先計算標準化故移至這\n",
    "    dB = torch.diff(B, dim=1)\n",
    "    dB = torch.cat((dB[:, 0:1], dB), dim=1)\n",
    "    dB_dt = dB * (seq_length * F.view(-1, 1, 1))\n",
    "\n",
    "    # # 4. Compute normalization parameters (均值 & 標準差)**\n",
    "    # norm = [\n",
    "    #     [torch.mean(B).item(), torch.std(B).item()],  # B\n",
    "    #     [torch.mean(H).item(), torch.std(H).item()],  # H\n",
    "    #     [torch.mean(F).item(), torch.std(F).item()],  # F\n",
    "    #     [torch.mean(T).item(), torch.std(T).item()],  # T\n",
    "    #     [torch.mean(Hdc).item(), torch.std(Hdc).item()],  # Hdc #*(250317新加入)\n",
    "    #     [torch.mean(N).item(), torch.std(N).item()],  # N #*(250317新加入)\n",
    "    #     [torch.mean(Pcv).item(), torch.std(Pcv).item()],  # Pv\n",
    "    # ]\n",
    "\n",
    "    #  4. Compute normalization parameters (均值 & 標準差)**\n",
    "    # ! 溫度頻率不變加入微小的 epsilon\n",
    "    norm = [\n",
    "        safe_mean_std(B),  # 0: B\n",
    "        safe_mean_std(H),  # 1: H\n",
    "        safe_mean_std(F),  # 2: F\n",
    "        safe_mean_std(T),  # 3: T\n",
    "        safe_mean_std(dB_dt),  # 4: dB/dt\n",
    "        safe_mean_std(Pcv),  # 5: Pcv\n",
    "        safe_mean_std(Hdc),  # 6: Hdc\n",
    "        safe_mean_std(N)  # 7: N\n",
    "    ]\n",
    "\n",
    "    # 用來做test固定標準化參數的\n",
    "    material_name = f\"{material}\"\n",
    "    print(f'\"{material_name}\": [')\n",
    "    for param in norm:\n",
    "        print(f\"    {param},\")\n",
    "    print(\"]\")\n",
    "\n",
    "    # 5. Data Normalization\n",
    "    in_B = (B - norm[0][0]) / norm[0][1]  # B\n",
    "    out_H = (H - norm[1][0]) / norm[1][1]  # H\n",
    "    in_F = (F - norm[2][0]) / norm[2][1]  # F\n",
    "    in_T = (T - norm[3][0]) / norm[3][1]  # T\n",
    "\n",
    "    in_Pcv = (Pcv - norm[5][0]) / norm[5][1]  # Pcv\n",
    "    in_Hdc = (Hdc - norm[6][0]) / norm[6][1]  # Hdc\n",
    "    in_N = (N - norm[7][0]) / norm[7][1]  # N\n",
    "\n",
    "    # 6. Extra features\n",
    "\n",
    "    in_dB = torch.diff(B, dim=1)\n",
    "    in_dB = torch.cat((in_dB[:, 0:1], in_dB), dim=1)\n",
    "\n",
    "    in_dB_dt = (dB_dt - norm[4][0]) / norm[4][1]\n",
    "\n",
    "    max_B, _ = torch.max(in_B, dim=1)\n",
    "    min_B, _ = torch.min(in_B, dim=1)\n",
    "\n",
    "    s0 = get_operator_init(in_B[:, 0] - in_dB[:, 0], in_dB, max_B, min_B)\n",
    "\n",
    "    # 7. Create dataloader to speed up data processing\n",
    "    full_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.cat((in_B, in_dB, in_dB_dt), dim=2),  # B 部分（144 點）\n",
    "        torch.cat((in_F, in_T, in_Hdc, in_N, in_Pcv), dim=1),  # 輔助變量\n",
    "        s0,  # 初始狀態\n",
    "        out_H)\n",
    "\n",
    "    # Split dataset into train, validation, and test sets (60:20:20)\n",
    "    train_size = int(0.8 * len(full_dataset))\n",
    "    valid_size = len(full_dataset) - train_size\n",
    "\n",
    "    train_dataset, valid_dataset = torch.utils.data.random_split(\n",
    "        full_dataset, [train_size, valid_size],\n",
    "        generator=torch.Generator().manual_seed(Config.SEED))\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                               batch_size=Config.BATCH_SIZE,\n",
    "                                               shuffle=True,\n",
    "                                               num_workers=0,\n",
    "                                               collate_fn=filter_input)\n",
    "\n",
    "    valid_loader = torch.utils.data.DataLoader(valid_dataset,\n",
    "                                               batch_size=Config.BATCH_SIZE,\n",
    "                                               shuffle=False,\n",
    "                                               num_workers=0,\n",
    "                                               collate_fn=filter_input)\n",
    "\n",
    "    return train_loader, valid_loader, norm\n",
    "\n",
    "\n",
    "# %% Predict the operator state at t0\n",
    "def get_operator_init(B1,\n",
    "                      dB,\n",
    "                      Bmax,\n",
    "                      Bmin,\n",
    "                      max_out_H=5,\n",
    "                      operator_size=Config.OPERATOR_SIZE):\n",
    "    \"\"\"Compute the initial state of hysteresis operators\"\"\"\n",
    "    s0 = torch.zeros((dB.shape[0], operator_size))\n",
    "    operator_thre = torch.from_numpy(\n",
    "        np.linspace(max_out_H / operator_size, max_out_H,\n",
    "                    operator_size)).view(1, -1)\n",
    "\n",
    "    for i in range(dB.shape[0]):\n",
    "        for j in range(operator_size):\n",
    "            r = operator_thre[0, j]\n",
    "            if (Bmax[i] >= r) or (Bmin[i] <= -r):\n",
    "                if dB[i, 0] >= 0:\n",
    "                    if B1[i] > Bmin[i] + 2 * r:\n",
    "                        s0[i, j] = r\n",
    "                    else:\n",
    "                        s0[i, j] = B1[i] - (r + Bmin[i])\n",
    "                else:\n",
    "                    if B1[i] < Bmax[i] - 2 * r:\n",
    "                        s0[i, j] = -r\n",
    "                    else:\n",
    "                        s0[i, j] = B1[i] + (r - Bmax[i])\n",
    "    return s0\n",
    "\n",
    "\n",
    "def filter_input(batch):\n",
    "    inputs, features, s0, target_H = zip(*batch)\n",
    "\n",
    "    # 如果 inputs 是 tuple，先堆疊成張量\n",
    "    inputs = torch.stack(inputs)  # B 的所有輸入部分（144 點）\n",
    "\n",
    "    # 保留 in_B, in_dB, in_dB_dt 作為模型輸入\n",
    "    inputs = inputs[:, :, :3]\n",
    "\n",
    "    # 保留 features（包括 in_F 和 in_T）\n",
    "    features = torch.stack(\n",
    "        features\n",
    "    )[:, :4]  #!(250317)保留 in_F, in_T, in_Hdc, in_N (排除 in_Pcv，in_Pcv要放在最面)\n",
    "\n",
    "    # 保留目標值 H\n",
    "    target_H = torch.stack(\n",
    "        target_H)[:, -downsample:, :]  # ?只取最後 128 點 (改1024看狀況有無變好)\n",
    "\n",
    "    s0 = torch.stack(s0)  # 初始狀態\n",
    "\n",
    "    return inputs, features, s0, target_H\n",
    "\n",
    "\n",
    "# ! 溫度頻率不變加入微小的 epsilon\n",
    "def safe_mean_std(tensor, eps=1e-8):\n",
    "    m_tensor = torch.mean(tensor)  # 還是 Tensor\n",
    "    s_tensor = torch.std(tensor)  # 還是 Tensor\n",
    "\n",
    "    m_val = m_tensor.item()  # 第一次轉成 float\n",
    "    s_val = s_tensor.item()\n",
    "    if s_val < eps:\n",
    "        s_val = 1.0\n",
    "\n",
    "    return [m_val, s_val]  # 直接回傳 float\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2: Define Network Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Magnetization mechansim-determined neural network\n",
    "\"\"\"\n",
    "    Parameters:\n",
    "    - hidden_size: number of eddy current slices (RNN neuron)\n",
    "    - operator_size: number of operators\n",
    "    - input_size: number of inputs (1.B 2.dB 3.dB/dt)\n",
    "# ! - var_size: number of supplenmentary variables (1.F 2.T 3.Hdc 4.N)        \n",
    "    - output_size: number of outputs (1.H)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class MMINet(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            norm,  #*這裡改成從外部傳入 norm(250203)\n",
    "            hidden_size=Config.HIDDEN_SIZE,\n",
    "            operator_size=Config.OPERATOR_SIZE,\n",
    "            input_size=3,\n",
    "            var_size=4,\n",
    "            output_size=1):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.var_size = var_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.operator_size = operator_size\n",
    "        self.norm = norm  #*這裡改成從外部傳入 norm(250203)\n",
    "\n",
    "        self.rnn1 = StopOperatorCell(self.operator_size)\n",
    "        self.dnn1 = nn.Linear(self.operator_size + 4,\n",
    "                              1)  #!250317更新：operator_size + 4\n",
    "        self.rnn2 = EddyCell(\n",
    "            6, self.hidden_size,\n",
    "            output_size)  #!250317更新：4 (F, T, B, dB/dt ) + 2 (Hdc, N)\n",
    "        self.dnn2 = nn.Linear(self.hidden_size, 1)\n",
    "\n",
    "        self.rnn2_hx = None\n",
    "\n",
    "    def forward(self, x, var, s0, n_init=16):\n",
    "        \"\"\"\n",
    "         Parameters: \n",
    "          - x(batch,seq,input_size): Input features (1.B, 2.dB, 3.dB/dt)  \n",
    "# !       - var(batch,var_size): Supplementary inputs (1.F 2.T 3.Hdc 4.N) \n",
    "          - s0(batch,1): Operator inital states\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)  # Batch size\n",
    "        seq_size = x.size(1)  # Ser\n",
    "        self.rnn1_hx = s0\n",
    "\n",
    "        # Initialize DNN2 input (1.B 2.dB/dt)\n",
    "        x2 = torch.cat((x[:, :, 0:1], x[:, :, 2:3]), dim=2)\n",
    "\n",
    "        for t in range(seq_size):\n",
    "            # RNN1 input (dB,state)\n",
    "            self.rnn1_hx = self.rnn1(x[:, t, 1:2], self.rnn1_hx)\n",
    "\n",
    "            # DNN1 input (rnn1_hx,F,T,Hdc,N)\n",
    "            dnn1_in = torch.cat((self.rnn1_hx, var), dim=1)\n",
    "\n",
    "            # H hysteresis prediction\n",
    "            H_hyst_pred = self.dnn1(dnn1_in)\n",
    "\n",
    "            # DNN2 input (B,dB/dt,T,F)\n",
    "            rnn2_in = torch.cat((x2[:, t, :], var), dim=1)\n",
    "\n",
    "            # Initialize second rnn state\n",
    "            if t == 0:\n",
    "                H_eddy_init = x[:, t, 0:1] - H_hyst_pred\n",
    "                buffer = x.new_ones(x.size(0), self.hidden_size)\n",
    "                self.rnn2_hx = Variable(\n",
    "                    (buffer / torch.sum(self.dnn2.weight, dim=1)) *\n",
    "                    H_eddy_init)\n",
    "\n",
    "            #rnn2_in = torch.cat((rnn2_in,H_hyst_pred),dim=1)\n",
    "            self.rnn2_hx = self.rnn2(rnn2_in, self.rnn2_hx)\n",
    "\n",
    "            # H eddy prediction\n",
    "            H_eddy = self.dnn2(self.rnn2_hx)\n",
    "\n",
    "            # H total\n",
    "            H_total = (H_hyst_pred + H_eddy).view(batch_size, 1,\n",
    "                                                  self.output_size)\n",
    "            if t == 0:\n",
    "                output = H_total\n",
    "            else:\n",
    "                output = torch.cat((output, H_total), dim=1)\n",
    "\n",
    "        H = (output[:, n_init:, :])\n",
    "\n",
    "        return H\n",
    "\n",
    "\n",
    "class StopOperatorCell():\n",
    "    def __init__(self, operator_size):\n",
    "        self.operator_thre = torch.from_numpy(\n",
    "            np.linspace(5 / operator_size, 5, operator_size)).view(1, -1)\n",
    "\n",
    "    def sslu(self, X):\n",
    "        a = torch.ones_like(X)\n",
    "        return torch.max(-a, torch.min(a, X))\n",
    "\n",
    "    def __call__(self, dB, state):\n",
    "        r = self.operator_thre.to(dB.device)\n",
    "        output = self.sslu((dB + state) / r) * r\n",
    "        return output.float()\n",
    "\n",
    "\n",
    "class EddyCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size=1):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.x2h = nn.Linear(input_size, hidden_size, bias=False)\n",
    "        self.h2h = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        hidden = self.x2h(x) + self.h2h(hidden)\n",
    "        hidden = torch.sigmoid(hidden)\n",
    "        return hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3: Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def load_dataset(material, base_path=\"./Data/\"):\n",
    "\n",
    "    in_file1 = f\"{base_path}{material}/train/B_Field.csv\"\n",
    "    in_file2 = f\"{base_path}{material}/train/Frequency.csv\"\n",
    "    in_file3 = f\"{base_path}{material}/train/Temperature.csv\"\n",
    "    in_file4 = f\"{base_path}{material}/train/H_Field.csv\"\n",
    "    in_file5 = f\"{base_path}{material}/train/Volumetric_Loss.csv\"\n",
    "    in_file6 = f\"{base_path}{material}/train/Hdc.csv\"  # *250317新增：直流偏置磁場\n",
    "    in_file7 = f\"{base_path}{material}/train/Turns.csv\"  # *250317新增：匝數\n",
    "\n",
    "    data_B = np.genfromtxt(in_file1, delimiter=',')  # N x 1024\n",
    "    data_F = np.genfromtxt(in_file2, delimiter=',')  # N x 1\n",
    "    data_T = np.genfromtxt(in_file3, delimiter=',')  # N x 1\n",
    "    data_H = np.genfromtxt(in_file4, delimiter=',')  # N x 1024  # *250317新增\n",
    "    data_Pcv = np.genfromtxt(in_file5, delimiter=',')  # N x 1\n",
    "    data_Hdc = np.genfromtxt(in_file6, delimiter=',')  # N x 1  # *250317新增\n",
    "    data_N = np.genfromtxt(in_file7, delimiter=',')  # N x 1\n",
    "\n",
    "    return data_B, data_F, data_T, data_H, data_Pcv, data_Hdc, data_N\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def train_model(norm, train_loader, valid_loader):\n",
    "\n",
    "    model = MMINet(norm=norm).to(device)\n",
    "    print(\"Number of parameters: \", count_parameters(model))\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=Config.LEARNING_RATE)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    # **新增 Loss 記錄**\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    fixed_idx = None\n",
    "\n",
    "    for epoch in range(Config.NUM_EPOCHS):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "\n",
    "        for inputs, features, s0, target_H in train_loader:\n",
    "            inputs, features, s0, target_H = inputs.to(device), features.to(\n",
    "                device), s0.to(device), target_H.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs, features, s0)  # 模型的輸出\n",
    "            loss = criterion(outputs, target_H)  # 使用真實的 H(t) 計算損失\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)  # **記錄 Train Loss**\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, features, s0, target_H in valid_loader:\n",
    "                inputs, features, s0, target_H = inputs.to(\n",
    "                    device), features.to(device), s0.to(device), target_H.to(\n",
    "                        device)\n",
    "                outputs = model(inputs, features, s0)\n",
    "                loss = criterion(outputs, target_H)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(valid_loader)\n",
    "        val_losses.append(val_loss)  # **記錄 Validation Loss**\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch+1}, Train Loss: {train_loss:.6f}, Validation Loss: {val_loss:.6f}\"\n",
    "        )\n",
    "\n",
    "        # ======================================================繪製訓練情況======================================================\n",
    "\n",
    "        if (epoch + 1) % plot_interval == 0:\n",
    "\n",
    "            # 第一次產生固定的隨機索引\n",
    "            if fixed_idx is None:\n",
    "                batch_size_fix = 3\n",
    "                fixed_idx = torch.randperm(batch_size_fix)[:train_show_sample]\n",
    "\n",
    "            # # -------------------------設定圖表H(t)比較---------------------------------------\n",
    "\n",
    "            # outputs = [fixed_idx, :downsample,\n",
    "            #  0].detach().cpu().numpy()\n",
    "            # targets_np = target_H[fixed_idx, :downsample,\n",
    "            #                       0].detach().cpu().numpy()\n",
    "\n",
    "            # plt.figure(figsize=(12, 6))\n",
    "\n",
    "            # for i in range(outputs.shape[0]):  # 每一批數據繪製一個圖表\n",
    "            #     plt.plot(outputs[i, :, 0],\n",
    "            #              label=f\"Pred: Sample {i+1}\",\n",
    "            #              linestyle='--',\n",
    "            #              marker='o')\n",
    "            #     plt.plot(targets[i, :, 0],\n",
    "            #              label=f\"Target: Sample {i+1}\",\n",
    "            #              linestyle='-',\n",
    "            #              marker='x')\n",
    "\n",
    "            # # 添加標題和標籤\n",
    "            # plt.title(f\"Compare - Epoch {epoch + 1}\", fontsize=16)\n",
    "            # plt.xlabel(\"Index\", fontsize=14)\n",
    "            # plt.ylabel(\"Value\", fontsize=14)\n",
    "            # plt.legend(loc=\"upper right\", fontsize=12)\n",
    "            # plt.grid(alpha=0.5)\n",
    "\n",
    "            # # 顯示圖表\n",
    "            # plt.show()\n",
    "            # # -------------------------設定圖表H(t)比較 結束---------------------------------------\n",
    "\n",
    "            # # -------------------------設定圖表B-H比較---------------------------------------\n",
    "            # 取對應 sample\n",
    "            outputs_np = outputs[fixed_idx, :downsample,\n",
    "                                 0].detach().cpu().numpy()\n",
    "            targets_np = target_H[fixed_idx, :downsample,\n",
    "                                  0].detach().cpu().numpy()\n",
    "            B_seq_np = inputs[fixed_idx, :downsample, 0].detach().cpu().numpy()\n",
    "\n",
    "            # 設定圖表\n",
    "            plt.figure()\n",
    "\n",
    "            for i in range(train_show_sample):  # 每一批數據繪製一個圖表\n",
    "                plt.plot(outputs_np[i],\n",
    "                         B_seq_np[i],\n",
    "                         label=f\"Pred: Sample {i+1}\",\n",
    "                         markersize=1)\n",
    "\n",
    "                plt.plot(targets_np[i],\n",
    "                         B_seq_np[i],\n",
    "                         label=f\"Target: Sample {i+1}\",\n",
    "                         alpha=0.5)\n",
    "\n",
    "            # 添加標題和標籤\n",
    "            plt.title(f\"Compare - Epoch {epoch + 1}\")\n",
    "            plt.xlabel(\"Index\")\n",
    "            plt.ylabel(\"Value\")\n",
    "            plt.grid(alpha=0.5)\n",
    "            plt.legend()\n",
    "            figure_save_path1 = os.path.join(\n",
    "                figure_save_base_path,\n",
    "                f\"Compare_Epoch {epoch + 1}.svg\")  # 定義模型保存檔名\n",
    "            plt.savefig(figure_save_path1)\n",
    "            # 顯示圖表\n",
    "            plt.show()\n",
    "            # # -------------------------設定圖表B-H比較 END---------------------------------------\n",
    "        # ======================================================繪製訓練情況  END ======================================================\n",
    "\n",
    "        # ======================================================Early stop======================================================\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), model_save_path)  # 保存最佳模型\n",
    "            print(\n",
    "                f\"Saving model at epoch {epoch+1} with validation loss {val_loss:.6f}...\"\n",
    "            )\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= Config.EARLY_STOPPING_PATIENCE:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "        # ======================================================Early stop======================================================\n",
    "\n",
    "    print(f\"Training complete. Best model saved at {model_save_path}.\")\n",
    "\n",
    "    # ==============================繪製 Train Loss 與 Validation Loss 圖==============================\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(\n",
    "        range(1,\n",
    "              len(train_losses) + 1),\n",
    "        train_losses,\n",
    "        label=\"Train Loss\",\n",
    "    )\n",
    "    plt.plot(range(1,\n",
    "                   len(val_losses) + 1),\n",
    "             val_losses,\n",
    "             label=\"Validation Loss\")\n",
    "\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training & Validation Loss Curve\")\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.5)\n",
    "    figure_save_path2 = os.path.join(figure_save_base_path,\n",
    "                                     \"Training_Loss_Curve.svg\")  # 定義模型保存檔名\n",
    "    plt.savefig(figure_save_path2)\n",
    "    plt.show()\n",
    "    # ==============================繪製 Train Loss 與 Validation Loss 圖 END==============================\n",
    "\n",
    "    # ===================================使用最佳模型來產生驗證結果=============================\n",
    "    model.load_state_dict(torch.load(model_save_path))  # 載入最佳模型\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, features, s0, target_H in valid_loader:\n",
    "            inputs, features, s0, target_H = inputs.to(device), features.to(\n",
    "                device), s0.to(device), target_H.to(device)\n",
    "\n",
    "            outputs = model(inputs, features, s0)  # 使用最佳模型產生預測值\n",
    "            break  # 只使用一批驗證數據進行可視化\n",
    "\n",
    "    # 選取對應資料（index tensor 要先轉 list 才能 index numpy）\n",
    "    outputs_np = outputs[fixed_idx, :downsample, 0].detach().cpu().numpy()\n",
    "    targets_np = target_H[fixed_idx, :downsample, 0].detach().cpu().numpy()\n",
    "    B_seq_np = inputs[fixed_idx, :downsample, 0].detach().cpu().numpy()\n",
    "\n",
    "    # 設定圖表\n",
    "    plt.figure()\n",
    "\n",
    "    for i in range(train_show_sample):  # 每一批數據繪製一個圖表\n",
    "        plt.plot(outputs_np[i], B_seq_np[i], label=f\"Pred: Sample {i+1}\")\n",
    "\n",
    "        plt.plot(targets_np[i],\n",
    "                 B_seq_np[i],\n",
    "                 label=f\"Target: Sample {i+1}\",\n",
    "                 alpha=0.7)\n",
    "\n",
    "    # 添加標題和標籤\n",
    "    plt.title(f\"Best Model - Predicted vs Target\")\n",
    "    plt.xlabel(\"Index\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.grid(alpha=0.5)\n",
    "    plt.legend()\n",
    "    figure_save_path3 = os.path.join(\n",
    "        figure_save_base_path,\n",
    "        \"Best Model_Predicted vs Target.svg\")  # 定義模型保存檔名\n",
    "    plt.savefig(figure_save_path3)\n",
    "\n",
    "    # ===================================使用最佳模型來產生驗證結果 END============================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Train!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data_H: (1808, 144)\n",
      "Shape of data_B: (1808, 144)\n",
      "\"CH467160\": [\n",
      "    [-0.0015919295838102698, 0.02852831408381462],\n",
      "    [-8.099489212036133, 156.4864501953125],\n",
      "    [2.0, 1.0],\n",
      "    [25.0, 1.0],\n",
      "    [0.04085405170917511, 0.4432935118675232],\n",
      "    [1.8067814111709595, 0.715567409992218],\n",
      "    [1250.265380859375, 701.6740112304688],\n",
      "    [12.159845352172852, 3.296860456466675],\n",
      "]\n",
      "Number of parameters:  1146\n",
      "Epoch 1, Train Loss: 1.050303, Validation Loss: 0.948567\n",
      "Saving model at epoch 1 with validation loss 0.948567...\n",
      "Epoch 2, Train Loss: 0.888567, Validation Loss: 0.791595\n",
      "Saving model at epoch 2 with validation loss 0.791595...\n",
      "Epoch 3, Train Loss: 0.721349, Validation Loss: 0.621273\n",
      "Saving model at epoch 3 with validation loss 0.621273...\n",
      "Epoch 4, Train Loss: 0.540114, Validation Loss: 0.414215\n",
      "Saving model at epoch 4 with validation loss 0.414215...\n",
      "Epoch 5, Train Loss: 0.315960, Validation Loss: 0.182312\n",
      "Saving model at epoch 5 with validation loss 0.182312...\n",
      "Epoch 6, Train Loss: 0.116885, Validation Loss: 0.065055\n",
      "Saving model at epoch 6 with validation loss 0.065055...\n",
      "Epoch 7, Train Loss: 0.072040, Validation Loss: 0.061156\n",
      "Saving model at epoch 7 with validation loss 0.061156...\n",
      "Epoch 8, Train Loss: 0.057729, Validation Loss: 0.045925\n",
      "Saving model at epoch 8 with validation loss 0.045925...\n",
      "Epoch 9, Train Loss: 0.047461, Validation Loss: 0.039298\n",
      "Saving model at epoch 9 with validation loss 0.039298...\n",
      "Epoch 10, Train Loss: 0.039209, Validation Loss: 0.034347\n",
      "Saving model at epoch 10 with validation loss 0.034347...\n",
      "Epoch 11, Train Loss: 0.034820, Validation Loss: 0.030018\n",
      "Saving model at epoch 11 with validation loss 0.030018...\n",
      "Epoch 12, Train Loss: 0.030835, Validation Loss: 0.026398\n",
      "Saving model at epoch 12 with validation loss 0.026398...\n",
      "Epoch 13, Train Loss: 0.027278, Validation Loss: 0.023494\n",
      "Saving model at epoch 13 with validation loss 0.023494...\n",
      "Epoch 14, Train Loss: 0.025501, Validation Loss: 0.020987\n",
      "Saving model at epoch 14 with validation loss 0.020987...\n",
      "Epoch 15, Train Loss: 0.021748, Validation Loss: 0.018794\n",
      "Saving model at epoch 15 with validation loss 0.018794...\n",
      "Epoch 16, Train Loss: 0.019791, Validation Loss: 0.016892\n",
      "Saving model at epoch 16 with validation loss 0.016892...\n",
      "Epoch 17, Train Loss: 0.017887, Validation Loss: 0.015300\n",
      "Saving model at epoch 17 with validation loss 0.015300...\n",
      "Epoch 18, Train Loss: 0.015970, Validation Loss: 0.013882\n",
      "Saving model at epoch 18 with validation loss 0.013882...\n",
      "Epoch 19, Train Loss: 0.014624, Validation Loss: 0.012706\n",
      "Saving model at epoch 19 with validation loss 0.012706...\n",
      "Epoch 20, Train Loss: 0.013342, Validation Loss: 0.011652\n",
      "Saving model at epoch 20 with validation loss 0.011652...\n",
      "Epoch 21, Train Loss: 0.012368, Validation Loss: 0.010882\n",
      "Saving model at epoch 21 with validation loss 0.010882...\n",
      "Epoch 22, Train Loss: 0.011538, Validation Loss: 0.009819\n",
      "Saving model at epoch 22 with validation loss 0.009819...\n",
      "Epoch 23, Train Loss: 0.010411, Validation Loss: 0.009026\n",
      "Saving model at epoch 23 with validation loss 0.009026...\n",
      "Epoch 24, Train Loss: 0.009638, Validation Loss: 0.008344\n",
      "Saving model at epoch 24 with validation loss 0.008344...\n",
      "Epoch 25, Train Loss: 0.008929, Validation Loss: 0.007714\n",
      "Saving model at epoch 25 with validation loss 0.007714...\n",
      "Epoch 26, Train Loss: 0.008346, Validation Loss: 0.007128\n",
      "Saving model at epoch 26 with validation loss 0.007128...\n",
      "Epoch 27, Train Loss: 0.007950, Validation Loss: 0.006638\n",
      "Saving model at epoch 27 with validation loss 0.006638...\n",
      "Epoch 28, Train Loss: 0.007191, Validation Loss: 0.006169\n",
      "Saving model at epoch 28 with validation loss 0.006169...\n",
      "Epoch 29, Train Loss: 0.006807, Validation Loss: 0.005732\n",
      "Saving model at epoch 29 with validation loss 0.005732...\n",
      "Epoch 30, Train Loss: 0.006362, Validation Loss: 0.005327\n",
      "Saving model at epoch 30 with validation loss 0.005327...\n",
      "Epoch 31, Train Loss: 0.005820, Validation Loss: 0.004971\n",
      "Saving model at epoch 31 with validation loss 0.004971...\n",
      "Epoch 32, Train Loss: 0.005414, Validation Loss: 0.004676\n",
      "Saving model at epoch 32 with validation loss 0.004676...\n",
      "Epoch 33, Train Loss: 0.005101, Validation Loss: 0.004314\n",
      "Saving model at epoch 33 with validation loss 0.004314...\n",
      "Epoch 34, Train Loss: 0.004775, Validation Loss: 0.004162\n",
      "Saving model at epoch 34 with validation loss 0.004162...\n",
      "Epoch 35, Train Loss: 0.004423, Validation Loss: 0.003817\n",
      "Saving model at epoch 35 with validation loss 0.003817...\n",
      "Epoch 36, Train Loss: 0.004211, Validation Loss: 0.003514\n",
      "Saving model at epoch 36 with validation loss 0.003514...\n",
      "Epoch 37, Train Loss: 0.003921, Validation Loss: 0.003334\n",
      "Saving model at epoch 37 with validation loss 0.003334...\n",
      "Epoch 38, Train Loss: 0.003730, Validation Loss: 0.003106\n",
      "Saving model at epoch 38 with validation loss 0.003106...\n",
      "Epoch 39, Train Loss: 0.003421, Validation Loss: 0.002901\n",
      "Saving model at epoch 39 with validation loss 0.002901...\n",
      "Epoch 40, Train Loss: 0.003183, Validation Loss: 0.002722\n",
      "Saving model at epoch 40 with validation loss 0.002722...\n",
      "Epoch 41, Train Loss: 0.003020, Validation Loss: 0.002596\n",
      "Saving model at epoch 41 with validation loss 0.002596...\n",
      "Epoch 42, Train Loss: 0.002971, Validation Loss: 0.002421\n",
      "Saving model at epoch 42 with validation loss 0.002421...\n",
      "Epoch 43, Train Loss: 0.002828, Validation Loss: 0.002322\n",
      "Saving model at epoch 43 with validation loss 0.002322...\n",
      "Epoch 44, Train Loss: 0.002558, Validation Loss: 0.002160\n",
      "Saving model at epoch 44 with validation loss 0.002160...\n",
      "Epoch 45, Train Loss: 0.002474, Validation Loss: 0.002057\n",
      "Saving model at epoch 45 with validation loss 0.002057...\n",
      "Epoch 46, Train Loss: 0.002357, Validation Loss: 0.002017\n",
      "Saving model at epoch 46 with validation loss 0.002017...\n",
      "Epoch 47, Train Loss: 0.002249, Validation Loss: 0.001868\n",
      "Saving model at epoch 47 with validation loss 0.001868...\n",
      "Epoch 48, Train Loss: 0.002130, Validation Loss: 0.001806\n",
      "Saving model at epoch 48 with validation loss 0.001806...\n",
      "Epoch 49, Train Loss: 0.002059, Validation Loss: 0.001751\n",
      "Saving model at epoch 49 with validation loss 0.001751...\n",
      "Epoch 50, Train Loss: 0.001966, Validation Loss: 0.001678\n",
      "Saving model at epoch 50 with validation loss 0.001678...\n",
      "Epoch 51, Train Loss: 0.001889, Validation Loss: 0.001595\n",
      "Saving model at epoch 51 with validation loss 0.001595...\n",
      "Epoch 52, Train Loss: 0.001829, Validation Loss: 0.001564\n",
      "Saving model at epoch 52 with validation loss 0.001564...\n",
      "Epoch 53, Train Loss: 0.001786, Validation Loss: 0.001508\n",
      "Saving model at epoch 53 with validation loss 0.001508...\n",
      "Epoch 54, Train Loss: 0.001762, Validation Loss: 0.001517\n",
      "Epoch 55, Train Loss: 0.001696, Validation Loss: 0.001437\n",
      "Saving model at epoch 55 with validation loss 0.001437...\n",
      "Epoch 56, Train Loss: 0.001622, Validation Loss: 0.001453\n",
      "Epoch 57, Train Loss: 0.001644, Validation Loss: 0.001461\n",
      "Epoch 58, Train Loss: 0.001584, Validation Loss: 0.001366\n",
      "Saving model at epoch 58 with validation loss 0.001366...\n",
      "Epoch 59, Train Loss: 0.001540, Validation Loss: 0.001347\n",
      "Saving model at epoch 59 with validation loss 0.001347...\n",
      "Epoch 60, Train Loss: 0.001500, Validation Loss: 0.001339\n",
      "Saving model at epoch 60 with validation loss 0.001339...\n",
      "Epoch 61, Train Loss: 0.001517, Validation Loss: 0.001304\n",
      "Saving model at epoch 61 with validation loss 0.001304...\n",
      "Epoch 62, Train Loss: 0.001557, Validation Loss: 0.001316\n",
      "Epoch 63, Train Loss: 0.001460, Validation Loss: 0.001280\n",
      "Saving model at epoch 63 with validation loss 0.001280...\n",
      "Epoch 64, Train Loss: 0.001442, Validation Loss: 0.001418\n",
      "Epoch 65, Train Loss: 0.001487, Validation Loss: 0.001273\n",
      "Saving model at epoch 65 with validation loss 0.001273...\n",
      "Epoch 66, Train Loss: 0.001456, Validation Loss: 0.001292\n",
      "Epoch 67, Train Loss: 0.001429, Validation Loss: 0.001278\n",
      "Epoch 68, Train Loss: 0.001443, Validation Loss: 0.001222\n",
      "Saving model at epoch 68 with validation loss 0.001222...\n",
      "Epoch 69, Train Loss: 0.001398, Validation Loss: 0.001216\n",
      "Saving model at epoch 69 with validation loss 0.001216...\n",
      "Epoch 70, Train Loss: 0.001378, Validation Loss: 0.001340\n",
      "Epoch 71, Train Loss: 0.001446, Validation Loss: 0.001207\n",
      "Saving model at epoch 71 with validation loss 0.001207...\n",
      "Epoch 72, Train Loss: 0.001388, Validation Loss: 0.001190\n",
      "Saving model at epoch 72 with validation loss 0.001190...\n",
      "Epoch 73, Train Loss: 0.001332, Validation Loss: 0.001177\n",
      "Saving model at epoch 73 with validation loss 0.001177...\n",
      "Epoch 74, Train Loss: 0.001314, Validation Loss: 0.001169\n",
      "Saving model at epoch 74 with validation loss 0.001169...\n",
      "Epoch 75, Train Loss: 0.001300, Validation Loss: 0.001244\n",
      "Epoch 76, Train Loss: 0.001316, Validation Loss: 0.001142\n",
      "Saving model at epoch 76 with validation loss 0.001142...\n",
      "Epoch 77, Train Loss: 0.001269, Validation Loss: 0.001136\n",
      "Saving model at epoch 77 with validation loss 0.001136...\n",
      "Epoch 78, Train Loss: 0.001285, Validation Loss: 0.001135\n",
      "Saving model at epoch 78 with validation loss 0.001135...\n",
      "Epoch 79, Train Loss: 0.001285, Validation Loss: 0.001178\n",
      "Epoch 80, Train Loss: 0.001267, Validation Loss: 0.001115\n",
      "Saving model at epoch 80 with validation loss 0.001115...\n",
      "Epoch 81, Train Loss: 0.001238, Validation Loss: 0.001103\n",
      "Saving model at epoch 81 with validation loss 0.001103...\n",
      "Epoch 82, Train Loss: 0.001222, Validation Loss: 0.001099\n",
      "Saving model at epoch 82 with validation loss 0.001099...\n",
      "Epoch 83, Train Loss: 0.001237, Validation Loss: 0.001089\n",
      "Saving model at epoch 83 with validation loss 0.001089...\n",
      "Epoch 84, Train Loss: 0.001209, Validation Loss: 0.001095\n",
      "Epoch 85, Train Loss: 0.001190, Validation Loss: 0.001114\n",
      "Epoch 86, Train Loss: 0.001210, Validation Loss: 0.001110\n",
      "Epoch 87, Train Loss: 0.001195, Validation Loss: 0.001064\n",
      "Saving model at epoch 87 with validation loss 0.001064...\n",
      "Epoch 88, Train Loss: 0.001175, Validation Loss: 0.001104\n",
      "Epoch 89, Train Loss: 0.001182, Validation Loss: 0.001046\n",
      "Saving model at epoch 89 with validation loss 0.001046...\n",
      "Epoch 90, Train Loss: 0.001183, Validation Loss: 0.001062\n",
      "Epoch 91, Train Loss: 0.001191, Validation Loss: 0.001114\n",
      "Epoch 92, Train Loss: 0.001184, Validation Loss: 0.001099\n",
      "Epoch 93, Train Loss: 0.001236, Validation Loss: 0.001125\n",
      "Epoch 94, Train Loss: 0.001190, Validation Loss: 0.001036\n",
      "Saving model at epoch 94 with validation loss 0.001036...\n",
      "Epoch 95, Train Loss: 0.001158, Validation Loss: 0.001017\n",
      "Saving model at epoch 95 with validation loss 0.001017...\n",
      "Epoch 96, Train Loss: 0.001118, Validation Loss: 0.001025\n",
      "Epoch 97, Train Loss: 0.001129, Validation Loss: 0.001001\n",
      "Saving model at epoch 97 with validation loss 0.001001...\n",
      "Epoch 98, Train Loss: 0.001095, Validation Loss: 0.001048\n",
      "Epoch 99, Train Loss: 0.001147, Validation Loss: 0.001025\n",
      "Epoch 100, Train Loss: 0.001131, Validation Loss: 0.000991\n",
      "Saving model at epoch 100 with validation loss 0.000991...\n",
      "Epoch 101, Train Loss: 0.001143, Validation Loss: 0.000994\n",
      "Epoch 102, Train Loss: 0.001111, Validation Loss: 0.000989\n",
      "Saving model at epoch 102 with validation loss 0.000989...\n",
      "Epoch 103, Train Loss: 0.001119, Validation Loss: 0.001003\n",
      "Epoch 104, Train Loss: 0.001095, Validation Loss: 0.001054\n",
      "Epoch 105, Train Loss: 0.001097, Validation Loss: 0.000995\n",
      "Epoch 106, Train Loss: 0.001071, Validation Loss: 0.000976\n",
      "Saving model at epoch 106 with validation loss 0.000976...\n",
      "Epoch 107, Train Loss: 0.001060, Validation Loss: 0.001001\n",
      "Epoch 108, Train Loss: 0.001103, Validation Loss: 0.001012\n",
      "Epoch 109, Train Loss: 0.001066, Validation Loss: 0.000974\n",
      "Saving model at epoch 109 with validation loss 0.000974...\n",
      "Epoch 110, Train Loss: 0.001070, Validation Loss: 0.000964\n",
      "Saving model at epoch 110 with validation loss 0.000964...\n",
      "Epoch 111, Train Loss: 0.001126, Validation Loss: 0.001006\n",
      "Epoch 112, Train Loss: 0.001136, Validation Loss: 0.001027\n",
      "Epoch 113, Train Loss: 0.001087, Validation Loss: 0.001075\n",
      "Epoch 114, Train Loss: 0.001078, Validation Loss: 0.000967\n",
      "Epoch 115, Train Loss: 0.001058, Validation Loss: 0.000997\n",
      "Epoch 116, Train Loss: 0.001050, Validation Loss: 0.000930\n",
      "Saving model at epoch 116 with validation loss 0.000930...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 10\u001b[0m\n\u001b[0;32m      3\u001b[0m data_B, data_F, data_T, data_H, data_Pcv, data_Hdc, data_N \u001b[38;5;241m=\u001b[39m load_dataset(\n\u001b[0;32m      4\u001b[0m     material)\n\u001b[0;32m      6\u001b[0m train_loader, valid_loader, norm \u001b[38;5;241m=\u001b[39m get_dataloader(data_B, data_F, data_T,\n\u001b[0;32m      7\u001b[0m                                                   data_H, data_N, data_Hdc,\n\u001b[0;32m      8\u001b[0m                                                   data_Pcv)\n\u001b[1;32m---> 10\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[44], line 26\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(norm, train_loader, valid_loader)\u001b[0m\n\u001b[0;32m     22\u001b[0m inputs, features, s0, target_H \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), features\u001b[38;5;241m.\u001b[39mto(\n\u001b[0;32m     23\u001b[0m     device), s0\u001b[38;5;241m.\u001b[39mto(device), target_H\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     25\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 26\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms0\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 模型的輸出\u001b[39;00m\n\u001b[0;32m     27\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, target_H)  \u001b[38;5;66;03m# 使用真實的 H(t) 計算損失\u001b[39;00m\n\u001b[0;32m     28\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[41], line 58\u001b[0m, in \u001b[0;36mMMINet.forward\u001b[1;34m(self, x, var, s0, n_init)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrnn1_hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrnn1(x[:, t, \u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m2\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrnn1_hx)\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# DNN1 input (rnn1_hx,F,T,Hdc,N)\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m dnn1_in \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnn1_hx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvar\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# H hysteresis prediction\u001b[39;00m\n\u001b[0;32m     61\u001b[0m H_hyst_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdnn1(dnn1_in)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    data_B, data_F, data_T, data_H, data_Pcv, data_Hdc, data_N = load_dataset(\n",
    "        material)\n",
    "\n",
    "    train_loader, valid_loader, norm = get_dataloader(data_B, data_F, data_T,\n",
    "                                                      data_H, data_N, data_Hdc,\n",
    "                                                      data_Pcv)\n",
    "\n",
    "    train_model(norm, train_loader, valid_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
