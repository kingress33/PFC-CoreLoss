# %% [markdown]
# # Part 1: Network Training

# %% [markdown]
# ## Step0: Import Package & Hyperparameter Configuration

# %%
# # æ¸…ç©ºæ‰€æœ‰è®Šæ•¸
# %reset -f
# # å¼·åˆ¶ Python å›æ”¶è¨˜æ†¶é«”
# import gc
# gc.collect()

# %% [markdown]
# ### Package
#

# %%
import os
import torch
import numpy as np
import random
import torch.nn as nn
from torch.autograd import Variable
import matplotlib.pyplot as plt
import time
from datetime import datetime
import json
import subprocess

# %% [markdown]
# ### Hyperparameter Config


# %%
# %%
# Unified Hyperparameter Configuration
class Config:
    SEED = 1
    NUM_EPOCHS = 3000
    BATCH_SIZE = 256
    LEARNING_RATE = 0.002  #è«–æ–‡æä¾›
    LR_SCHEDULER_GAMMA = 0.99  #è«–æ–‡æä¾›
    DECAY_EPOCH = 200
    DECAY_RATIO = 0.5
    EARLY_STOPPING_PATIENCE = 500
    HIDDEN_SIZE = 30
    OPERATOR_SIZE = 30


# Reproducibility
random.seed(Config.SEED)
np.random.seed(Config.SEED)
torch.manual_seed(Config.SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

# %% [markdown]
# ### Material & Number of Data

# %%
material = "CH467160"
fix_way = "perChannelScaling_MLP"
note = "n_init2"
downsample = 1024
save_figure = True
timestamp = datetime.now().strftime("%Y%m%d")

# è¨“ç·´æƒ…æ³æ³
plot_interval = 150
train_show_sample = 1

# å®šç¾©ä¿å­˜æ¨¡å‹çš„è·¯å¾‘
model_save_dir = f"./Model/{fix_way}/{downsample}/"
os.makedirs(model_save_dir, exist_ok=True)  # å¦‚æœè·¯å¾‘ä¸å­˜åœ¨ï¼Œå‰µå»ºè·¯å¾‘
model_save_path = os.path.join(model_save_dir,
                               f"{material}_{note}_{timestamp}.pt")  # å®šç¾©æ¨¡å‹ä¿å­˜æª”å

figure_save_base_path = f"./figure/{fix_way}/{downsample}/"
os.makedirs(figure_save_base_path, exist_ok=True)  # å¦‚æœè·¯å¾‘ä¸å­˜åœ¨ï¼Œå‰µå»ºè·¯å¾‘

# Select device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# %% [markdown]
# ## Step1: Data processing and data loader generate


# %%
# %% Preprocess data into a data loader
def get_dataloader(data_B,
                   data_F,
                   data_T,
                   data_H,
                   data_N,
                   data_Hdc,
                   data_Pcv,
                   n_init=16):

    # Data pre-process

    # â”€â”€ 0. å…¨åŸŸè¨­å®š/é™éšè¨­å®š â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    eps = 1e-8  # é˜²æ­¢é™¤ä»¥ 0
    if downsample == 1024:
        seq_length = 1024  # å–®ç­†æ³¢å½¢é»æ•¸ (ä¸å† down-sample)
    else:
        seq_length = downsample
        cols = np.linspace(0, 1023, seq_length, dtype=int)
        data_B = data_B[:, cols]
        data_H = data_H[:, cols]

    # â”€â”€ 1. æ³¢å½¢æ‹¼æ¥ (è£œ n_init é»ä½œåˆå§‹ç£åŒ–) â”€â”€â”€â”€
    data_length = seq_length + n_init
    data_B = np.hstack((data_B[:, -n_init:], data_B))  # (batch, data_length)
    data_H = np.hstack((data_H[:, -n_init:], data_H))

    # â”€â”€ 2. è½‰æˆ Tensor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    B = torch.from_numpy(data_B).view(-1, data_length, 1).float()  # (B,N,1)
    H = torch.from_numpy(data_H).view(-1, data_length, 1).float()
    F = torch.log10(torch.from_numpy(data_F).view(-1, 1).float())  # ç´”é‡
    T = torch.from_numpy(data_T).view(-1, 1).float()
    Hdc = torch.from_numpy(data_Hdc).view(-1, 1).float()
    N = torch.from_numpy(data_N).view(-1, 1).float()
    Pcv = torch.log10(torch.from_numpy(data_Pcv).view(-1, 1).float())

    # â”€â”€ 3. æ¯ç­†æ¨£æœ¬å„è‡ªæ‰¾æœ€å¤§å¹…å€¼ (per-profile scale) â”€
    scale_B = torch.max(torch.abs(B), dim=1,
                        keepdim=True).values + eps  # (B,1,1)
    scale_H = torch.max(torch.abs(H), dim=1, keepdim=True).values + eps

    # â”€â”€ 4. å…ˆè¨ˆç®—å°æ•¸ï¼Œå†é™¤ä»¥ scale_B â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    dB = torch.diff(B, dim=1, prepend=B[:, :1])
    dB_dt = dB * (seq_length * F.view(-1, 1, 1))  # çœŸå¯¦æ–œç‡
    d2B = torch.diff(dB, dim=1, prepend=dB[:, :1])
    d2B_dt = d2B * (seq_length * F.view(-1, 1, 1))

    # â”€â”€ 5. å½¢æˆæ¨¡å‹è¼¸å…¥ (å·²ç¶“ç¸®æ”¾åˆ° [-1,1]) â”€â”€â”€â”€â”€â”€â”€â”€
    in_B = B / scale_B
    out_H = H / scale_H  # é æ¸¬ç›®æ¨™
    in_dB_dt = dB_dt / scale_B
    in_d2B_dt = d2B_dt / scale_B

    # â”€â”€ 6. ç´”é‡ç‰¹å¾µï¼šè¨ˆç®— z-score åƒæ•¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def safe_mean_std(tensor, eps=1e-8):
        m = torch.mean(tensor).item()
        s = torch.std(tensor).item()
        return [m, 1.0 if s < eps else s]

    #  Compute normalization parameters (å‡å€¼ & æ¨™æº–å·®)**
    norm = [
        safe_mean_std(F),
        safe_mean_std(T),
        safe_mean_std(Hdc),
        safe_mean_std(N),
        safe_mean_std(Pcv)
    ]

    # ç”¨ä¾†åštestå›ºå®šæ¨™æº–åŒ–åƒæ•¸çš„
    print("0.F, 1.T, 2.Hdc, 3.N, 4.Pcv")
    material_name = f"{material}"
    print(f'"{material_name}": [')
    for param in norm:
        print(f"    {param},")
    print("]")

    # Data Normalization
    in_F = (F - norm[0][0]) / norm[0][1]  # F
    in_T = (T - norm[1][0]) / norm[1][1]  # T
    in_Hdc = (Hdc - norm[2][0]) / norm[2][1]  # Hdc
    in_N = (N - norm[3][0]) / norm[3][1]  # N
    in_Pcv = (Pcv - norm[4][0]) / norm[4][1]  # Pcv

    #   â†’ æ–¹ä¾¿æ¨è«–å¾©åŸï¼Œä¿ç•™ scale_B, scale_H ç•¶ä½œé¡å¤–ç´”é‡
    aux_features = torch.cat(
        (in_F, in_T, in_Hdc, in_N, in_Pcv, scale_B.squeeze(-1),
         scale_H.squeeze(-1)),  # (batch, 7)
        dim=1)

    # â”€â”€ 7. ç”¢ç”Ÿåˆå§‹ Preisach operator ç‹€æ…‹ s0 â”€â”€â”€â”€â”€â”€
    max_B, _ = torch.max(in_B, dim=1)
    min_B, _ = torch.min(in_B, dim=1)
    s0 = get_operator_init(in_B[:, 0] - dB[:, 0] / scale_B.squeeze(-1),
                           dB / scale_B, max_B, min_B)

    # â”€â”€ 8. çµ„åˆ Dataset â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    wave_inputs = torch.cat(
        (
            in_B,  # â‘  B
            dB / scale_B,  # â‘¡ Î”B
            in_dB_dt,  # â‘¢ dB/dt
            in_d2B_dt),
        dim=2)  # â‘£ dÂ²B/dtÂ²   â†’ (B,L,4)

    aux_features = torch.cat((in_F, in_T, in_Hdc, in_N), dim=1)  # (B,4)
    amps = torch.cat((scale_B.squeeze(-1), scale_H.squeeze(-1)),
                     dim=1)  # (B,2)

    # é€™è£¡æŠŠ Pcvï¼ˆå·² z-scoreï¼‰å–®ç¨æ‹¿å‡ºä¾†ç•¶å¦ä¸€å€‹ label
    target_Pcv = in_Pcv  # (B,1)

    full_dataset = torch.utils.data.TensorDataset(
        wave_inputs,  # 0  â†’ æ¨¡å‹åºåˆ—è¼¸å…¥
        aux_features,  # 1  â†’ 4 å€‹ç´”é‡
        amps,  # 2  â†’ å¹…å€¼ä¿‚æ•¸
        s0,  # 3  â†’ Preisach åˆå§‹ç‹€æ…‹
        out_H,  # 4  â†’ ç›®æ¨™ H  (å·² scale_H)
        target_Pcv)  # 5  â†’ ç›®æ¨™ Pcv (å·² z-score)

    # â”€â”€ 9. Train / Valid split & DataLoader â”€â”€â”€â”€â”€â”€â”€
    train_size = int(0.8 * len(full_dataset))
    valid_size = len(full_dataset) - train_size
    train_set, valid_set = torch.utils.data.random_split(
        full_dataset, [train_size, valid_size],
        generator=torch.Generator().manual_seed(Config.SEED))

    train_loader = torch.utils.data.DataLoader(
        train_set,
        batch_size=Config.BATCH_SIZE,
        shuffle=True,
        #    num_workers=4,
        pin_memory=True,
        collate_fn=filter_input)

    valid_loader = torch.utils.data.DataLoader(
        valid_set,
        batch_size=Config.BATCH_SIZE,
        shuffle=False,
        #    num_workers=4,
        pin_memory=True,
        collate_fn=filter_input)

    return train_loader, valid_loader, norm


# %% Predict the operator state at t0
def get_operator_init(B1,
                      dB,
                      Bmax,
                      Bmin,
                      max_out_H=1,
                      operator_size=Config.OPERATOR_SIZE):
    """Compute the initial state of hysteresis operators"""
    s0 = torch.zeros((dB.shape[0], operator_size))
    operator_thre = torch.from_numpy(
        np.linspace(max_out_H / operator_size, max_out_H,
                    operator_size)).view(1, -1)

    for i in range(dB.shape[0]):
        for j in range(operator_size):
            r = operator_thre[0, j]
            if (Bmax[i] >= r) or (Bmin[i] <= -r):
                if dB[i, 0] >= 0:
                    if B1[i] > Bmin[i] + 2 * r:
                        s0[i, j] = r
                    else:
                        s0[i, j] = B1[i] - (r + Bmin[i])
                else:
                    if B1[i] < Bmax[i] - 2 * r:
                        s0[i, j] = -r
                    else:
                        s0[i, j] = B1[i] + (r - Bmax[i])
    return s0


def filter_input(batch):
    inputs, features, amps, s0, target_H, target_Pcv = zip(*batch)

    inputs = torch.stack(inputs)  # (B,L,4)
    features = torch.stack(features)  # (B,4)
    amps = torch.stack(amps)
    s0 = torch.stack(s0)
    target_H = torch.stack(target_H)[:, -downsample:, :]  # ä¿ç•™å…¨é•·
    target_Pcv = torch.stack(target_Pcv)  # (B,1)

    # return inputs, features, s0, target_H, target_Pcv
    return inputs, features, amps, s0, target_H, target_Pcv


# æº«åº¦é »ç‡ä¸è®ŠåŠ å…¥å¾®å°çš„ epsilon
def safe_mean_std(tensor, eps=1e-8):
    m_tensor = torch.mean(tensor)  # é‚„æ˜¯ Tensor
    s_tensor = torch.std(tensor)  # é‚„æ˜¯ Tensor

    m_val = m_tensor.item()  # ç¬¬ä¸€æ¬¡è½‰æˆ float
    s_val = s_tensor.item()
    if s_val < eps:
        s_val = 1.0
    return [m_val, s_val]  # ç›´æ¥å›å‚³ float


# %% [markdown]
# ## Step2: Define Network Structure

# %%
# %% Magnetization mechansim-determined neural network
"""
    Parameters:
    - hidden_size: number of eddy current slices (RNN neuron)
    - operator_size: number of operators
    - input_size: number of inputs (1.B 2.dB 3.dB/dt 4.d2B/dt)
    - var_size: number of supplenmentary variables (1.F 2.T 3.Hdc 4.N)        
    - output_size: number of outputs (1.H)
    
    åªå…ˆæŠŠd2B/dtè€ƒé‡åœ¨EddyCellè£¡é¢
"""


class MMINet(nn.Module):

    def __init__(
            self,
            norm,
            hidden_size=Config.HIDDEN_SIZE,
            operator_size=Config.OPERATOR_SIZE,
            input_size=4,  #!æ–°å¢d2B(250203)
            var_size=4,
            output_size=1):
        super().__init__()
        self.input_size = input_size
        self.var_size = var_size
        self.hidden_size = hidden_size
        self.output_size = output_size
        self.operator_size = operator_size
        self.norm = norm  #*é€™è£¡æ”¹æˆå¾å¤–éƒ¨å‚³å…¥ norm(250203)

        self.rnn1 = StopOperatorCell(self.operator_size)
        self.dnn1 = nn.Linear(self.operator_size + self.var_size, 1)
        #!250520æ›´æ–°ï¼š5 (F, T, B, dB/dt, d2B/dt ) + 2 (Hdc, N)
        self.rnn2 = EddyCell(7, self.hidden_size, output_size)
        self.dnn2 = nn.Linear(self.hidden_size, 1)
        self.rnn2_hx = None

        self.loss_mlp = nn.Sequential(
            nn.Linear(self.var_size + 1,
                      64),  # var_size=4: F, T, Hdc, N + 1 for P_prelim
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(32, 1))

    def forward(self, x, var, amps, s0, n_init=16):
        """
         Parameters: 
          - x(batch,seq,input_size): Input features (1.B, 2.dB, 3.dB/dt)  
# !       - var(batch,var_size): Supplementary inputs (1.F 2.T 3.Hdc 4.N) 
          - s0(batch,1): Operator inital states
        """
        batch_size = x.size(0)  # Batch size
        seq_size = x.size(1)  # Ser
        self.rnn1_hx = s0

        # !Initialize DNN2 input (1.B 2.dB/dt 3.d2B)
        # x2 = torch.cat((x[:, :, 0:1], x[:, :, 2:3]), dim=2)
        # !é¸å– B, dB/dt, d2B/dt
        x2 = torch.cat((x[:, :, 0:1], x[:, :, 2:4]), dim=2)

        for t in range(seq_size):
            # RNN1 input (dB,state)
            self.rnn1_hx = self.rnn1(x[:, t, 1:2], self.rnn1_hx)

            # DNN1 input (rnn1_hx,F,T,Hdc,N)
            dnn1_in = torch.cat((self.rnn1_hx, var), dim=1)

            # H hysteresis prediction
            H_hyst_pred = self.dnn1(dnn1_in)

            # DNN2 input (B,dB/dt,T,F)
            rnn2_in = torch.cat((x2[:, t, :], var), dim=1)

            # Initialize second rnn state
            if t == 0:
                H_eddy_init = x[:, t, 0:1] - H_hyst_pred
                buffer = x.new_ones(x.size(0), self.hidden_size)
                self.rnn2_hx = Variable(
                    (buffer / torch.sum(self.dnn2.weight, dim=1)) *
                    H_eddy_init)

            #rnn2_in = torch.cat((rnn2_in,H_hyst_pred),dim=1)
            self.rnn2_hx = self.rnn2(rnn2_in, self.rnn2_hx)

            # H eddy prediction
            H_eddy = self.dnn2(self.rnn2_hx)

            # H total
            H_total = (H_hyst_pred + H_eddy).view(batch_size, 1,
                                                  self.output_size)
            if t == 0:
                output = H_total
            else:
                output = torch.cat((output, H_total), dim=1)

        H = (output[:, n_init:, :])

        amp_B = amps[:, 0:1]  # (batch,1)
        amp_H = amps[:, 1:2]  # (batch,1)
        B_amp = x[:, n_init:, 0:1] * amp_B.unsqueeze(1)
        H_amp = output[:, n_init:, :] * amp_H.unsqueeze(1)
        P_prelim = torch.trapz(H_amp, B_amp, axis=1) * (10**(
            var[:, 0:1] * self.norm[0][1] + self.norm[0][0]))
        Pcv_log = torch.log10(P_prelim.clamp(min=1e-12))
        Pcv = (Pcv_log - self.norm[4][0]) / self.norm[4][1]
        mlp_input = torch.cat((var, Pcv), dim=1)  # (batch, 5)
        s = self.loss_mlp(mlp_input)
        Pcv_mlp = Pcv + s

        return H, Pcv_mlp


class StopOperatorCell():

    def __init__(self, operator_size):
        self.operator_thre = torch.from_numpy(
            np.linspace(5 / operator_size, 5, operator_size)).view(1, -1)

    def sslu(self, X):
        a = torch.ones_like(X)
        return torch.max(-a, torch.min(a, X))

    def __call__(self, dB, state):
        r = self.operator_thre.to(dB.device)
        output = self.sslu((dB + state) / r) * r
        return output.float()


class EddyCell(nn.Module):

    def __init__(self, input_size, hidden_size, output_size=1):
        super().__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size

        self.x2h = nn.Linear(input_size, hidden_size, bias=False)
        self.h2h = nn.Linear(hidden_size, hidden_size, bias=False)

    def forward(self, x, hidden=None):
        hidden = self.x2h(x) + self.h2h(hidden)
        hidden = torch.sigmoid(hidden)
        return hidden


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


# %% [markdown]
# ## Step3: Training the Model

# %% [markdown]
# ### Load Dataset


# %%
# %%
def load_dataset(material, base_path="./Data/"):

    in_file1 = f"{base_path}{material}/train/B_Field.csv"
    in_file2 = f"{base_path}{material}/train/Frequency.csv"
    in_file3 = f"{base_path}{material}/train/Temperature.csv"
    in_file4 = f"{base_path}{material}/train/H_Field.csv"
    in_file5 = f"{base_path}{material}/train/Volumetric_Loss.csv"
    in_file6 = f"{base_path}{material}/train/Hdc.csv"  # *250317æ–°å¢ï¼šç›´æµåç½®ç£å ´
    in_file7 = f"{base_path}{material}/train/Turns.csv"  # *250317æ–°å¢ï¼šåŒæ•¸

    data_B = np.genfromtxt(in_file1, delimiter=',')  # N x 1024
    data_F = np.genfromtxt(in_file2, delimiter=',')  # N x 1
    data_T = np.genfromtxt(in_file3, delimiter=',')  # N x 1
    data_H = np.genfromtxt(in_file4, delimiter=',')  # N x 1024
    data_Pcv = np.genfromtxt(in_file5, delimiter=',')  # N x 1
    data_Hdc = np.genfromtxt(in_file6, delimiter=',')  # N x 1
    data_N = np.genfromtxt(in_file7, delimiter=',')  # N x 1

    return data_B, data_F, data_T, data_H, data_Pcv, data_Hdc, data_N


# %% [markdown]
# ### Train Logger


# %%
class TrainLogger:

    def __init__(self, exp_name, config_dict, result_dir):
        self.exp_name = exp_name
        self.result_dir = result_dir
        self.config = config_dict
        os.makedirs(self.result_dir, exist_ok=True)

        self._save_config()
        self._write_metadata()

    def _save_config(self):
        with open(os.path.join(self.result_dir, "config.json"), "w") as f:
            json.dump(self.config, f, indent=2)

    def _write_metadata(self):
        metadata = {
            "experiment_name": self.exp_name,
            "timestamp": datetime.now().isoformat()
        }
        with open(os.path.join(self.result_dir, "meta.json"), "w") as f:
            json.dump(metadata, f, indent=2)

    def save_summary(self, best_epoch, best_val_loss, best_loss_H,
                     best_loss_Pcv, model_save_path, elapsed):
        summary = {
            "exp_name": self.exp_name,
            "timestamp": datetime.now().isoformat(),
            "duration_sec": elapsed,
            "config": self.config,
            "best_model": {
                "path": model_save_path,
                "epoch": best_epoch,
                "val_loss": best_val_loss,
                "loss_H": best_loss_H,
                "loss_Pcv": best_loss_Pcv
            },
            "note": note
        }
        with open(os.path.join(self.result_dir, "summary.json"), "w") as f:
            json.dump(summary, f, indent=2)


# %% [markdown]
# ### Train Code


# %%
def train_model(norm, train_loader, valid_loader, logger):

    start_time = time.perf_counter()
    model = MMINet(norm=norm).to(device)
    print("=== Start Train  ===")
    print(r"""
    (\_/)
    ( â€¢_â€¢)
    / > æ‹œè¨—é †åˆ©é˜¿
    """)
    print("Number of parameters: ", count_parameters(model))

    criterion_H = nn.MSELoss()
    criterion_Pcv = nn.MSELoss()

    optimizer = torch.optim.Adam(model.parameters(), lr=Config.LEARNING_RATE)

    # Loss è¨˜éŒ„
    best_val_loss = float('inf')
    # Early stopping ç´€éŒ„
    patience_counter = 0
    train_losses = []
    val_losses = []
    fixed_idx = None
    # ä¿å­˜æ¯å€‹ epoch çš„æ™‚é–“
    epoch_times = []
    # Logger ç´€éŒ„
    best_epoch = 0
    best_loss_H = 0
    best_loss_Pcv = 0

    for epoch in range(Config.NUM_EPOCHS):
        t0 = time.perf_counter()

        model.train()
        train_loss = 0

        for inputs, features, amps, s0, target_H, target_Pcv in train_loader:
            inputs, features, amps, s0, target_H, target_Pcv = inputs.to(
                device), features.to(device), amps.to(device), s0.to(
                    device), target_H.to(device), target_Pcv.to(device)

            optimizer.zero_grad()

            with torch.autocast(device_type="cuda"):
                outputs_H, outputs_Pcv = model(inputs, features, amps,
                                               s0)  # æ¨¡å‹çš„è¼¸å‡º
                loss_H = criterion_H(outputs_H, target_H)  # ä½¿ç”¨çœŸå¯¦çš„ H(t) è¨ˆç®—æå¤±
                loss_Pcv = criterion_Pcv(outputs_Pcv, target_Pcv)
                # if epoch < 1000:
                #     loss = loss_H
                # else:
                #     alpha = (epoch + 1) / Config.NUM_EPOCHS
                #     loss = (1 - alpha) * loss_H + alpha * loss_Pcv
                alpha = 0.5
                # alpha = (epoch + 1) / Config.NUM_EPOCHS
                loss = (1 - alpha) * loss_H + alpha * loss_Pcv

            loss.backward()
            optimizer.step()
            train_loss += loss.item()

        train_loss /= len(train_loader)
        train_losses.append(train_loss)  # **è¨˜éŒ„ Train Loss**

        model.eval()
        val_loss = 0

        with torch.no_grad():
            for inputs, features, amps, s0, target_H, target_Pcv in valid_loader:
                inputs, features, amps, s0, target_H, target_Pcv = inputs.to(
                    device), features.to(device), amps.to(device), s0.to(
                        device), target_H.to(device), target_Pcv.to(device)

                outputs_H, outputs_Pcv = model(inputs, features, amps,
                                               s0)  # æ¨¡å‹çš„è¼¸å‡º
                loss_H = criterion_H(outputs_H, target_H)  # ä½¿ç”¨çœŸå¯¦çš„ H(t) è¨ˆç®—æå¤±
                loss_Pcv = criterion_Pcv(outputs_Pcv, target_Pcv)
                # if epoch < 1000:
                #     loss = loss_H
                # else:
                #     alpha = (epoch + 1) / Config.NUM_EPOCHS
                #     loss = (1 - alpha) * loss_H + alpha * loss_Pcv

                # alpha = (epoch + 1) / Config.NUM_EPOCHS
                alpha = 0.5
                loss = (1 - alpha) * loss_H + alpha * loss_Pcv

                val_loss += loss.item()

        val_loss /= len(valid_loader)
        val_losses.append(val_loss)  # **è¨˜éŒ„ Validation Loss**

        # â”€â”€â”€ è¨ˆç®—ä¸¦è¼¸å‡ºé€™å€‹ epoch çš„è€—æ™‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        te = time.perf_counter() - t0
        epoch_times.append(te)
        print(
            f"Epoch {epoch+1}, loss_H: {loss_H.item():.6f}, loss_Pcv: {loss_Pcv.item():.6f}"
        )
        print(
            f"Epoch {epoch+1} | Train Loss: {train_loss:.6f} | Val Loss: {val_loss:.6f} | Time: {te:.2f}s"
        )

        # ======================================================ç¹ªè£½è¨“ç·´æƒ…æ³======================================================

        if (epoch + 1) % plot_interval == 0:

            # ç¬¬ä¸€æ¬¡ç”¢ç”Ÿå›ºå®šçš„éš¨æ©Ÿç´¢å¼•
            if fixed_idx is None:
                batch_size_fix = 3
                fixed_idx = torch.randperm(batch_size_fix)[:train_show_sample]

            # # -------------------------è¨­å®šåœ–è¡¨H(t)æ¯”è¼ƒ---------------------------------------

            # outputs = [fixed_idx, :downsample,
            #  0].detach().cpu().numpy()
            # targets_np = target_H[fixed_idx, :downsample,
            #                       0].detach().cpu().numpy()

            # plt.figure(figsize=(12, 6))

            # for i in range(outputs.shape[0]):  # æ¯ä¸€æ‰¹æ•¸æ“šç¹ªè£½ä¸€å€‹åœ–è¡¨
            #     plt.plot(outputs[i, :, 0],
            #              label=f"Pred: Sample {i+1}",
            #              linestyle='--',
            #              marker='o')
            #     plt.plot(targets[i, :, 0],
            #              label=f"Target: Sample {i+1}",
            #              linestyle='-',
            #              marker='x')

            # # æ·»åŠ æ¨™é¡Œå’Œæ¨™ç±¤
            # plt.title(f"Compare - Epoch {epoch + 1}", fontsize=16)
            # plt.xlabel("Index", fontsize=14)
            # plt.ylabel("Value", fontsize=14)
            # plt.legend(loc="upper right", fontsize=12)
            # plt.grid(alpha=0.5)

            # # é¡¯ç¤ºåœ–è¡¨
            # plt.show()
            # # -------------------------è¨­å®šåœ–è¡¨H(t)æ¯”è¼ƒ çµæŸ---------------------------------------

            # # -------------------------è¨­å®šåœ–è¡¨B-Hæ¯”è¼ƒ---------------------------------------
            # å–å°æ‡‰ sample
            outputs_np = outputs_H[fixed_idx, -downsample:,
                                   0].detach().cpu().numpy()
            targets_np = target_H[fixed_idx, -downsample:,
                                  0].detach().cpu().numpy()
            B_seq_np = inputs[fixed_idx, -downsample:,
                              0].detach().cpu().numpy()

            # è¨­å®šåœ–è¡¨
            plt.figure()

            for i in range(train_show_sample):  # æ¯ä¸€æ‰¹æ•¸æ“šç¹ªè£½ä¸€å€‹åœ–è¡¨
                plt.plot(outputs_np[i],
                         B_seq_np[i],
                         label=f"Pred: Sample {i+1}",
                         markersize=1)

                plt.plot(targets_np[i],
                         B_seq_np[i],
                         label=f"Target: Sample {i+1}",
                         alpha=0.5)

            # æ·»åŠ æ¨™é¡Œå’Œæ¨™ç±¤
            plt.title(f"Compare - Epoch {epoch + 1}")
            plt.xlabel("Index")
            plt.ylabel("Value")
            plt.grid(alpha=0.5)
            plt.legend()
            if save_figure == True:
                figure_save_path1 = os.path.join(
                    figure_save_base_path,
                    f"Compare_Epoch {epoch + 1}.svg")  # å®šç¾©æ¨¡å‹ä¿å­˜æª”å
                plt.savefig(figure_save_path1)
            plt.show()
            # # -------------------------è¨­å®šåœ–è¡¨B-Hæ¯”è¼ƒ END---------------------------------------
        # ======================================================ç¹ªè£½è¨“ç·´æƒ…æ³  END ======================================================

        # ======================================================Early stop======================================================
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            best_val_loss = val_loss
            best_epoch = epoch + 1
            best_loss_H = loss_H.item()
            best_loss_Pcv = loss_Pcv.item()
            torch.save(model.state_dict(), model_save_path)  # ä¿å­˜æœ€ä½³æ¨¡å‹
            print(
                f"Saving model at epoch {epoch+1} with validation loss {val_loss:.6f}..."
            )
            patience_counter = 0
        else:
            patience_counter += 1

        if patience_counter >= Config.EARLY_STOPPING_PATIENCE:
            print("Early stopping triggered.")
            break

        # ======================================================Early stop======================================================

    print(f"Training complete. Best model saved at {model_save_path}.")
    elapsed = time.perf_counter() - start_time  # â† è¨“ç·´çµæŸï¼Œè¨ˆç®—è€—æ™‚
    hrs = int(elapsed // 3600)
    mins = int((elapsed % 3600) // 60)
    secs = elapsed % 60
    print(f"è¨“ç·´ç¸½è€—æ™‚ï¼š{hrs} å°æ™‚ {mins} åˆ† {secs:.2f} ç§’")
    logger.save_summary(best_epoch, best_val_loss, best_loss_H, best_loss_Pcv,
                        model_save_path, elapsed)

    # ==============================ç¹ªè£½ Train Loss èˆ‡ Validation Loss åœ–==============================
    plt.figure(figsize=(10, 5))
    plt.plot(
        range(1,
              len(train_losses) + 1),
        train_losses,
        label="Train Loss",
    )
    plt.plot(range(1,
                   len(val_losses) + 1),
             val_losses,
             label="Validation Loss")

    plt.xlabel("Epochs")
    plt.ylabel("Loss")
    plt.title("Training & Validation Loss Curve")
    plt.legend()
    plt.grid(alpha=0.5)
    if save_figure == True:
        # å°‡åœ–è¡¨ä¿å­˜ç‚º SVG æ ¼å¼
        figure_save_path2 = os.path.join(figure_save_base_path,
                                         "Training_Validation_Loss_Curve.svg")
        plt.savefig(figure_save_path2)
    plt.show()
    # ==============================ç¹ªè£½ Train Loss èˆ‡ Validation Loss åœ– END==============================

    # ===================================ä½¿ç”¨æœ€ä½³æ¨¡å‹ä¾†ç”¢ç”Ÿé©—è­‰çµæœ=============================
    model.load_state_dict(torch.load(model_save_path))  # è¼‰å…¥æœ€ä½³æ¨¡å‹
    model.eval()

    with torch.no_grad():
        for inputs, features, amps, s0, target_H, target_Pcv in valid_loader:
            inputs, features, amps, s0, target_H, target_Pcv = inputs.to(
                device), features.to(device), amps.to(device), s0.to(
                    device), target_H.to(device), target_Pcv.to(device)

            outputs_H, outputs_Pcv = model(inputs, features, amps, s0)
            break  # åªä½¿ç”¨ä¸€æ‰¹é©—è­‰æ•¸æ“šé€²è¡Œå¯è¦–åŒ–

    # é¸å–å°æ‡‰è³‡æ–™ï¼ˆindex tensor è¦å…ˆè½‰ list æ‰èƒ½ index numpyï¼‰
    outputs_np = outputs_H[fixed_idx, -downsample:, 0].detach().cpu().numpy()
    targets_np = target_H[fixed_idx, -downsample:, 0].detach().cpu().numpy()
    B_seq_np = inputs[fixed_idx, -downsample:, 0].detach().cpu().numpy()

    # è¨­å®šåœ–è¡¨
    plt.figure()

    for i in range(train_show_sample):  # æ¯ä¸€æ‰¹æ•¸æ“šç¹ªè£½ä¸€å€‹åœ–è¡¨
        plt.plot(outputs_np[i], B_seq_np[i], label=f"Pred: Sample {i+1}")

        plt.plot(targets_np[i],
                 B_seq_np[i],
                 label=f"Target: Sample {i+1}",
                 alpha=0.7)

    # æ·»åŠ æ¨™é¡Œå’Œæ¨™ç±¤
    plt.title(f"Best Model - Predicted vs Target")
    plt.xlabel("Index")
    plt.ylabel("Value")
    plt.grid(alpha=0.5)
    plt.legend()
    if save_figure == True:
        figure_save_path3 = os.path.join(
            figure_save_base_path,
            "Best Model_Predicted vs Target.svg")  # å®šç¾©æ¨¡å‹ä¿å­˜æª”å
        plt.savefig(figure_save_path3)

    # ===================================ä½¿ç”¨æœ€ä½³æ¨¡å‹ä¾†ç”¢ç”Ÿé©—è­‰çµæœ END=============================


# %% [markdown]
# ### Start Train!!!


# %%
def main():
    # # é€™æ®µæ”¾åœ¨æª”æ¡ˆæœ€å‰é¢ï¼ˆimport ä¹‹å¾Œï¼‰
    # BASE_DIR = Path(__file__).resolve().parent
    # os.chdir(BASE_DIR)
    # print("ğŸ‘‰ Switch CWD to script folder:", os.getcwd())

    data_B, data_F, data_T, data_H, data_Pcv, data_Hdc, data_N = load_dataset(
        material)

    train_loader, valid_loader, norm = get_dataloader(data_B, data_F, data_T,
                                                      data_H, data_N, data_Hdc,
                                                      data_Pcv)

    # ---- å°ç¬¬ä¸€å€‹ batch æª¢æŸ¥ ----
    # inputs, features, s0, target_H, target_Pcv = next(iter(train_loader))
    inputs, features, amps, s0, target_H, target_Pcv = next(iter(train_loader))

    print("=== Batch shape check ===")
    print(f"inputs      : {inputs.shape}")  # (batch, seq_len, 4)
    print(f"features    : {features.shape}")  # (batch, 4)
    print(f"s0          : {s0.shape}")  # (batch, operator_size)
    print(f"target_H    : {target_H.shape}")  # (batch, seq_len, 1)
    # print(f"target_Pcv  : {target_Pcv.shape}")  # (batch, 1)
    print()

    # é¸ä¸€ç­†æ¨£æœ¬çœ‹çœ‹æ•¸å€¼ç¯„åœ
    idx = 0
    print("ç¯„ä¾‹ inputs[0] (å‰ 3 å€‹æ™‚é–“é»):")
    print(inputs[idx, :3, :])  # B, Î”B, dB/dt, dÂ²B/dtÂ² (å·²æ­¸ä¸€åŒ–åˆ° ~[-1,1])
    print("ç¯„ä¾‹ features[0]:", features[idx])  # F, T, Hdc, N (å·² z-score)
    print("ç¯„ä¾‹ s0[0]:", s0[idx, :5])  # å‰ 5 å€‹ Preisach operator ç‹€æ…‹
    print("ç¯„ä¾‹ target_H[0] (å‰ 3 é»):", target_H[idx, :3, 0])
    # print("ç¯„ä¾‹ target_Pcv[0]:", target_Pcv[idx, 0])

    # ç”¢ç”Ÿ Loggerï¼ˆæ”¾åœ¨ train_model å‰ï¼‰
    result_dir = os.path.join("results",
                              f"{timestamp}_{fix_way}_{material}_{note}")
    logger = TrainLogger(
        exp_name=f"{material}_{note}_{timestamp}",
        config_dict={
            k: getattr(Config, k)
            for k in dir(Config)
            if not k.startswith('__') and not callable(getattr(Config, k))
        },
        result_dir=result_dir)

    train_model(norm, train_loader, valid_loader, logger)  # logger

    # train_model(norm, train_loader, valid_loader)


# %%
if __name__ == "__main__":
    main()
