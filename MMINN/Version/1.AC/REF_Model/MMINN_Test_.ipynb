{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# from Model import MMINet\n",
    "# from Data import get_dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Preprocess data into a data loader\n",
    "def get_dataloader(data_B, data_F, data_T, norm, n_init=16):\n",
    "    \"\"\"Get a test dataloader \n",
    "\n",
    "    Parameters\n",
    "    ---------\n",
    "    data_B/data_F/data_T : array \n",
    "         B/F/T data\n",
    "    norm : list \n",
    "         B/F/T normalization data\n",
    "    n_init : int\n",
    "         Additional points for computing the history magnetization\n",
    "    \"\"\"\n",
    "\n",
    "    # Data pre-process\n",
    "    # 1. Down-sample to 128 points\n",
    "    seq_length = 128\n",
    "    cols = range(0, 1024, int(1024 / seq_length))\n",
    "    data_B = data_B[:, cols]\n",
    "\n",
    "    # 2. Add extra points for initial magnetization calculation\n",
    "    data_length = seq_length + n_init\n",
    "    data_B = np.hstack((data_B, data_B[:, :n_init]))\n",
    "\n",
    "    # 3. Format data into tensors\n",
    "    B = torch.from_numpy(data_B).view(-1, data_length, 1).float()\n",
    "    F = torch.log10(torch.from_numpy(data_F).view(-1, 1).float())\n",
    "    T = torch.from_numpy(data_T).view(-1, 1).float()\n",
    "\n",
    "    # 4. Data Normalization\n",
    "    in_B = (B - norm[0][0]) / norm[0][1]\n",
    "    in_T = (T - norm[3][0]) / norm[3][1]\n",
    "    in_F = (F - norm[2][0]) / norm[2][1]\n",
    "\n",
    "    # 5. Extra features\n",
    "    dB = torch.diff(B, dim=1)\n",
    "    dB = torch.cat((dB[:, 0:1], dB), dim=1)\n",
    "    dB_dt = dB * (seq_length * F.view(-1, 1, 1))\n",
    "\n",
    "    in_dB = torch.diff(B, dim=1)  # Flux density change\n",
    "    in_dB = torch.cat((in_dB[:, 0:1], in_dB), dim=1)\n",
    "\n",
    "    in_dB_dt = (dB_dt - norm[4][0]) / norm[4][1]  # Flux density change rate\n",
    "\n",
    "    max_B, _ = torch.max(in_B, dim=1)\n",
    "    min_B, _ = torch.min(in_B, dim=1)\n",
    "\n",
    "    s0 = get_operator_init(in_B[:, 0] - in_dB[:, 0], in_dB, max_B,\n",
    "                           min_B)  # Operator inital state\n",
    "\n",
    "    # 6. Create dataloader to speed up data processing\n",
    "    test_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.cat((in_B, in_dB, in_dB_dt), dim=2),\n",
    "        torch.cat((in_F, in_T), dim=1), s0)\n",
    "    kwargs = {\n",
    "        'num_workers': 4,\n",
    "        'batch_size': 128,\n",
    "        'pin_memory': True,\n",
    "        'pin_memory_device': \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "        'drop_last': False\n",
    "    }\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, **kwargs)\n",
    "\n",
    "    return test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Predict the operator state at t0\n",
    "def get_operator_init(B1, dB, Bmax, Bmin, max_out_H=5, operator_size=30):\n",
    "    \"\"\"Compute the inital state of hysteresis operators\n",
    "\n",
    "    Parameters\n",
    "    ---------\n",
    "    B1 : torch_like (batch)\n",
    "         Stop operator excitation at t1\n",
    "    dB : torch_like (batch, data_length)\n",
    "         Flux density changes at each t\n",
    "    Bmax/Bmin : torch_like (batch)\n",
    "         Max/Min flux density of each cycle \n",
    "    \"\"\"\n",
    "    # 1. Parameter setting\n",
    "    s0 = torch.zeros((dB.shape[0], operator_size))  # Allocate cache for s0\n",
    "    operator_thre = torch.from_numpy(\n",
    "        np.linspace(max_out_H / operator_size, max_out_H,\n",
    "                    operator_size)).view(1,\n",
    "                                         -1)  # hysteresis operators' threshold\n",
    "\n",
    "    # 2. Iterate each excitation for the operator inital state computation\n",
    "    for i in range(dB.shape[0]):\n",
    "        for j in range(operator_size):\n",
    "            r = operator_thre[0, j]\n",
    "            if (Bmax[i] >= r) or (Bmin[i] <= -r):\n",
    "                if dB[i, 0] >= 0:\n",
    "                    if B1[i] > Bmin[i] + 2 * r:\n",
    "                        s0[i, j] = r\n",
    "                    else:\n",
    "                        s0[i, j] = B1[i] - (r + Bmin[i])\n",
    "                else:\n",
    "                    if B1[i] < Bmax[i] - 2 * r:\n",
    "                        s0[i, j] = -r\n",
    "                    else:\n",
    "                        s0[i, j] = B1[i] + (r - Bmax[i])\n",
    "\n",
    "    return s0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Preprocess data into a data loader\n",
    "def get_dataloader(data_B, data_F, data_T, norm, n_init=16):\n",
    "    \"\"\"Get a test dataloader \n",
    "\n",
    "    Parameters\n",
    "    ---------\n",
    "    data_B/data_F/data_T : array \n",
    "         B/F/T data\n",
    "    norm : list \n",
    "         B/F/T normalization data\n",
    "    n_init : int\n",
    "         Additional points for computing the history magnetization\n",
    "    \"\"\"\n",
    "\n",
    "    # Data pre-process\n",
    "    # 1. Down-sample to 128 points\n",
    "    seq_length = 128\n",
    "    cols = range(0, 1024, int(1024 / seq_length))\n",
    "    data_B = data_B[:, cols]\n",
    "\n",
    "    # 2. Add extra points for initial magnetization calculation\n",
    "    data_length = seq_length + n_init\n",
    "    data_B = np.hstack((data_B, data_B[:, :n_init]))\n",
    "\n",
    "    # 3. Format data into tensors\n",
    "    B = torch.from_numpy(data_B).view(-1, data_length, 1).float()\n",
    "    F = torch.log10(torch.from_numpy(data_F).view(-1, 1).float())\n",
    "    T = torch.from_numpy(data_T).view(-1, 1).float()\n",
    "\n",
    "    # 4. Data Normalization\n",
    "    in_B = (B - norm[0][0]) / norm[0][1]\n",
    "\n",
    "    in_T = (T - norm[3][0]) / norm[3][1]\n",
    "    in_F = (F - norm[2][0]) / norm[2][1]\n",
    "\n",
    "    # 5. Extra features\n",
    "    dB = torch.diff(B, dim=1)\n",
    "    dB = torch.cat((dB[:, 0:1], dB), dim=1)\n",
    "    dB_dt = dB * (seq_length * F.view(-1, 1, 1))\n",
    "\n",
    "    in_dB = torch.diff(B, dim=1)  # Flux density change\n",
    "    in_dB = torch.cat((in_dB[:, 0:1], in_dB), dim=1)\n",
    "\n",
    "    in_dB_dt = (dB_dt - norm[4][0]) / norm[4][1]  # Flux density change rate\n",
    "\n",
    "    max_B, _ = torch.max(in_B, dim=1)\n",
    "    min_B, _ = torch.min(in_B, dim=1)\n",
    "\n",
    "    s0 = get_operator_init(in_B[:, 0] - in_dB[:, 0], in_dB, max_B,\n",
    "                           min_B)  # Operator inital state\n",
    "\n",
    "    # 6. Create dataloader to speed up data processing\n",
    "    test_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.cat((in_B, in_dB, in_dB_dt), dim=2),\n",
    "        torch.cat((in_F, in_T), dim=1), s0)\n",
    "    kwargs = {\n",
    "        'num_workers': 4,\n",
    "        'batch_size': 128,\n",
    "        'pin_memory': True,\n",
    "        'pin_memory_device': \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "        'drop_last': False\n",
    "    }\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, **kwargs)\n",
    "\n",
    "    return test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Predict the operator state at t0\n",
    "def get_operator_init(B1, dB, Bmax, Bmin, max_out_H=5, operator_size=30):\n",
    "    \"\"\"Compute the inital state of hysteresis operators\n",
    "\n",
    "    Parameters\n",
    "    ---------\n",
    "    B1 : torch_like (batch)\n",
    "         Stop operator excitation at t1\n",
    "    dB : torch_like (batch, data_length)\n",
    "         Flux density changes at each t\n",
    "    Bmax/Bmin : torch_like (batch)\n",
    "         Max/Min flux density of each cycle \n",
    "    \"\"\"\n",
    "    # 1. Parameter setting\n",
    "    s0 = torch.zeros((dB.shape[0], operator_size))  # Allocate cache for s0\n",
    "    operator_thre = torch.from_numpy(\n",
    "        np.linspace(max_out_H / operator_size, max_out_H,\n",
    "                    operator_size)).view(1,\n",
    "                                         -1)  # hysteresis operators' threshold\n",
    "\n",
    "    # 2. Iterate each excitation for the operator inital state computation\n",
    "    for i in range(dB.shape[0]):\n",
    "        for j in range(operator_size):\n",
    "            r = operator_thre[0, j]\n",
    "            if (Bmax[i] >= r) or (Bmin[i] <= -r):\n",
    "                if dB[i, 0] >= 0:\n",
    "                    if B1[i] > Bmin[i] + 2 * r:\n",
    "                        s0[i, j] = r\n",
    "                    else:\n",
    "                        s0[i, j] = B1[i] - (r + Bmin[i])\n",
    "                else:\n",
    "                    if B1[i] < Bmax[i] - 2 * r:\n",
    "                        s0[i, j] = -r\n",
    "                    else:\n",
    "                        s0[i, j] = B1[i] + (r - Bmax[i])\n",
    "\n",
    "    return s0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Material normalization data (1.B 2.H 3.F 4.T 5.dB/dt 6.Pv)\n",
    "normsDict = {\n",
    "    \"Material A\": [[-4.02296069e-19, 6.42790612e-02],\n",
    "                   [1.15118525e-01, 1.22041107e+01],\n",
    "                   [5.16368866e+00, 2.68540382e-01],\n",
    "                   [5.52569885e+01, 2.61055470e+01],\n",
    "                   [2.42224485e-01, 2.37511802e+00],\n",
    "                   [4.94751596e+00, 8.27844262e-01]],\n",
    "    \"Material B\": [[6.75135623e-20, 6.27030179e-02],\n",
    "                   [3.95575739e-02, 7.62486081e+00],\n",
    "                   [5.26432657e+00, 2.88519919e-01],\n",
    "                   [5.80945930e+01, 2.40673885e+01],\n",
    "                   [2.72521585e-01, 2.46433449e+00],\n",
    "                   [5.05083704e+00, 7.10303366e-01]],\n",
    "    \"Material C\": [[-7.61633305e-19, 7.95720905e-02],\n",
    "                   [1.11319124e-01, 1.30629103e+01],\n",
    "                   [5.18559408e+00, 2.68714815e-01],\n",
    "                   [5.84123573e+01, 2.40717468e+01],\n",
    "                   [3.26634765e-01, 3.03949690e+00],\n",
    "                   [4.74633312e+00, 8.05532336e-01]],\n",
    "    \"Material D\": [[-3.82835526e-18, 8.10498434e-02],\n",
    "                   [-1.14488902e-02, 2.83868927e+01],\n",
    "                   [5.25141287e+00, 2.50821203e-01],\n",
    "                   [6.72413788e+01, 2.59518223e+01],\n",
    "                   [3.00584078e-01, 3.24369454e+00],\n",
    "                   [5.01819372e+00, 8.41059685e-01]],\n",
    "    \"Material E\": [[-4.22607249e-18, 1.28762770e-01],\n",
    "                   [3.88389004e-01, 4.80431443e+01],\n",
    "                   [5.18909550e+00, 2.77695119e-01],\n",
    "                   [5.64505730e+01, 2.46127701e+01],\n",
    "                   [6.35038793e-01, 5.19237566e+00],\n",
    "                   [5.68955612e+00, 7.26979315e-01]]\n",
    "}\n",
    "\n",
    "\n",
    "# %% Magnetization mechansim-determined neural network\n",
    "class MMINet(nn.Module):\n",
    "    \"\"\"\n",
    "     Parameters:\n",
    "      - hidden_size: number of eddy current slices (RNN neuron)\n",
    "      - operator_size: number of operators\n",
    "      - input_size: number of inputs (1.B 2.dB 3.dB/dt)\n",
    "      - var_size: number of supplenmentary variables (1.F 2.T)        \n",
    "      - output_size: number of outputs (1.H)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 Material,\n",
    "                 hidden_size=30,\n",
    "                 operator_size=30,\n",
    "                 input_size=3,\n",
    "                 var_size=2,\n",
    "                 output_size=1):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.var_size = var_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.operator_size = operator_size\n",
    "        self.norm = normsDict[Material]  # normalization data\n",
    "\n",
    "        # Consturct the network\n",
    "        self.rnn1 = StopOperatorCell(self.operator_size)\n",
    "        self.dnn1 = nn.Linear(self.operator_size + 2, 1)\n",
    "        self.rnn2 = EddyCell(4, self.hidden_size, output_size)\n",
    "        self.dnn2 = nn.Linear(self.hidden_size, 1)\n",
    "\n",
    "        self.rnn2_hx = None\n",
    "\n",
    "    def forward(self, x, var, s0, n_init=16):\n",
    "        \"\"\"\n",
    "         Parameters: \n",
    "          - x(batch,seq,input_size): Input features (1.B, 2.dB, 3.dB/dt)  \n",
    "          - var(batch,var_size): Supplementary inputs (1.F 2.T)\n",
    "          - s0(batch,1): Operator inital states\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)  # Batch size\n",
    "        seq_size = x.size(1)  # Ser\n",
    "        self.rnn1_hx = s0\n",
    "\n",
    "        # Initialize DNN2 input (1.B 2.dB/dt)\n",
    "        x2 = torch.cat((x[:, :, 0:1], x[:, :, 2:3]), dim=2)\n",
    "\n",
    "        for t in range(seq_size):\n",
    "            # RNN1 input (dB,state)\n",
    "            self.rnn1_hx = self.rnn1(x[:, t, 1:2], self.rnn1_hx)\n",
    "\n",
    "            # DNN1 input (rnn1_hx,F,T)\n",
    "            dnn1_in = torch.cat((self.rnn1_hx, var), dim=1)\n",
    "\n",
    "            # H hysteresis prediction\n",
    "            H_hyst_pred = self.dnn1(dnn1_in)\n",
    "\n",
    "            # DNN2 input (B,dB/dt,T,F)\n",
    "            rnn2_in = torch.cat((x2[:, t, :], var), dim=1)\n",
    "\n",
    "            # Initialize second rnn state\n",
    "            if t == 0:\n",
    "                H_eddy_init = x[:, t, 0:1] - H_hyst_pred\n",
    "                buffer = x.new_ones(x.size(0), self.hidden_size)\n",
    "                self.rnn2_hx = Variable(\n",
    "                    (buffer / torch.sum(self.dnn2.weight, dim=1)) *\n",
    "                    H_eddy_init)\n",
    "\n",
    "            #rnn2_in = torch.cat((rnn2_in,H_hyst_pred),dim=1)\n",
    "            self.rnn2_hx = self.rnn2(rnn2_in, self.rnn2_hx)\n",
    "\n",
    "            # H eddy prediction\n",
    "            H_eddy = self.dnn2(self.rnn2_hx)\n",
    "\n",
    "            # H total\n",
    "            H_total = (H_hyst_pred + H_eddy).view(batch_size, 1,\n",
    "                                                  self.output_size)\n",
    "            if t == 0:\n",
    "                output = H_total\n",
    "            else:\n",
    "                output = torch.cat((output, H_total), dim=1)\n",
    "\n",
    "        # Compute the power loss density\n",
    "        B = (x[:, n_init:, 0:1] * self.norm[0][1] + self.norm[0][0])\n",
    "        H = (output[:, n_init:, :] * self.norm[1][1] + self.norm[1][0])\n",
    "        Pv = torch.trapz(H, B, axis=1) * (10**(var[:, 0:1] * self.norm[2][1] +\n",
    "                                               self.norm[2][0]))\n",
    "\n",
    "        return torch.flatten(Pv)\n",
    "\n",
    "\n",
    "# %% MMINN Sub-layer: Static hysteresis prediction using stop operators\n",
    "class StopOperatorCell():\n",
    "    \"\"\" \n",
    "      Parameters:\n",
    "      - operator_size: number of operator\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, operator_size):\n",
    "        self.operator_thre = torch.from_numpy(\n",
    "            np.linspace(5 / operator_size, 5, operator_size)).view(1, -1)\n",
    "\n",
    "    def sslu(self, X):\n",
    "        \"\"\" Hardsimoid-like or symmetric saturated linear unit definition\n",
    "\n",
    "        \"\"\"\n",
    "        a = torch.ones_like(X)\n",
    "        return torch.max(-a, torch.min(a, X))\n",
    "\n",
    "    def __call__(self, dB, state):\n",
    "        \"\"\" Update operator of each time step\n",
    "\n",
    "        \"\"\"\n",
    "        r = self.operator_thre.to(dB.device)\n",
    "        output = self.sslu((dB + state) / r) * r\n",
    "        return output.float()\n",
    "\n",
    "\n",
    "# %% MMINN subsubnetwork: Dynamic hysteresis prediction\n",
    "class EddyCell(nn.Module):\n",
    "    \"\"\" \n",
    "      Parameters:\n",
    "      - input_size: feature size \n",
    "      - hidden_size: number of hidden units (eddy current layers)\n",
    "      - output_size: number of the output\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size=1):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.x2h = nn.Linear(input_size, hidden_size, bias=False)\n",
    "        self.h2h = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        \"\"\"\n",
    "         Parameters:\n",
    "         - x(batch,input_size): features (1.B 2.dB/dt 3.F 4.T)\n",
    "         - hidden(batch,hidden_size): dynamic hysteresis effects at each eddy current layer \n",
    "        \"\"\"\n",
    "        hidden = self.x2h(x) + self.h2h(hidden)\n",
    "        hidden = torch.sigmoid(hidden)\n",
    "        return hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Material\n",
    "Material = \"Material E\"\n",
    "\n",
    "# Select GPU as default device\n",
    "Device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "#%% Load Dataset\n",
    "def load_dataset(in_file1=\"./Data/Testing/\" + Material + \"/B_Field.csv\",\n",
    "                 in_file2=\"./Data/Testing/\" + Material + \"/Frequency.csv\",\n",
    "                 in_file3=\"./Data/Testing/\" + Material + \"/Temperature.csv\"):\n",
    "\n",
    "    data_B = np.genfromtxt(in_file1, delimiter=',')  # N by 1024, in T\n",
    "    data_F = np.genfromtxt(in_file2, delimiter=',')  # N by 1, in Hz\n",
    "    data_T = np.genfromtxt(in_file3, delimiter=',')  # N by 1, in C\n",
    "\n",
    "    return data_B, data_F, data_T\n",
    "\n",
    "\n",
    "#%% Calculate Core Loss\n",
    "def core_loss(data_B, data_F, data_T):\n",
    "\n",
    "    #================ Wrap your model or algorithm here=======================#\n",
    "    # 1.Create model isntances\n",
    "    net = MMINet(Material).to(Device)\n",
    "\n",
    "    # 2.Load specific model\n",
    "    net_file = \"./Model/\" + Material + \".pt\"\n",
    "    state_dict = torch.load(net_file, map_location=Device)\n",
    "    net.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "    # 3.Get dataloader\n",
    "    loader = get_dataloader(data_B, data_F, data_T, net.norm)\n",
    "\n",
    "    # 4.Validate the models\n",
    "    data_P = torch.Tensor([]).to(\n",
    "        Device)  # Allocate memory to store loss density\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Start model evaluation explicitly\n",
    "        net.eval()\n",
    "\n",
    "        for inputs, vars, s0 in loader:\n",
    "            Pv = net(inputs.to(Device), vars.to(Device), s0.to(Device))\n",
    "\n",
    "            data_P = torch.cat((data_P, Pv), dim=0)\n",
    "        print(data_P.shape)\n",
    "    #=========================================================================#\n",
    "    np.savetxt(\"./Data/Testing/Result/Volumetric_Loss_\" + Material + \".csv\",\n",
    "               data_P.to('cpu'),\n",
    "               delimiter=',')\n",
    "\n",
    "    print('Model inference is finished!')\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_37492\\2678163534.py:27: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(net_file, map_location=Device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3738])\n",
      "Model inference is finished!\n"
     ]
    }
   ],
   "source": [
    "#%% Main Function for Model Inference\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    # Reproducibility\n",
    "    MYSEED = 1\n",
    "    random.seed(MYSEED)  # Random seed\n",
    "    np.random.seed(MYSEED)\n",
    "    torch.manual_seed(MYSEED)  # Data Loading\n",
    "    torch.backends.cudnn.deterministic = True  # Deterministic operations\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    # Generate dataloader\n",
    "    data_B, data_F, data_T = load_dataset()\n",
    "\n",
    "    # Predict and save file\n",
    "    core_loss(data_B, data_F, data_T)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0440,  0.0598,  0.0293,  0.1591, -0.0938,  0.0126,  0.0146,  0.0302,\n",
      "          0.1027, -0.2004,  0.1319, -0.0054,  0.0303,  0.0011,  0.0014,  0.0328,\n",
      "          0.0744,  0.0481,  0.2033,  0.1038, -0.0171, -0.0580,  0.0191, -0.0275,\n",
      "         -0.0079,  0.0572,  0.1541,  0.1448, -0.1241,  0.1583,  0.0309,  0.0690]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.0628], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 1.4944e+00,  5.3623e-01,  9.9983e-01, -3.6611e-02],\n",
      "        [ 9.2726e-02,  9.5585e-02,  8.0337e-02, -8.1617e-02],\n",
      "        [ 1.7293e+00, -4.3700e-01, -1.6699e-01, -1.9062e+00],\n",
      "        [ 2.0573e+00,  8.4392e-02,  3.0469e-02, -1.5226e-01],\n",
      "        [-1.5637e-01,  3.2802e-01,  3.5908e-01, -1.5385e-01],\n",
      "        [ 1.0940e-01, -9.7014e-01,  3.5369e-02, -5.3470e-02],\n",
      "        [-3.6352e+00, -2.3051e-01,  7.1003e-02, -9.1589e-02],\n",
      "        [-2.5176e+00, -1.0482e+00, -1.6136e-01, -6.5170e-02],\n",
      "        [ 4.8711e+00,  9.7124e-02, -6.0954e-01, -2.7885e-01],\n",
      "        [ 3.6330e+00,  5.6055e-01, -4.5089e-02, -2.0845e-01],\n",
      "        [-3.4206e-02,  5.1867e-01,  8.6467e-02,  7.3254e-02],\n",
      "        [-5.6195e-01,  4.9711e-01,  1.7088e-02, -5.6902e-02],\n",
      "        [ 7.5423e+00, -2.8145e-01,  1.8171e-01,  1.0005e-01],\n",
      "        [-1.8292e-01,  5.0520e-01, -1.4598e-01, -4.1222e-01],\n",
      "        [-7.1878e-01, -2.5423e+00, -5.0890e-02, -1.5901e-02],\n",
      "        [-5.9388e+00, -5.0949e-01, -2.7093e-01, -1.1888e-02],\n",
      "        [-1.3388e+00,  1.2101e-01,  1.3332e-01,  9.1486e-02],\n",
      "        [-3.6770e-01,  6.0093e-03, -1.1840e-01, -1.0150e-01],\n",
      "        [-4.7364e-01,  3.4482e-02, -4.9562e-02, -8.4108e-02],\n",
      "        [-9.6542e-02,  4.6652e-01,  1.0574e-01, -1.2855e-02],\n",
      "        [-1.3072e+00,  3.2037e-01,  4.0998e-01, -1.1353e-01],\n",
      "        [-6.8863e-01, -8.6806e-01,  1.9465e-01, -6.1842e-01],\n",
      "        [ 1.7863e+00, -4.9452e-01,  8.6113e-01,  1.3754e-02],\n",
      "        [ 1.0510e+00, -1.3780e+00, -2.2005e-01, -3.2523e-02],\n",
      "        [-5.5902e-01, -7.5538e-01, -2.6149e-02,  4.9179e-02],\n",
      "        [-1.2631e+00, -2.1451e-01,  2.5432e-01,  5.8885e-02],\n",
      "        [-1.2314e-01,  3.4275e-01, -2.2639e+00, -6.6626e-02],\n",
      "        [ 6.0756e+00, -7.1592e-04,  1.4184e-01,  2.6877e-02],\n",
      "        [-1.3257e-01, -7.3667e-01,  2.0892e-01,  1.4173e-01],\n",
      "        [ 2.2029e+00,  1.2881e+00,  9.2061e-02, -4.1587e-02]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 8.1533e-01, -1.2761e+00,  7.0841e-02, -1.8962e+00,  1.7234e+00,\n",
      "         -1.3427e-01, -1.1929e-01, -5.8848e+00,  2.3637e-01, -2.9163e+00,\n",
      "         -1.2310e-02, -3.0190e-01, -6.6412e-01, -2.0177e+00, -7.8205e-01,\n",
      "          8.0959e-01, -9.6627e-02,  5.4707e+00, -6.8099e-01,  6.2250e-01,\n",
      "         -1.5490e-01, -7.4831e-02,  9.8117e-02, -4.8413e+00, -1.2204e+00,\n",
      "         -1.5051e+01,  2.7743e-02, -9.0282e-01, -4.4132e-02, -8.5504e-01],\n",
      "        [-3.2767e-01,  3.2568e+00, -2.7148e-01,  6.5608e-01, -6.0368e-01,\n",
      "          1.0860e+00, -2.3220e-01, -1.1663e+00, -2.1300e-01, -4.1589e-01,\n",
      "          1.6051e-02, -1.1773e+00, -2.2872e-02, -9.3973e-01,  8.5986e-01,\n",
      "          4.9921e-01,  3.4626e-01,  1.2825e+01,  9.5685e+00,  5.1049e+01,\n",
      "         -1.1192e+00,  2.3808e+00,  2.5437e+00,  8.7551e-01, -1.4543e+01,\n",
      "         -1.2450e+01, -2.6576e-01, -2.8376e-01,  2.3534e-01,  3.8967e+00],\n",
      "        [ 8.6397e-01,  1.0098e+00, -7.4006e-01, -5.6548e-01,  1.8362e+00,\n",
      "          3.2306e-01,  7.8310e-01,  6.3570e-01, -1.0328e+00, -7.2565e-01,\n",
      "         -3.6878e-01, -4.4237e-02, -5.2548e-01, -1.6348e+00, -9.2997e-02,\n",
      "         -1.1148e+00,  1.0279e+00, -1.3652e+00,  5.9497e-01,  6.0966e-01,\n",
      "          9.0158e-01, -1.2377e-01,  1.4734e+00, -5.4841e-01, -1.6637e+00,\n",
      "          8.4528e-01, -1.2906e-01,  5.4128e-01,  1.1175e+00, -6.7846e-01],\n",
      "        [ 2.5144e+00, -1.0997e+00, -5.0434e-01,  2.6456e-01, -1.5902e+00,\n",
      "          1.0657e+00, -9.4087e-01,  1.8792e+00, -2.4813e+00,  1.5350e-01,\n",
      "         -1.0758e-01, -2.0190e-02, -8.6606e-01, -2.2547e-02,  2.2077e-01,\n",
      "         -5.2451e-01, -1.1056e+00,  2.9546e+00,  2.3093e-01, -5.3113e-01,\n",
      "          1.0129e+00, -5.1352e-01,  5.9621e-01, -5.7517e-01,  9.5249e-01,\n",
      "          4.4545e-01,  1.2173e-01, -4.3591e-02,  3.7165e-02, -1.3923e-01],\n",
      "        [-5.6803e-01, -2.5934e+00, -5.8122e-01, -1.1354e-03,  2.6962e-01,\n",
      "          5.9413e-01,  1.5157e+00,  1.0256e-01, -1.5278e+00,  2.8091e-01,\n",
      "         -4.4708e-01, -6.0847e-02,  6.9477e-01,  8.8091e-01, -5.0895e-01,\n",
      "          2.4086e-02,  2.1277e-01,  1.0865e+00, -7.0956e-01, -1.7958e-01,\n",
      "          1.0850e+00, -4.2882e-01,  7.7511e-01, -8.8483e-01,  1.9921e+00,\n",
      "          1.2050e+00, -7.3276e-01,  1.5212e-01,  1.3282e+00, -4.5503e+00],\n",
      "        [ 1.5745e+00,  4.8550e-01, -4.7397e-01, -2.3936e-01,  2.3461e+00,\n",
      "          2.0626e+00, -5.2726e-01, -7.4633e-01, -5.8844e-03, -3.1087e-01,\n",
      "         -3.2364e+00, -1.0531e+00, -1.1649e+00, -4.7755e-02,  1.3584e+00,\n",
      "         -8.0517e-01, -4.8564e-01, -2.1195e-01,  1.0881e+00,  9.3661e-02,\n",
      "          4.0913e-02, -4.8261e-01, -5.2552e-01,  1.0872e+00,  3.3493e+00,\n",
      "         -3.7467e-01,  1.4408e-02, -8.6427e-01, -1.0977e+00, -1.4927e-02],\n",
      "        [-3.4669e-01, -6.3838e-01, -7.8134e-01,  5.4312e+00,  2.7636e+00,\n",
      "         -3.1106e-01,  9.8728e-01,  5.8523e+01,  1.4961e+00, -2.3972e-01,\n",
      "          1.2161e+00,  2.2946e+00, -2.5768e-01,  1.6053e+00,  1.7107e-01,\n",
      "          3.6741e+00, -9.9152e-01,  6.7685e+00, -2.3905e+00,  2.8057e+00,\n",
      "         -3.3146e+00,  6.0374e-01, -5.2179e-02,  7.5886e-01,  7.6340e-01,\n",
      "         -4.2034e+00, -9.2421e-02, -3.7824e-01, -4.5547e+00,  6.1790e-02],\n",
      "        [-1.2308e+00,  4.4008e-01, -4.0822e-01,  2.7017e+00,  1.2838e+00,\n",
      "         -9.8813e-01, -5.6306e-01, -4.7449e-01, -1.7378e+00, -9.9867e-01,\n",
      "          9.4115e-01,  6.6801e-01, -2.8323e+00, -2.4554e-01,  8.0977e-02,\n",
      "          9.2189e-01, -1.6719e+00, -2.7107e+00, -1.5070e+00, -1.6710e+00,\n",
      "          3.0886e-01, -5.2183e-02,  1.3163e+00, -1.3037e+01, -4.5072e-01,\n",
      "          5.2137e-01, -7.2763e-01, -8.1356e+00, -4.2747e+00, -1.4118e-01],\n",
      "        [-1.6427e+00,  1.8904e-01, -1.0997e+00, -1.3158e+00, -9.8038e-01,\n",
      "          1.1778e-01,  1.0035e+00, -3.0941e-01,  4.5255e-01, -2.6494e-01,\n",
      "         -1.8923e+00, -1.2143e+00,  5.2840e-01, -8.4077e-01, -8.3473e-01,\n",
      "          7.5394e-01,  5.8609e-01, -2.4662e+00,  2.1307e+00,  2.0993e+00,\n",
      "          1.3709e+00, -4.9948e-01, -2.6295e+00, -1.6814e+00,  7.3551e-01,\n",
      "          1.1014e+01,  1.6651e+00,  2.9555e-01,  8.1610e-01, -1.5663e+00],\n",
      "        [-1.0071e+00, -7.8701e-01,  2.0781e-01,  1.4038e+00,  2.2065e+00,\n",
      "         -1.2959e+00,  5.4550e-01, -4.2794e+00, -3.7476e-01,  2.3473e+00,\n",
      "         -1.5615e+00, -8.6586e-01,  2.1108e+00, -1.6383e+00, -1.6001e+00,\n",
      "         -9.1761e-01,  7.2305e-01,  2.8816e+00, -2.3596e+00, -3.4838e+00,\n",
      "          2.1369e+00,  3.5184e-01,  1.3776e+00, -1.0227e+00, -8.6694e-01,\n",
      "          5.1474e+00, -1.0905e+00,  1.0205e+00,  3.9638e+00, -3.3823e-01],\n",
      "        [-8.8928e-01, -2.7786e-02, -1.6098e-01,  9.1654e-01, -6.1279e-01,\n",
      "         -4.0134e+00, -2.4763e-01,  4.0434e-01,  3.3174e-01,  6.4668e-01,\n",
      "          2.0034e+00, -2.0869e-01,  5.2446e-01,  7.7083e-01, -7.5417e-01,\n",
      "          5.4658e-01, -2.1160e-01,  2.3294e+00, -2.0513e+00,  5.1912e+00,\n",
      "         -5.1832e-02, -7.8412e-01, -2.4443e+00,  9.9778e-01, -2.9705e+00,\n",
      "         -3.3859e+00, -2.1474e-01, -5.8143e-01,  6.4751e-01, -9.4582e-02],\n",
      "        [-6.3719e+00,  8.4562e-02,  3.1684e-01,  3.5523e+00, -1.6836e+00,\n",
      "         -6.0083e-01, -1.7169e-04, -3.3159e+00,  3.5873e-02,  8.1190e-01,\n",
      "         -9.3363e-01,  1.6308e+00, -9.5755e-01, -5.8474e-03,  1.1704e+00,\n",
      "         -8.2773e-02,  6.6388e-01, -1.6464e+00,  9.0595e-02,  6.3873e-02,\n",
      "          1.6877e+00,  7.9980e-01,  1.7300e+00, -1.8035e+00, -3.1169e+00,\n",
      "         -4.5239e-01,  2.0940e-02, -1.8336e+00, -2.9473e+00,  7.3545e-01],\n",
      "        [-5.1692e-01, -2.2449e-01,  2.1794e-01, -3.3301e+00, -7.8760e-01,\n",
      "          9.2408e-01,  1.0638e+00,  2.3925e+00,  5.0658e-01, -4.1581e-01,\n",
      "         -2.0839e+00, -1.0244e+00,  7.0071e-03,  1.9425e-01, -7.5721e-02,\n",
      "         -8.5979e-01,  1.3886e+00,  2.3507e+00,  1.6114e+00, -1.0789e+00,\n",
      "         -5.9233e-01,  2.3867e-02,  1.3601e+00, -9.6076e-01,  3.6081e+00,\n",
      "          3.1525e+00, -2.4359e-01, -1.3382e-01,  1.2576e+00,  3.4277e-02],\n",
      "        [-3.3093e-01, -2.4377e+00,  6.2220e-01, -4.2017e-01,  2.0448e+00,\n",
      "         -8.0701e+00,  4.0938e-01,  6.0955e-01,  5.1044e-01,  4.3154e-01,\n",
      "          8.3132e-01, -8.7030e-01, -1.7518e-01,  1.5827e+00, -1.8785e-01,\n",
      "          5.3646e-01,  1.7142e+00,  3.2412e+00,  1.2259e-01, -3.7428e+00,\n",
      "         -1.3721e+00, -1.5774e+00,  3.0544e-01, -1.4257e-01, -2.7884e+00,\n",
      "          2.4637e+00, -7.3695e-01,  1.8302e-01, -2.1143e-01,  4.8189e-01],\n",
      "        [ 9.1862e-01,  4.6873e-01,  1.6123e-01, -3.2934e-02, -9.5769e-01,\n",
      "          5.8039e-01, -8.5634e-02,  2.9662e-01,  3.2446e-01, -1.3465e+00,\n",
      "         -1.2983e+00,  5.9637e-01, -6.8698e-01, -8.5070e-02,  1.5358e+00,\n",
      "         -7.5037e-01,  1.4014e+00, -6.3905e+00,  3.7004e+00, -4.5542e+00,\n",
      "          1.1981e+00, -3.0634e-01,  1.1968e+00,  2.5167e+00, -1.9765e+00,\n",
      "          2.0322e+00, -8.6924e-01,  1.1561e+00,  5.5202e-01, -7.3370e-01],\n",
      "        [-1.7896e+00,  1.7030e-01,  6.2238e-01, -8.7136e+00, -1.3951e+00,\n",
      "         -5.6629e-01,  1.3555e+00, -3.0083e-03, -4.5208e-01,  1.0741e+00,\n",
      "          1.5594e-01,  9.5597e-02, -7.8554e-01, -7.5261e-01,  6.9634e-01,\n",
      "          2.3240e+00,  3.0387e-01,  5.2453e+00,  6.4792e-01, -8.4978e-01,\n",
      "         -1.7173e+00, -1.6184e+00,  4.3220e+00, -1.3721e+00, -1.8117e+00,\n",
      "         -2.5088e+00,  1.2843e+00, -1.6568e+00,  1.7594e+00, -3.3118e+00],\n",
      "        [ 8.0714e-01, -9.1902e-01, -1.9737e-01, -2.0036e+00, -5.8313e-01,\n",
      "         -1.8998e+00, -7.8650e-01, -1.1345e-01,  9.3054e-01, -3.5092e-02,\n",
      "          2.9534e-01,  1.0591e-01,  5.0298e-01, -5.2474e-01,  7.2831e-02,\n",
      "          4.7670e-01,  1.2457e+00,  1.1102e+00,  7.9969e-01, -9.8578e-02,\n",
      "          1.8138e+00, -7.4107e-01, -2.4723e+00,  1.1938e+00, -2.3046e+00,\n",
      "          1.2220e+00,  1.5624e-01,  8.8354e-01,  2.9048e-01,  1.6071e-01],\n",
      "        [-2.4443e+00,  4.0964e-02,  4.7678e-02, -5.8547e+00, -4.3044e+00,\n",
      "          3.4636e+00, -1.2366e+00, -4.0798e+00, -2.5986e+00,  1.3399e+00,\n",
      "          2.3317e+00, -2.9834e-01, -1.3906e+00, -2.1703e+00, -5.5991e-01,\n",
      "          2.3684e-02,  8.7992e-01,  1.5316e+00, -2.9323e-01, -1.3970e+00,\n",
      "          8.5975e-02, -9.0343e-01,  3.4343e+00, -2.8409e+00,  1.9962e+00,\n",
      "          2.7906e+00, -7.6302e-01, -1.2201e+00,  4.8503e+00, -1.5062e-01],\n",
      "        [-1.6679e+00, -2.2871e-01, -1.1622e-01,  1.8957e+00, -5.6318e-01,\n",
      "          4.3298e-01, -9.1230e-01, -1.1999e+00, -4.9965e-02, -1.7740e-01,\n",
      "         -7.0262e+00,  5.5785e-01, -9.6434e-01, -3.4912e+00,  6.3938e-01,\n",
      "         -9.0164e-01,  4.4675e-01,  1.5505e+00,  1.5690e+00,  6.3396e+00,\n",
      "         -6.3680e-01, -3.8826e-01,  4.8421e-01,  1.7529e+00,  1.0343e+00,\n",
      "          3.4715e+00, -2.9112e-02,  6.4868e-01, -1.5672e+00, -3.1852e-01],\n",
      "        [-6.1307e-01, -2.0896e+00,  5.8100e-02,  1.8925e+00, -1.3237e+00,\n",
      "          1.8090e+00,  6.2935e-01, -1.7729e+00,  1.1400e+00,  2.4541e-01,\n",
      "         -3.0918e+01, -4.1850e-01, -3.7587e-01,  3.0477e+00,  3.3057e-01,\n",
      "         -2.1702e+00,  1.8352e-01, -2.1485e+00,  2.2823e+00,  2.4168e+00,\n",
      "          2.4452e-01, -1.9993e+00, -1.1993e+00,  1.5056e+00, -1.1017e-02,\n",
      "         -1.2060e-01,  2.8786e-01,  1.5882e-02, -8.6130e+00, -1.7674e+00],\n",
      "        [ 1.0634e+00, -5.8408e-01, -1.7832e+00,  6.4184e+00,  6.3360e-01,\n",
      "         -2.2307e+00, -1.7863e+00, -4.2107e+00,  7.3751e-01, -1.1627e+00,\n",
      "          1.3915e+00, -8.0328e-01,  8.9539e-01, -2.6283e+00,  1.3284e+00,\n",
      "          3.7165e+00,  1.5933e-01, -6.1694e+00,  1.7108e+00, -8.0647e-01,\n",
      "          9.9683e-01,  4.5599e-01, -6.4624e+00, -3.3409e+00, -3.3089e+00,\n",
      "         -7.2322e-01, -3.1034e+00, -1.6695e+00, -2.8740e+00,  1.0834e+00],\n",
      "        [ 9.1681e-01,  2.1957e+00, -1.6269e-01,  2.3821e+00, -2.8673e+00,\n",
      "          2.8111e-01,  3.4535e-01, -1.2493e+00, -7.7025e-01,  5.4439e-01,\n",
      "         -1.3287e+00,  1.0200e+00, -4.3346e-01, -1.4593e+00, -1.2140e-01,\n",
      "         -6.3084e-02,  2.7503e-01,  2.7457e+00,  6.9804e+00,  8.8154e+01,\n",
      "         -7.2141e-01,  3.3335e-01,  7.1720e+00, -1.9490e+00, -5.1474e+00,\n",
      "         -1.7824e+01, -6.5245e-02,  5.0055e-03,  2.6247e+00,  9.1493e-01],\n",
      "        [ 8.8133e-01,  1.3160e+00, -1.3751e+00, -1.8126e+00,  2.2154e+00,\n",
      "         -1.1745e+00, -6.6128e-01, -6.5395e+00,  8.9134e-01, -8.4662e-01,\n",
      "         -1.2998e+00,  1.3819e+00, -1.8725e+00, -1.8168e+00, -8.5215e-01,\n",
      "         -5.5885e-01, -1.2617e+00, -1.6194e+00, -1.1229e-01, -2.3764e+00,\n",
      "         -2.7641e+00, -1.4130e+00, -2.1650e+00,  1.5627e+00, -1.1958e+00,\n",
      "          7.7793e+00,  1.0704e+00, -1.0885e+00, -7.3952e+00,  1.1692e+00],\n",
      "        [-6.7824e+00, -5.3358e-01, -3.2665e-01,  5.8277e+00, -1.3969e+00,\n",
      "          8.6082e-01, -1.3624e-01,  2.7366e+00, -1.5747e+00, -1.7258e+00,\n",
      "          2.4333e-01, -1.2439e+00, -2.6623e+00, -6.2477e-01,  8.5517e-01,\n",
      "         -4.9576e-01, -3.1404e-01,  1.8559e+00, -1.5382e+00,  4.0021e+00,\n",
      "         -1.1376e+00, -3.5109e-01, -1.5208e+00,  1.1279e-01,  1.1617e+00,\n",
      "          2.7949e+00, -3.1304e-01, -1.8788e+00, -4.0366e+00,  5.6735e-01],\n",
      "        [ 1.7372e+00, -2.2800e-01, -2.2433e-02,  3.7682e+00,  4.6444e-01,\n",
      "          6.7146e-01, -5.2631e-01,  1.1461e+00, -2.4319e-01, -6.3394e-01,\n",
      "          1.7271e-01, -8.5555e-01, -1.1882e+00, -2.9570e+00, -5.2610e-01,\n",
      "         -1.2832e+00, -2.6294e+00,  5.3269e-01,  1.2933e+00,  1.5207e+00,\n",
      "         -7.7907e-01, -4.7126e-01,  1.1466e+00,  8.5960e-01, -5.0190e-01,\n",
      "          1.8290e+00, -3.5148e-01, -1.3434e+00, -6.5845e-01,  1.3084e-01],\n",
      "        [ 2.9361e-02, -1.6364e+00,  4.6946e-01, -5.2175e+00, -3.1072e+00,\n",
      "          3.7915e-01, -5.6234e-01,  4.1538e-01, -4.7007e-01, -7.7463e+00,\n",
      "         -5.0514e+00, -1.2104e+00,  9.7981e-01,  3.2783e+00, -5.7631e-01,\n",
      "         -6.2655e-01,  1.3447e+00,  1.5757e+00, -9.2112e-01,  1.6222e+00,\n",
      "         -3.0841e+00, -9.0632e-01,  2.1072e+00, -1.2415e+00,  1.7507e+00,\n",
      "          6.2466e-01, -8.8960e-01,  8.6472e-02,  9.6632e-01, -4.1570e-01],\n",
      "        [ 3.3604e-01,  1.1696e+00,  6.8724e-01, -3.7370e+00, -3.6198e-01,\n",
      "          3.1763e+00, -3.5778e-01,  1.1352e+01,  5.5379e-01, -2.6145e+00,\n",
      "         -6.7301e-01,  6.0061e-01, -5.2961e-01, -1.5507e+00,  2.4011e+00,\n",
      "          1.1252e+00,  8.5430e-01,  5.6028e+00,  4.7211e-01, -2.6136e+00,\n",
      "          5.6326e+00,  7.8996e-01,  2.0220e+00, -2.8867e+00, -7.6291e-01,\n",
      "          1.5089e+01,  2.9961e+00, -3.0744e+00,  3.1574e+00, -1.7679e-01],\n",
      "        [-2.2275e+00, -1.0803e+00, -3.7899e-01,  1.1738e+00, -1.7216e+00,\n",
      "          8.2484e-01, -5.8314e-01, -5.4560e+00,  1.1554e+00, -1.0068e+00,\n",
      "         -1.6404e+00, -8.1370e-01,  4.7992e-01, -2.9637e+00,  8.4415e-01,\n",
      "          6.5274e-01, -1.0773e+00,  4.2036e+00,  1.4239e+00, -5.7388e+00,\n",
      "         -1.2403e+00,  8.2520e-01, -4.0344e+00, -1.4418e+00,  2.6176e-01,\n",
      "          7.6552e+00,  3.6639e-01,  2.3085e+00,  1.6166e+00, -4.6922e-01],\n",
      "        [ 3.3426e-01, -2.7808e+00,  2.3661e-01,  5.7634e-04, -1.4660e+00,\n",
      "          2.8729e+00,  1.5311e+00,  5.0807e+00,  8.5518e-01, -5.3010e-02,\n",
      "          1.4562e+00, -1.7720e+00, -7.5354e-01, -7.1592e-01, -4.6340e+00,\n",
      "         -2.9847e+00, -1.0673e+00,  2.1167e+00, -1.8145e+00, -1.5161e+01,\n",
      "         -4.7333e+00, -6.4592e-01, -2.3472e+00, -4.4856e+00,  5.6827e+00,\n",
      "          5.7626e+00, -3.0617e-01, -4.9809e-01,  4.5537e+00,  8.5137e-01],\n",
      "        [-3.2654e+00,  1.3476e+00, -5.6029e-02, -5.4377e+00, -5.9861e+00,\n",
      "          2.6877e+00, -3.2447e+00, -2.5020e+00, -5.7243e-01, -6.6575e-01,\n",
      "         -2.6873e+00,  9.3848e-02, -7.8891e-01, -1.3552e+00,  9.3177e-01,\n",
      "         -1.2024e+00,  1.7633e-01, -2.4765e+00,  5.5315e-01, -1.5535e+00,\n",
      "         -1.9614e+00,  8.6536e-01, -1.3882e+00,  2.9330e+00, -6.2646e-01,\n",
      "          1.6452e+00,  1.6551e-01,  3.3173e-02, -1.1754e+01,  3.4283e+00]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 3.1975e-01, -3.0664e-01,  1.8788e-01,  1.1610e+00,  2.6765e-01,\n",
      "         -3.1599e-01, -4.2886e-01, -5.1950e-01,  2.8809e-01,  6.0154e-03,\n",
      "          3.9876e-01,  2.1376e-01,  3.6953e-01,  1.6138e-01,  8.1620e-02,\n",
      "          2.0796e-02, -4.0256e-01, -1.0628e-01, -3.9726e-01,  5.8846e-02,\n",
      "         -5.1845e-04, -5.7074e-02, -2.4032e-01,  6.1672e-01, -9.4004e-01,\n",
      "         -1.1178e+00,  8.8886e-03,  4.6138e-02, -5.4089e-01,  9.5447e-02]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.0916], device='cuda:0', requires_grad=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_37492\\793971333.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(net_file, map_location=Device)\n"
     ]
    }
   ],
   "source": [
    "# net = MMINet(Material).to(Device)\n",
    "# net_file=\"./Model/\"+Material+\".pt\"\n",
    "# state_dict = torch.load(net_file, map_location=Device)\n",
    "# net.load_state_dict(state_dict,strict=True)\n",
    "\n",
    "# for parm in net.parameters():\n",
    "#     print(parm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
