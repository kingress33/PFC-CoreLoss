{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Package & Hyperparameter Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 清空所有變數\n",
    "%reset -f\n",
    "# # 強制 Python 回收記憶體\n",
    "# import gc\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook 環境，跳過切換目錄\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from datetime import datetime\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    os.chdir(os.path.dirname(os.path.abspath(__file__)))\n",
    "except NameError:\n",
    "    print(\"Notebook 環境，跳過切換目錄\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Unified Hyperparameter Configuration\n",
    "class Config:\n",
    "    SEED = 1\n",
    "    NUM_EPOCHS = 1000\n",
    "    BATCH_SIZE = 64\n",
    "    LEARNING_RATE = 0.02  #論文提供\n",
    "    LR_SCHEDULER_GAMMA = 0.99  #論文提供\n",
    "    DECAY_EPOCH = 455\n",
    "    EARLY_STOPPING_PATIENCE = 100\n",
    "    HIDDEN_SIZE = 40\n",
    "    OPERATOR_SIZE = 30\n",
    "    MAXOUT_H = 1\n",
    "\n",
    "\n",
    "# Reproducibility\n",
    "random.seed(Config.SEED)\n",
    "np.random.seed(Config.SEED)\n",
    "torch.manual_seed(Config.SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Material & Number of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "material = \"CH467160\"\n",
    "fix_way = \"uesed_for_PFC_test5\"\n",
    "note = \"Add_NRMSE_MAPE\"\n",
    "note_detail = \"使用準確度儲存最佳模型\"\n",
    "downsample = 1024\n",
    "save_figure = True\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d\")\n",
    "\n",
    "# 訓練情況況\n",
    "plot_interval = 150\n",
    "train_show_sample = 1\n",
    "\n",
    "result_dir = os.path.join(\"results\",\n",
    "                          f\"{timestamp}_{fix_way}_{material}_{note}\")\n",
    "os.makedirs(result_dir, exist_ok=True)\n",
    "\n",
    "# 定義保存模型的路徑\n",
    "model_save_dir = result_dir\n",
    "model_save_path = os.path.join(\n",
    "    model_save_dir, f\"{material}_{fix_way}_{note}_{timestamp}.pt\")  # 定義模型保存檔名\n",
    "\n",
    "figure_save_base_path = result_dir\n",
    "\n",
    "# Select device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing and data loader generate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Preprocess data into a data loader\n",
    "def get_dataloader(data_B,\n",
    "                   data_F,\n",
    "                   data_T,\n",
    "                   data_H,\n",
    "                   data_N,\n",
    "                   data_Hdc,\n",
    "                   data_Duty_P,\n",
    "                   data_Duty_N,\n",
    "                   data_Pcv,\n",
    "                   global_B_max,\n",
    "                   global_H_max,\n",
    "                   n_init=16):\n",
    "\n",
    "    # Data pre-process\n",
    "\n",
    "    # ── 0. 全域設定/降階設定 ──────────────────────────────\n",
    "    eps = 1e-8  # 防止除以 0\n",
    "    if downsample == 1024:\n",
    "        seq_length = 1024  # 單筆波形點數 (不再 down-sample)\n",
    "    else:\n",
    "        seq_length = downsample\n",
    "        cols = np.linspace(0, 1023, seq_length, dtype=int)\n",
    "        data_B = data_B[:, cols]\n",
    "        data_H = data_H[:, cols]\n",
    "\n",
    "    # ── 1. 波形拼接 (補 n_init 點作初始磁化) ────\n",
    "    data_length = seq_length + n_init\n",
    "    data_B = np.hstack((data_B[:, -n_init:], data_B))  # (batch, data_length)\n",
    "    data_H = np.hstack((data_H[:, -n_init:], data_H))\n",
    "\n",
    "    print(\"B shape:\", data_B.shape)\n",
    "    print(\"H shape:\", data_H.shape)\n",
    "    print(\"F shape:\", data_F.shape)\n",
    "    print(\"T shape:\", data_T.shape)\n",
    "    print(\"Hdc shape:\", data_Hdc.shape)\n",
    "    print(\"N shape:\", data_N.shape)\n",
    "    print(\"Duty Pos shape:\", data_Duty_P.shape)\n",
    "    print(\"Duty Neg shape:\", data_Duty_N.shape)\n",
    "    print(\"Pcv shape:\", data_Pcv.shape)\n",
    "\n",
    "    # ── 2. 轉成 Tensor ───────────────────────────\n",
    "    B = torch.from_numpy(data_B).view(-1, data_length, 1).float()  # (B,N,1)\n",
    "    H = torch.from_numpy(data_H).view(-1, data_length, 1).float()\n",
    "    F = torch.log10(torch.from_numpy(data_F).view(-1, 1).float())  # 純量\n",
    "    T = torch.from_numpy(data_T).view(-1, 1).float()\n",
    "    Hdc = torch.from_numpy(data_Hdc).view(-1, 1).float()\n",
    "    N = torch.from_numpy(data_N).view(-1, 1).float()\n",
    "    Duty_P = torch.from_numpy(data_Duty_P).view(-1, 1).float()\n",
    "    Duty_N = torch.from_numpy(data_Duty_N).view(-1, 1).float()\n",
    "    Pcv = torch.log10(torch.from_numpy(data_Pcv).view(-1, 1).float())\n",
    "\n",
    "    # ── 3. 每筆樣本各自找最大幅值 (per-profile scale) ─\n",
    "    # scale_B = torch.max(torch.abs(B), dim=1,\n",
    "    #                     keepdim=True).values + eps  # (B,1,1)\n",
    "    # scale_H = torch.max(torch.abs(H), dim=1, keepdim=True).values + eps\n",
    "\n",
    "    # ── 4. 先計算導數，再除以 scale_B ─────────────\n",
    "    dB = torch.diff(B, dim=1, prepend=B[:, :1])\n",
    "    dB_dt = dB * (seq_length * F.view(-1, 1, 1))  # 真實斜率\n",
    "    # d2B = torch.diff(dB, dim=1, prepend=dB[:, :1])\n",
    "    # d2B_dt = d2B * (seq_length * F.view(-1, 1, 1))\n",
    "\n",
    "    # ── 5. 形成模型輸入 (已經縮放到 [-1,1]) ────────\n",
    "    # in_B = B / scale_B\n",
    "    # out_H = H / scale_H  # 預測目標\n",
    "    # in_dB_dt = dB_dt / scale_B\n",
    "    # 後續發現d2B無改善準確度(可能要多波形種類才有效幫助)，先以輸入0代入\n",
    "    # in_d2B_dt = d2B_dt / scale_B\n",
    "\n",
    "    # *修正成使用全域最大幅值 (ver.250806)\n",
    "    in_B = B / global_B_max\n",
    "    out_H = H / global_H_max\n",
    "    in_dB_dt = dB_dt / global_B_max\n",
    "    in_d2B_dt = torch.zeros_like(in_dB_dt)\n",
    "\n",
    "    # ── 6. 純量特徵：計算 z-score 參數 ─────────────\n",
    "    def safe_mean_std(tensor, eps=1e-8):\n",
    "        m = torch.mean(tensor).item()\n",
    "        s = torch.std(tensor).item()\n",
    "        return [m, 1.0 if s < eps else s]\n",
    "\n",
    "    #  Compute normalization parameters (均值 & 標準差)**\n",
    "    norm = [\n",
    "        safe_mean_std(F),\n",
    "        safe_mean_std(T),\n",
    "        safe_mean_std(Hdc),\n",
    "        safe_mean_std(N),\n",
    "        safe_mean_std(Pcv)\n",
    "    ]\n",
    "\n",
    "    # 用來做test固定標準化參數的\n",
    "    print(\"0.F, 1.T, 2.Hdc, 3.N, 4.Pcv\")\n",
    "    material_name = f\"{material}\"\n",
    "    print(f'\"{material_name}\": [')\n",
    "    for param in norm:\n",
    "        print(f\"    {param},\")\n",
    "    print(\"]\")\n",
    "\n",
    "    # Data Normalization\n",
    "    in_F = (F - norm[0][0]) / norm[0][1]  # F\n",
    "    in_T = (T - norm[1][0]) / norm[1][1]  # T\n",
    "    in_Hdc = (Hdc - norm[2][0]) / norm[2][1]  # Hdc\n",
    "    in_N = (N - norm[3][0]) / norm[3][1]  # N\n",
    "    in_Pcv = (Pcv - norm[4][0]) / norm[4][1]  # Pcv\n",
    "    in_Duty_P = Duty_P  # Duty Pos\n",
    "    in_Duty_N = Duty_N  # Duty Neg\n",
    "\n",
    "    # #   → 方便推論復原，保留 scale_B, scale_H 當作額外純量\n",
    "    # aux_features = torch.cat(\n",
    "    #     (in_F, in_T, in_Hdc, in_N, in_Duty_P, in_Duty_N, in_Pcv,\n",
    "    #      scale_B.squeeze(-1), scale_H.squeeze(-1)),\n",
    "    #     dim=1)\n",
    "\n",
    "    # ── 7. 產生初始 Preisach operator 狀態 s0 ──────\n",
    "    max_B, _ = torch.max(in_B, dim=1)\n",
    "    min_B, _ = torch.min(in_B, dim=1)\n",
    "    # s0 = get_operator_init(in_B[:, 0] - dB[:, 0] / scale_B.squeeze(-1),\n",
    "    #                        dB / scale_B, max_B, min_B)\n",
    "\n",
    "    s0 = get_operator_init(in_B[:, 0] - dB[:, 0] / global_B_max,\n",
    "                           dB / global_B_max, max_B, min_B)\n",
    "\n",
    "    # ── 8. 組合 Dataset ───────────────────────────\n",
    "    # wave_inputs = torch.cat(\n",
    "    #     (\n",
    "    #         in_B,  # ① B\n",
    "    #         dB / scale_B,  # ② ΔB\n",
    "    #         in_dB_dt,  # ③ dB/dt\n",
    "    #         in_d2B_dt),\n",
    "    #     dim=2)  # ④ d²B/dt²   → (B,L,4)\n",
    "\n",
    "    # amps = torch.cat((scale_B.squeeze(-1), scale_H.squeeze(-1)),\n",
    "    #                 dim=1)  # (B,2)\n",
    "\n",
    "    wave_inputs = torch.cat(\n",
    "        (\n",
    "            in_B,  # ① B\n",
    "            dB / global_B_max,  # ② ΔB\n",
    "            in_dB_dt,  # ③ dB/dt\n",
    "            in_d2B_dt),\n",
    "        dim=2)  # ④ d²B/dt²   → (B,L,4)\n",
    "\n",
    "    aux_features = torch.cat((in_F, in_T, in_Hdc, in_N, in_Duty_P, in_Duty_N),\n",
    "                             dim=1)  # (B,4)\n",
    "\n",
    "    amp_B = torch.full((len(B), 1), global_B_max, dtype=torch.float32)\n",
    "    amp_H = torch.full((len(B), 1), global_H_max, dtype=torch.float32)\n",
    "    amps = torch.cat((amp_B, amp_H), dim=1)  # 仍給 RNN2 用\n",
    "\n",
    "    # 這裡把 Pcv（已 z-score）單獨拿出來當另一個 label\n",
    "    target_Pcv = in_Pcv  # (B,1)\n",
    "\n",
    "    full_dataset = torch.utils.data.TensorDataset(\n",
    "        wave_inputs,  # 0  → 模型序列輸入\n",
    "        aux_features,  # 1  → 4 個純量\n",
    "        amps,  # 2  → 幅值係數\n",
    "        s0,  # 3  → Preisach 初始狀態\n",
    "        out_H,  # 4  → 目標 H  (已 scale_H)\n",
    "        target_Pcv)  # 5  → 目標 Pcv (已 z-score)\n",
    "\n",
    "    # ── 9. Train / Valid split & DataLoader ───────\n",
    "    train_size = int(0.8 * len(full_dataset))\n",
    "    valid_size = len(full_dataset) - train_size\n",
    "    train_set, valid_set = torch.utils.data.random_split(\n",
    "        full_dataset, [train_size, valid_size],\n",
    "        generator=torch.Generator().manual_seed(Config.SEED))\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_set,\n",
    "                                               batch_size=Config.BATCH_SIZE,\n",
    "                                               shuffle=True,\n",
    "                                               num_workers=0,\n",
    "                                               pin_memory=True,\n",
    "                                               collate_fn=filter_input)\n",
    "\n",
    "    valid_loader = torch.utils.data.DataLoader(valid_set,\n",
    "                                               batch_size=Config.BATCH_SIZE,\n",
    "                                               shuffle=False,\n",
    "                                               num_workers=0,\n",
    "                                               pin_memory=True,\n",
    "                                               collate_fn=filter_input)\n",
    "\n",
    "    return train_loader, valid_loader, norm\n",
    "\n",
    "\n",
    "# %% Predict the operator state at t0\n",
    "def get_operator_init(B1,\n",
    "                      dB,\n",
    "                      Bmax,\n",
    "                      Bmin,\n",
    "                      max_out_H=Config.MAXOUT_H,\n",
    "                      operator_size=Config.OPERATOR_SIZE):\n",
    "    \"\"\"Compute the initial state of hysteresis operators\"\"\"\n",
    "    s0 = torch.zeros((dB.shape[0], operator_size))\n",
    "    operator_thre = torch.from_numpy(\n",
    "        np.linspace(max_out_H / operator_size, max_out_H,\n",
    "                    operator_size)).view(1, -1)\n",
    "\n",
    "    for i in range(dB.shape[0]):\n",
    "        for j in range(operator_size):\n",
    "            r = operator_thre[0, j]\n",
    "            if (Bmax[i] >= r) or (Bmin[i] <= -r):\n",
    "                if dB[i, 0] >= 0:\n",
    "                    if B1[i] > Bmin[i] + 2 * r:\n",
    "                        s0[i, j] = r\n",
    "                    else:\n",
    "                        s0[i, j] = B1[i] - (r + Bmin[i])\n",
    "                else:\n",
    "                    if B1[i] < Bmax[i] - 2 * r:\n",
    "                        s0[i, j] = -r\n",
    "                    else:\n",
    "                        s0[i, j] = B1[i] + (r - Bmax[i])\n",
    "    return s0\n",
    "\n",
    "\n",
    "def filter_input(batch):\n",
    "    inputs, features, amps, s0, target_H, target_Pcv = zip(*batch)\n",
    "\n",
    "    inputs = torch.stack(inputs)\n",
    "    features = torch.stack(features)\n",
    "    amps = torch.stack(amps)\n",
    "    s0 = torch.stack(s0)\n",
    "    target_H = torch.stack(target_H)[:, -downsample:, :]  # 保留全長\n",
    "    target_Pcv = torch.stack(target_Pcv)  # (B,1)\n",
    "\n",
    "    return inputs, features, amps, s0, target_H, target_Pcv\n",
    "\n",
    "\n",
    "# 溫度頻率不變加入微小的 epsilon\n",
    "def safe_mean_std(tensor, eps=1e-8):\n",
    "    m_tensor = torch.mean(tensor)  # 還是 Tensor\n",
    "    s_tensor = torch.std(tensor)  # 還是 Tensor\n",
    "\n",
    "    m_val = m_tensor.item()  # 第一次轉成 float\n",
    "    s_val = s_tensor.item()\n",
    "    if s_val < eps:\n",
    "        s_val = 1.0\n",
    "    return [m_val, s_val]  # 直接回傳 float\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Network Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Magnetization mechansim-determined neural network\n",
    "\"\"\"\n",
    "    Parameters:\n",
    "    - hidden_size: number of eddy current slices (RNN neuron)\n",
    "    - operator_size: number of operators\n",
    "    - input_size: number of inputs (1.B 2.dB 3.dB/dt 4.d2B/dt)\n",
    "    - var_size: number of supplenmentary variables (1.F 2.T 3.Hdc 4.N 5.Duty_P 6.Duty_N)        \n",
    "    - output_size: number of outputs (1.H)\n",
    "    \n",
    "    只先把d2B/dt考量在EddyCell裡面\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class MMINet(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            norm,\n",
    "            hidden_size=Config.HIDDEN_SIZE,\n",
    "            operator_size=Config.OPERATOR_SIZE,\n",
    "            input_size=4,  # Add d2B(250203)\n",
    "            var_size=6,\n",
    "            output_size=1):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.var_size = var_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.operator_size = operator_size\n",
    "        self.norm = norm  #*這裡改成從外部傳入 norm(250203)\n",
    "\n",
    "        self.rnn1 = StopOperatorCell(self.operator_size)\n",
    "        self.dnn1 = nn.Linear(self.operator_size + self.var_size, 1)\n",
    "        # var_size (F T Hdc N Duty_P Duty_N ) + 3 (B, dB/dt, d2B/dt)\n",
    "        self.rnn2 = EddyCell(var_size + 3, self.hidden_size, output_size)\n",
    "        self.dnn2 = nn.Linear(self.hidden_size, 1)\n",
    "        self.rnn2_hx = None\n",
    "        # var_size=6: 1.F 2.T 3.Hdc 4.N 5.Duty_P 6.Duty_N + 1 for P_prelim\n",
    "        self.loss_mlp = nn.Sequential(nn.Linear(self.var_size + 1, 128),\n",
    "                                      nn.ReLU(), nn.Linear(128, 64), nn.ReLU(),\n",
    "                                      nn.Linear(64, 32), nn.ReLU(),\n",
    "                                      nn.Linear(32, 1))\n",
    "\n",
    "    def forward(self, x, var, amps, s0, n_init=16):\n",
    "        \"\"\"\n",
    "        Parameters: \n",
    "        - x(batch,seq,input_size): Input features (1.B, 2.dB, 3.dB/dt)  \n",
    "        - var(batch,var_size): Supplementary inputs (1.F 2.T 3.Hdc 4.N 5.Duty_P 6.Duty_N) \n",
    "        - s0(batch,1): Operator inital states\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)  # Batch size\n",
    "        seq_size = x.size(1)  # Ser\n",
    "        self.rnn1_hx = s0\n",
    "\n",
    "        # !Initialize DNN2 input (1.B 2.dB/dt 3.d2B)\n",
    "        # x2 = torch.cat((x[:, :, 0:1], x[:, :, 2:3]), dim=2)\n",
    "        # !選取 B, dB/dt, d2B/dt\n",
    "        x2 = torch.cat((x[:, :, 0:1], x[:, :, 2:4]), dim=2)\n",
    "\n",
    "        for t in range(seq_size):\n",
    "            # RNN1 input (dB,state)\n",
    "            self.rnn1_hx = self.rnn1(x[:, t, 1:2], self.rnn1_hx)\n",
    "\n",
    "            # DNN1 input (rnn1_hx,F,T,Hdc,N)\n",
    "            dnn1_in = torch.cat((self.rnn1_hx, var), dim=1)\n",
    "\n",
    "            # H hysteresis prediction\n",
    "            H_hyst_pred = self.dnn1(dnn1_in)\n",
    "\n",
    "            # DNN2 input (B,dB/dt,T,F)\n",
    "            rnn2_in = torch.cat((x2[:, t, :], var), dim=1)\n",
    "\n",
    "            # Initialize second rnn state\n",
    "            if t == 0:\n",
    "                H_eddy_init = x[:, t, 0:1] - H_hyst_pred\n",
    "                buffer = x.new_ones(x.size(0), self.hidden_size)\n",
    "                self.rnn2_hx = Variable(\n",
    "                    (buffer / torch.sum(self.dnn2.weight, dim=1)) *\n",
    "                    H_eddy_init)\n",
    "\n",
    "            #rnn2_in = torch.cat((rnn2_in,H_hyst_pred),dim=1)\n",
    "            self.rnn2_hx = self.rnn2(rnn2_in, self.rnn2_hx)\n",
    "\n",
    "            # H eddy prediction\n",
    "            H_eddy = self.dnn2(self.rnn2_hx)\n",
    "\n",
    "            # H total\n",
    "            H_total = (H_hyst_pred + H_eddy).view(batch_size, 1,\n",
    "                                                  self.output_size)\n",
    "            if t == 0:\n",
    "                output = H_total\n",
    "            else:\n",
    "                output = torch.cat((output, H_total), dim=1)\n",
    "\n",
    "        H = (output[:, n_init:, :])\n",
    "\n",
    "        amp_B = amps[:, 0:1]  # (batch,1)\n",
    "        amp_H = amps[:, 1:2]  # (batch,1)\n",
    "        B_amp = x[:, n_init:, 0:1] * amp_B.unsqueeze(1)\n",
    "        H_amp = output[:, n_init:, :] * amp_H.unsqueeze(1)\n",
    "        P_prelim = torch.trapz(H_amp, B_amp, axis=1) * (10**(\n",
    "            var[:, 0:1] * self.norm[0][1] + self.norm[0][0]))\n",
    "        Pcv_log = torch.log10(P_prelim.clamp(min=1e-12))\n",
    "        Pcv = (Pcv_log - self.norm[4][0]) / self.norm[4][1]\n",
    "        mlp_input = torch.cat((var, Pcv), dim=1)  # (batch, 5)\n",
    "        s = self.loss_mlp(mlp_input)\n",
    "        Pcv_mlp = Pcv + s\n",
    "\n",
    "        return H, Pcv_mlp\n",
    "\n",
    "\n",
    "class StopOperatorCell():\n",
    "\n",
    "    def __init__(self, operator_size):\n",
    "        self.operator_thre = torch.from_numpy(\n",
    "            np.linspace(Config.MAXOUT_H / operator_size, Config.MAXOUT_H,\n",
    "                        operator_size)).view(1, -1)\n",
    "\n",
    "    def sslu(self, X):\n",
    "        a = torch.ones_like(X)\n",
    "        return torch.max(-a, torch.min(a, X))\n",
    "\n",
    "    def __call__(self, dB, state):\n",
    "        r = self.operator_thre.to(dB.device)\n",
    "        output = self.sslu((dB + state) / r) * r\n",
    "        return output.float()\n",
    "\n",
    "\n",
    "class EddyCell(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size=1):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.x2h = nn.Linear(input_size, hidden_size, bias=False)\n",
    "        self.h2h = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        hidden = self.x2h(x) + self.h2h(hidden)\n",
    "        hidden = torch.sigmoid(hidden)\n",
    "        return hidden\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def load_dataset(material, base_path=\"./Data/\"):\n",
    "\n",
    "    in_file1 = f\"{base_path}{material}/train/B_Field.csv\"\n",
    "    in_file2 = f\"{base_path}{material}/train/Frequency.csv\"\n",
    "    in_file3 = f\"{base_path}{material}/train/Temperature.csv\"\n",
    "    in_file4 = f\"{base_path}{material}/train/H_Field.csv\"\n",
    "    in_file5 = f\"{base_path}{material}/train/Volumetric_Loss.csv\"\n",
    "    in_file6 = f\"{base_path}{material}/train/Hdc.csv\"\n",
    "    in_file7 = f\"{base_path}{material}/train/Turns.csv\"\n",
    "    in_file8 = f\"{base_path}{material}/train/Duty_P.csv\"\n",
    "    in_file9 = f\"{base_path}{material}/train/Duty_N.csv\"\n",
    "\n",
    "    data_B = np.genfromtxt(in_file1, delimiter=',')  # N x 1024\n",
    "    data_F = np.genfromtxt(in_file2, delimiter=',')  # N x 1\n",
    "    data_T = np.genfromtxt(in_file3, delimiter=',')  # N x 1\n",
    "    data_H = np.genfromtxt(in_file4, delimiter=',')  # N x 1024\n",
    "    data_Pcv = np.genfromtxt(in_file5, delimiter=',')  # N x 1\n",
    "    data_Hdc = np.genfromtxt(in_file6, delimiter=',')  # N x 1\n",
    "    data_N = np.genfromtxt(in_file7, delimiter=',')  # N x 1\n",
    "    data_Duty_P = np.genfromtxt(in_file8, delimiter=',')  # N x 1\n",
    "    data_Duty_N = np.genfromtxt(in_file9, delimiter=',')  # N x 1\n",
    "\n",
    "    return data_B, data_F, data_T, data_H, data_Pcv, data_Hdc, data_N, data_Duty_P, data_Duty_N\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainLogger:\n",
    "\n",
    "    def __init__(self, exp_name, config_dict, result_dir):\n",
    "        self.exp_name = exp_name\n",
    "        self.result_dir = result_dir\n",
    "        self.config = config_dict\n",
    "        os.makedirs(self.result_dir, exist_ok=True)\n",
    "\n",
    "        self._save_config()\n",
    "        self._write_metadata()\n",
    "\n",
    "    def _save_config(self):\n",
    "        with open(os.path.join(self.result_dir, \"config.json\"), \"w\") as f:\n",
    "            json.dump(self.config, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    def _write_metadata(self):\n",
    "        metadata = {\n",
    "            \"experiment_name\": self.exp_name,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "        with open(os.path.join(self.result_dir, \"meta.json\"), \"w\") as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "\n",
    "    def save_norm_params(self, norm, feature_names=[\"F\", \"T\", \"Hdc\", \"Pcv\"]):\n",
    "        \"\"\"\n",
    "        將標準化參數存成：\n",
    "        {\n",
    "          \"CH467160\": [\n",
    "             [mean_F, std_F],\n",
    "             [mean_T, std_T],\n",
    "             [mean_Hdc, std_Hdc],\n",
    "             [mean_N, std_N],\n",
    "             [mean_Pcv, std_Pcv],\n",
    "          ]\n",
    "        }\n",
    "        \"\"\"\n",
    "        # 從 exp_name 前半段取出 material\n",
    "        material_key = self.exp_name.split('_')[0]\n",
    "\n",
    "        # 直接把 norm (list of [mean, std]) 當成 value\n",
    "        output = {material_key: norm}\n",
    "\n",
    "        # 寫檔\n",
    "        with open(os.path.join(self.result_dir, \"norm_params.json\"), \"w\") as f:\n",
    "            json.dump(output, f, indent=4, ensure_ascii=False)\n",
    "        print(\n",
    "            f\"✅ Normalization parameters saved to {os.path.join(self.result_dir, 'norm_params.json')}\"\n",
    "        )\n",
    "\n",
    "    def save_summary(self, best_epoch, best_val_loss, best_loss_H,\n",
    "                     best_loss_Pcv, model_save_path, elapsed):\n",
    "        summary = {\n",
    "            \"exp_name\": self.exp_name,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"duration_sec\": elapsed,\n",
    "            \"config\": self.config,\n",
    "            \"best_model\": {\n",
    "                \"path\": model_save_path,\n",
    "                \"epoch\": best_epoch,\n",
    "                \"val_loss\": best_val_loss,\n",
    "                \"loss_H\": best_loss_H,\n",
    "                \"loss_Pcv\": best_loss_Pcv\n",
    "            },\n",
    "            \"note\": note,\n",
    "            \"note detail\": note_detail\n",
    "        }\n",
    "        with open(os.path.join(self.result_dir, \"summary.json\"), \"w\") as f:\n",
    "            json.dump(summary, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate clamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clamp_learning_rate(optimizer, min_lr=1e-5):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        if param_group['lr'] < min_lr:\n",
    "            param_group['lr'] = min_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caculate tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_nrmse(y_pred, y_true, eps=1e-9):\n",
    "    \"\"\"\n",
    "    計算 H-field 的歸一化均方根誤差 (Normalized Root Mean Square Error)。\n",
    "    這個指標用來評估波形「形狀」的相似度，數值越低越好。\n",
    "    \"\"\"\n",
    "    # y_pred, y_true 的 shape 都是 (batch, seq_len, 1)\n",
    "    error = torch.sqrt(torch.mean((y_pred - y_true)**2, dim=1))  # (batch, 1)\n",
    "    norm = torch.sqrt(torch.mean(y_true**2, dim=1))  # (batch, 1)\n",
    "\n",
    "    # 計算平均 NRMSE 並轉為百分比\n",
    "    return torch.mean(error / (norm + eps)).item() * 100\n",
    "\n",
    "\n",
    "def calculate_mape(y_pred, y_true, norm_params, eps=1e-9):\n",
    "    \"\"\"\n",
    "    計算 Pcv 的平均絕對百分比誤差 (Mean Absolute Percentage Error)。\n",
    "    這個指標直接反映了損耗預測值的「百分比誤差」，數值越低越好。\n",
    "    \"\"\"\n",
    "    # y_pred, y_true 的 shape 都是 (batch, 1)，並且是經過 log10 和 z-score 處理的\n",
    "    # 步驟 1: 將 z-score 還原成 log10(Pcv)\n",
    "    # norm_params[4] 是 Pcv 的 [mean, std]\n",
    "    pred_log = y_pred * norm_params[4][1] + norm_params[4][0]\n",
    "    true_log = y_true * norm_params[4][1] + norm_params[4][0]\n",
    "\n",
    "    # 步驟 2: 將 log10(Pcv) 還原成真實的 Pcv\n",
    "    pred_real = 10**pred_log\n",
    "    true_real = 10**true_log\n",
    "\n",
    "    # 步驟 3: 計算 MAPE 並轉為百分比\n",
    "    return torch.mean(torch.abs(\n",
    "        (pred_real - true_real) / (true_real + eps))).item() * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(norm, train_loader, valid_loader, logger):\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "    model = MMINet(norm=norm).to(device)\n",
    "    print(\"=== Start Train  ===\")\n",
    "    print(r\"\"\"\n",
    "          \n",
    "          \n",
    "                                                    ⠀⠀⠀⠀⢀⡤⠖⠋⠉⠉⠉⠉⠙⠲⣦⣀⠀⠀⠀⠀⠀\n",
    "                                                    ⠀⠀⠀⡴⠋⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠻⣦⡀⠀⠀⠀\n",
    "                                                    ⠀⠀⡼⢁⡠⢼⠁⠀⢱⢄⣀⠀⠀⠀⠀⠀⠎⢿⡄⠀⠀\n",
    "                                                    ⠀⣸⠁⠀⣧⣼⠀⠀⣧⣼⠉⠀⠀⠀⠀⠀⠐⢬⣷⠀⠀\n",
    "                                                    ⡼⣿⢀⠀⣿⡟⠀⠀⣿⣿⠀⠀⠀⠀⠀⠀⠀⠀⢹⣧⠀\n",
    "                  我好想畢業                         ⣇⢹⠀⠁⠈⠀⠉⠃⠈⠃⠀⠀⠀⠀⠀⠀⠀⠀⡰⢸⡇\n",
    "                                                    ⠙⢿⣧⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⣏⣈⣉⣤⠿⠁\n",
    "                                                    ⠀⣠⣾⣿⠤⡀⠀⠀⠀⠀⠀⢀⣤⣶⣿⣿⣿⣿⣅⠀⠀\n",
    "                                                    ⢰⣧⣿⣿⣿⣦⣉⡐⠒⠒⢲⣿⣿⣿⣿⣿⣿⣶⣿⣧⠀\n",
    "                                                    ⠘⠿⢿⣿⣿⣿⡿⠿⠛⠿⠿⠿⣿⣿⣿⣿⣿⣿⡿⠟⠀\n",
    "                                                    ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠉⠉⠁⠀⠀⠀⠀\n",
    "\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⣾⣷⣄⠀⠀⠀⣀⣤⣤⣤⡀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣶⠏⠀⠀⣿⠀⢀⡾⠛⠋⠀⣾⣿⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡀⡏⠀⠀⠀⣿⢀⣾⠁⠀⣰⠆⢹⡿⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠃⣧⠀⠀⢠⡟⢸⡇⠀⣰⠟⠀⣼⠃⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣀⣹⣆⢀⣸⣇⣸⠃⢠⡏⠀⣸⠋⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⣀⣤⣴⣶⣶⣶⠾⠟⠛⠉⠉⠉⠈⠉⠉⠛⠁⢾⠁⣴⠇⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣀⣤⣤⣶⣶⠾⠟⠛⠛⣻⣿⣙⡁⠀⠀⢾⣶⣾⣷⣿⣶⣄⠀⠀⠀⠀⠰⢿⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⢀⣀⣀⣀⣠⣴⣶⣶⠾⠟⠛⠉⠉⠉⠀⠀⠀⠀⠀⣿⣻⣟⣻⣿⡦⠀⠘⣿⣿⣛⡿⢶⡇⠀⠀⠀⠀⠀⠀⢻⣆⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⣠⣶⣶⣶⣾⣿⣿⣿⣿⣿⣿⣿⣧⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⡟⠙⣿⣿⡗⠀⠀⠿⠉⣿⣿⣿⣶⠀⠀⠀⠀⠀⠀⠈⢿⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⠳⣄⣿⡿⠁⠀⠀⠘⢦⣿⣿⠇⠟⠁⠀⠀⠀⠀⠀⠀⣸⡇⠀⠀⠀⠀⠀⠀⠀\n",
    "⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠁⡇⠀⠀⠀⠀⠀⠀⠀\n",
    "⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢰⣇⠀⠀⠀⠀⠀⠀⠀\n",
    "⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⣿⠀⠀⠀⠀⠀⠀⠀\n",
    "⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡏⠀⠀⠀⠀⠀⠀⠀\n",
    "⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀\n",
    "⢻⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡟⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣇⡇⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠻⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢤⣤⡀⠀⠀⠀⠀⠀⠀⠀⣿⡇⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠈⠙⢿⣿⣿⣿⣿⣿⣿⠟⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣀⣤⡾⠟⠛⠆⠀⠀⠀⠀⠀⢀⢻⡇⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠈⠙⠿⣿⣭⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⣤⣴⣶⠾⠟⠋⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⣾⠇⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠈⠉⠙⠛⠷⠶⢶⣶⣦⣤⣴⡆⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣠⣌⣿⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⣿⡄⠀⠀⠀⠀⠀⠀⠀⠀⠙⠛⠛⠛⠃⠀⠀⠀⠀⠀⠀⠀⣤⣴⣾⣿⣿⣿⣓⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣼⣿⣷⣦⣄⣀⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⣀⣠⣤⣶⣾⣟⣯⣽⠟⠋⠀⠉⠳⣄⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣾⢇⠀⠉⠛⠷⣮⣍⣩⡍⢻⡟⠉⣉⢹⡏⠉⣿⣹⣷⣦⣿⠿⠟⠉⠀⠀⠀⠀⠀⠀⠙⣆⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣰⠏⢸⠇⠀⠀⠀⠀⠀⠉⠉⠛⠛⠛⠛⠛⠛⠛⠋⠉⠉⠀⠀⠀⠀⠀⢠⣠⡶⠀⠀⠀⠀⠘⣧⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢰⡿⠀⣸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠟⠁⠀⠀⠀⠀⠀⠘⣆⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣾⠃⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣾⣇⡀⠀⠀⠀⠀⠀⠀⢹⡆⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⣾⠀⣾⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⢿⣥⢠⣤⠼⠇⠀⠀⠘⣿⡄\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⣽⡄⠈⢿⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⣿⠿⠾⠷⠄⠀⠀⠀⢀⣿⠁\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⣧⠀⠸⣷⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣤⣾⠋⠀⠀⠀⠀⠀⠀⢰⣾⡿⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠙⣦⣠⣿⣿⣶⣶⣤⣤⣄⣀⣀⣀⣀⠀⠀⠀⠀⠀⠀⠀⣀⣀⣠⣴⣿⣇⠀⠀⠀⠀⠀⠀⠀⣸⡟⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⢻⣿⠀⠉⠛⢿⣿⣯⣿⡟⢿⠻⣿⢻⣿⢿⣿⣿⣿⣿⣿⠿⠟⠹⣟⢷⣄⠀⠀⠀⢀⣼⠟⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⣿⣄⠀⠀⠘⢷⣌⡻⠿⣿⣛⣿⣟⣛⣛⣋⣉⣉⣉⣀⡀⠀⠀⠈⠻⢿⣷⣶⣶⢛⣧⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⣏⠀⠀⠀⠀⠹⢯⣟⣛⢿⣿⣽⣅⣀⡀⠀⣀⡀⠀⠀⠀⠠⢦⣀⠰⡦⠀⢸⠀⣏⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢿⡀⠀⠀⠀⠀⠀⠀⠈⠉⢻⣿⡟⠛⠉⠉⠁⠀⠀⠀⠀⠀⠀⠈⠛⠷⠀⣸⠀⣿⡀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⣧⠀⠀⠀⠀⠀⠀⠀⠀⠘⣿⣇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⠀⣿⡇⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠸⢿⠀⠀⢦⡀⡀⠀⠀⠀⠀⢹⣿⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⡄⡏⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⡄⠀⠈⠳⣝⠦⢄⠀⠀⠀⣟⣷⠀⠀⠀⣷⣄⠀⠀⠀⠀⠀⠀⠀⠀⣿⡇⡇⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣄⣷⡀⠀⠀⠈⠙⠂⠀⠀⠀⢸⣿⡄⠀⠀⠘⢦⡙⢦⡀⠀⠀⠀⠀⢰⣷⣷⡇⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢻⡿⢧⣤⣀⡀⠀⠀⠀⠀⠀⠀⢿⣷⣄⠀⠀⠀⠁⠋⠀⠀⠀⠀⠀⢸⣿⣿⣇⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⣷⡀⠈⠉⠛⠛⠛⠛⠛⠛⠛⠛⢿⡍⠛⠳⠶⣶⣤⣤⣤⣤⣤⣤⠼⠟⡟⢿⡇⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⣷⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⣧⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠰⣾⡇⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣠⣤⣴⣿⣷⣶⣶⣶⣶⣶⣶⣦⣀⣀⣀⣻⡀⠀⠀⠀⣀⣀⠀⡀⠀⠀⠀⢀⣼⣿⠇⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣴⠟⠉⠁⠀⠀⠈⠻⣿⡆⢹⣯⣽⣿⣿⠟⠋⠙⣿⣶⣿⣿⣿⣿⣾⣿⣿⣿⣟⠋⠉⣇⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⡇⠀⠀⠀⡀⠀⠀⠀⠈⢻⣆⣿⠀⠀⠀⢁⣶⣿⠿⠟⠛⠷⣶⣽⣿⣿⣻⣏⠙⠃⣴⢻⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠸⣷⣀⠀⠀⠉⠀⠀⠀⠀⠀⢹⣿⠀⣀⣴⣿⠋⠀⠀⠀⠀⠀⠀⠉⠻⣿⣧⣿⢀⣰⣿⣿⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⢿⣶⣶⣤⣤⣤⣤⣤⣤⣾⣿⣟⣿⣿⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⣿⣅⣾⢿⣵⠇⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠉⠛⠛⠛⠛⠛⠛⠛⠛⠉⠉⠉⠁⢹⣜⠷⠦⠤⠤⠤⠤⠤⠴⠶⠛⣉⣱⠿⠁⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠛⠿⠷⣦⣤⣤⣄⣠⣤⣤⡶⠟⠁⠀⠀⠀⠀⠀⠀⠀\n",
    "                        \n",
    "                        \n",
    "                我是Chill guy，這是我的碩論，我真的不會，但沒關係，大不了休學，我超爛\n",
    "                 \n",
    "    \"\"\")\n",
    "    print(\"Number of parameters: \", count_parameters(model))\n",
    "\n",
    "    criterion_H = nn.MSELoss()\n",
    "    criterion_Pcv = nn.MSELoss()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=Config.LEARNING_RATE)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(\n",
    "        optimizer, gamma=Config.LR_SCHEDULER_GAMMA)\n",
    "\n",
    "    # scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    #     optimizer,\n",
    "    #     gamma=Config.LR_SCHEDULER_GAMMA)\n",
    "\n",
    "    # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "    #                                                        mode=\"min\",\n",
    "    #                                                        factor=0.5,\n",
    "    #                                                        patience=50,\n",
    "    #                                                        min_lr=1e-5)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    # -----------Early stopping 紀錄------------------\n",
    "    # Loss 記錄\n",
    "    best_val_loss = float('inf')\n",
    "    # best_loss_Pcv = float('inf')\n",
    "    # best_loss_H = float('inf')\n",
    "    best_nrmse_H = float('inf')\n",
    "    best_mape_Pcv = float('inf')\n",
    "\n",
    "    # patience_counter = 0 # 單純固定的早停計數器\n",
    "    wait_H = wait_Pcv = 0\n",
    "    # MIN_DELTA = 1e-6  # 低進步門\n",
    "    PATIENCE_H = Config.EARLY_STOPPING_PATIENCE\n",
    "    PATIENCE_PCV = Config.EARLY_STOPPING_PATIENCE\n",
    "    joint_phase = False\n",
    "\n",
    "    # 保存每個 epoch 的時間\n",
    "    epoch_times = []\n",
    "    # Logger 紀錄\n",
    "    best_epoch = 0\n",
    "    history = {\n",
    "        \"epoch\": [],\n",
    "        \"train_loss\": [],\n",
    "        \"val_loss\": [],\n",
    "        \"loss_H\": [],\n",
    "        \"loss_Pcv\": [],\n",
    "        \"nrmse_H\": [],\n",
    "        \"mape_Pcv\": []\n",
    "    }\n",
    "\n",
    "    fixed_idx = None  # 用於繪圖時固定的隨機索引\n",
    "\n",
    "    for epoch in range(Config.NUM_EPOCHS):\n",
    "\n",
    "        t0 = time.perf_counter()\n",
    "        alpha = (epoch + 1) / Config.NUM_EPOCHS\n",
    "\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "\n",
    "        for inputs, features, amps, s0, target_H, target_Pcv in train_loader:\n",
    "\n",
    "            inputs, features, amps, s0, target_H, target_Pcv = inputs.to(\n",
    "                device), features.to(device), amps.to(device), s0.to(\n",
    "                    device), target_H.to(device), target_Pcv.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.autocast(device_type=\"cuda\"):\n",
    "                outputs_H, outputs_Pcv = model(inputs, features, amps,\n",
    "                                               s0)  # 模型的輸出\n",
    "                loss_H = criterion_H(outputs_H, target_H)  # 使用真實的 H(t) 計算損失\n",
    "                loss_Pcv = criterion_Pcv(outputs_Pcv, target_Pcv)\n",
    "\n",
    "                loss = (1 - alpha) * loss_H + alpha * loss_Pcv\n",
    "                # alpha = 0.5\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)  # **記錄 Train Loss**\n",
    "\n",
    "        # ------------------------------vaildation------------------------------\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_loss_H = 0.0\n",
    "        val_loss_Pcv = 0.0\n",
    "        all_val_nrmse = []\n",
    "        all_val_mape = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, features, amps, s0, target_H, target_Pcv in valid_loader:\n",
    "                inputs, features, amps, s0, target_H, target_Pcv = inputs.to(\n",
    "                    device), features.to(device), amps.to(device), s0.to(\n",
    "                        device), target_H.to(device), target_Pcv.to(device)\n",
    "                outputs_H, outputs_Pcv = model(inputs, features, amps, s0)\n",
    "                loss_H = criterion_H(outputs_H, target_H)  # 使用真實的 H(t) 計算損失\n",
    "                loss_Pcv = criterion_Pcv(outputs_Pcv, target_Pcv)\n",
    "                loss = (1 - alpha) * loss_H + alpha * loss_Pcv\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                val_loss_H += loss_H.item()\n",
    "                val_loss_Pcv += loss_Pcv.item()\n",
    "\n",
    "                # 算並收集這個 batch 的準確度\n",
    "                all_val_nrmse.append(calculate_nrmse(outputs_H, target_H))\n",
    "                all_val_mape.append(\n",
    "                    calculate_mape(outputs_Pcv, target_Pcv, model.norm))\n",
    "\n",
    "        # 求驗證集平均\n",
    "        val_loss_H /= len(valid_loader)\n",
    "        val_loss_Pcv /= len(valid_loader)\n",
    "        val_loss /= len(valid_loader)\n",
    "        val_losses.append(val_loss)  # 記錄 Validation Loss\n",
    "        avg_val_nrmse_H = np.mean(all_val_nrmse)\n",
    "        avg_val_mape_Pcv = np.mean(all_val_mape)\n",
    "\n",
    "        # # ──────────────學習率更新──────────────\n",
    "        #  ExponentialLR用\n",
    "        # if not joint_phase:\n",
    "        #     # H-phase: 監控 H loss\n",
    "        #     monitored_metric = val_loss_H\n",
    "        #     print(\n",
    "        #         f\"--- [Phase 1: H-focus] Monitoring val_loss_H: {monitored_metric:.6f} ---\"\n",
    "        #     )\n",
    "        # else:\n",
    "        #     # Pcv-phase: 監控 Pcv loss\n",
    "        #     monitored_metric = val_loss_Pcv\n",
    "        #     print(\n",
    "        #         f\"--- [Phase 2: Pcv-focus] Monitoring val_loss_Pcv: {monitored_metric:.6f} ---\"\n",
    "        #     )\n",
    "\n",
    "        # StepLR用\n",
    "        # scheduler.step(monitored_metric)  # 更新學習率\n",
    "        # current_lr = scheduler.get_last_lr()[0]\n",
    "        # print(f\"Epoch {epoch+1} | Learning Rate: {current_lr:.6f}\")\n",
    "\n",
    "        # ExponentialLR用\n",
    "        scheduler.step()  # scheduler 更新\n",
    "        clamp_learning_rate(optimizer)  # 避免learning rate掉到 0\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"Epoch {epoch+1} | Learning Rate: {current_lr:.6f}\")\n",
    "\n",
    "        # # ──────────────學習率更新 END──────────────\n",
    "\n",
    "        # ─────────── 單個epcho輸出資訊 ───────────\n",
    "        epoch_time = time.time() - t0\n",
    "        print(f\"---\")\n",
    "        print(\n",
    "            f\"Epoch {epoch+1} | run time {epoch_time} |  LR: {current_lr:.6f} | alpha: {alpha:.3f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"  Losses -> Train: {train_loss:.6f} | Val: {val_loss:.6f} | Val_H: {val_loss_H:.6f} | Val_Pcv: {val_loss_Pcv:.6f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"  Metrics -> NRMSE_H: {avg_val_nrmse_H:.4f}% | MAPE_Pcv: {avg_val_mape_Pcv:.4f}%\"\n",
    "        )\n",
    "        print(\n",
    "            f\"  (Best @ Epoch {best_epoch} | Best NRMSE_H: {best_nrmse_H:.4f}% | Best MAPE_Pcv: {best_mape_Pcv:.4f}%)\"\n",
    "        )\n",
    "\n",
    "        history[\"epoch\"].append(epoch + 1)\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"loss_H\"].append(val_loss_H)\n",
    "        history[\"loss_Pcv\"].append(val_loss_Pcv)\n",
    "        history[\"nrmse_H\"].append(avg_val_nrmse_H)\n",
    "        history[\"mape_Pcv\"].append(avg_val_mape_Pcv)\n",
    "        # ======================================================繪製訓練情況======================================================\n",
    "\n",
    "        if (epoch + 1) % plot_interval == 0:\n",
    "\n",
    "            # 第一次產生固定的隨機索引\n",
    "            if fixed_idx is None:\n",
    "                batch_size_fix = 3\n",
    "                fixed_idx = torch.randperm(batch_size_fix)[:train_show_sample]\n",
    "\n",
    "            # # -------------------------設定圖表H(t)比較---------------------------------------\n",
    "\n",
    "            # outputs = [fixed_idx, :downsample,\n",
    "            #  0].detach().cpu().numpy()\n",
    "            # targets_np = target_H[fixed_idx, :downsample,\n",
    "            #                       0].detach().cpu().numpy()\n",
    "\n",
    "            # plt.figure(figsize=(12, 6))\n",
    "\n",
    "            # for i in range(outputs.shape[0]):  # 每一批數據繪製一個圖表\n",
    "            #     plt.plot(outputs[i, :, 0],\n",
    "            #              label=f\"Pred: Sample {i+1}\",\n",
    "            #              linestyle='--',\n",
    "            #              marker='o')\n",
    "            #     plt.plot(targets[i, :, 0],\n",
    "            #              label=f\"Target: Sample {i+1}\",\n",
    "            #              linestyle='-',\n",
    "            #              marker='x')\n",
    "\n",
    "            # # 添加標題和標籤\n",
    "            # plt.title(f\"Compare - Epoch {epoch + 1}\", fontsize=16)\n",
    "            # plt.xlabel(\"Index\", fontsize=14)\n",
    "            # plt.ylabel(\"Value\", fontsize=14)\n",
    "            # plt.legend(loc=\"upper right\", fontsize=12)\n",
    "            # plt.grid(alpha=0.5)\n",
    "\n",
    "            # # 顯示圖表\n",
    "            # plt.show()\n",
    "            # # -------------------------設定圖表H(t)比較 結束---------------------------------------\n",
    "\n",
    "            # # -------------------------設定圖表B-H比較---------------------------------------\n",
    "            # 取對應 sample\n",
    "            outputs_np = outputs_H[fixed_idx, -downsample:,\n",
    "                                   0].detach().cpu().numpy()\n",
    "            targets_np = target_H[fixed_idx, -downsample:,\n",
    "                                  0].detach().cpu().numpy()\n",
    "            B_seq_np = inputs[fixed_idx, -downsample:,\n",
    "                              0].detach().cpu().numpy()\n",
    "\n",
    "            # 設定圖表\n",
    "            plt.figure()\n",
    "\n",
    "            for i in range(train_show_sample):  # 每一批數據繪製一個圖表\n",
    "                plt.plot(outputs_np[i],\n",
    "                         B_seq_np[i],\n",
    "                         label=f\"Pred: Sample {i+1}\",\n",
    "                         markersize=1)\n",
    "\n",
    "                plt.plot(targets_np[i],\n",
    "                         B_seq_np[i],\n",
    "                         label=f\"Target: Sample {i+1}\",\n",
    "                         alpha=0.5)\n",
    "\n",
    "            # 添加標題和標籤\n",
    "            plt.title(f\"Compare - Epoch {epoch + 1}\")\n",
    "            plt.xlabel(\"Index\")\n",
    "            plt.ylabel(\"Value\")\n",
    "            plt.grid(alpha=0.5)\n",
    "            plt.legend()\n",
    "            if save_figure == True:\n",
    "                figure_save_path1 = os.path.join(\n",
    "                    figure_save_base_path,\n",
    "                    f\"Compare_Epoch {epoch + 1}.svg\")  # 定義模型保存檔名\n",
    "                plt.savefig(figure_save_path1)\n",
    "            plt.show()\n",
    "            # # -------------------------設定圖表B-H比較 END---------------------------------------\n",
    "        # ======================================================繪製訓練情況  END ======================================================\n",
    "\n",
    "        # ======================================================Early stop======================================================\n",
    "        # # --- 只看整體---\n",
    "        # if val_loss < best_val_loss:\n",
    "        #     best_val_loss = val_loss\n",
    "        #     best_epoch = epoch + 1\n",
    "        #     best_loss_H = val_loss_H\n",
    "        #     best_loss_Pcv = val_loss_Pcv\n",
    "        #     torch.save(model.state_dict(), model_save_path)  # 保存最佳模型\n",
    "        #     print(\n",
    "        #         f\"→Saving model at epoch {epoch+1} with validation loss {val_loss:.6f}...\"\n",
    "        #     )\n",
    "        #     patience_counter = 0\n",
    "        # else:\n",
    "        #     patience_counter += 1\n",
    "        #     print(\n",
    "        #         f\"  無改善，patience_counter = {patience_counter}/{Config.EARLY_STOPPING_PATIENCE}\"\n",
    "        #     )\n",
    "\n",
    "        # if patience_counter >= Config.EARLY_STOPPING_PATIENCE:\n",
    "        #     print(\"Early stopping triggered.\")\n",
    "        #     break\n",
    "\n",
    "        # --- H跟Pcv一個結束就結束 ---\n",
    "        # joint_phase = (alpha >= SWITCH_ALPHA)  # 判斷現在在哪一段\n",
    "\n",
    "        # if not joint_phase:  # ① 只看 H\n",
    "        #     if val_loss_H < best_loss_H - MIN_DELTA:\n",
    "        #         best_loss_H = val_loss_H\n",
    "        #         best_epoch = epoch + 1\n",
    "        #         wait_H = 0\n",
    "        #         torch.save(model.state_dict(), model_save_path)\n",
    "        #         print(f\"✅ Save best H @ epoch {epoch+1}\")\n",
    "        #     else:\n",
    "        #         wait_H += 1\n",
    "        #         print(f\"  H 無改善，wait_H={wait_H}/{PATIENCE_H}\")\n",
    "        #     if wait_H >= PATIENCE_H:\n",
    "        #         print(\"🔸 Early-Stop (H) 觸發\")\n",
    "        #         break\n",
    "        # else:  # ② 只看 Pcv\n",
    "        #     if val_loss_Pcv < best_loss_Pcv - MIN_DELTA:\n",
    "        #         best_loss_Pcv = val_loss_Pcv\n",
    "        #         best_epoch = epoch + 1\n",
    "        #         wait_Pcv = 0\n",
    "        #         torch.save(model.state_dict(), model_save_path)\n",
    "        #         print(f\"✅ Save best Pcv @ epoch {epoch+1}\")\n",
    "        #     else:\n",
    "        #         wait_Pcv += 1\n",
    "        #         print(f\"  Pcv 無改善，wait_Pcv={wait_Pcv}/{PATIENCE_PCV}\")\n",
    "        #     if wait_Pcv >= PATIENCE_PCV:\n",
    "        #         print(\"🔸 Early-Stop (Pcv) 觸發\")\n",
    "        #         break\n",
    "\n",
    "        # # --- H結束接著Pcv ---\n",
    "        # if not joint_phase:  # H-phase\n",
    "        #     if val_loss_H < best_loss_H - MIN_DELTA:\n",
    "        #         best_loss_H = val_loss_H\n",
    "        #         best_loss_Pcv = val_loss_Pcv\n",
    "        #         best_epoch = epoch + 1\n",
    "        #         wait_H = 0\n",
    "        #         torch.save(model.state_dict(), model_save_path)\n",
    "        #         print(f\"✅ Save best H @ epoch {best_epoch}\")\n",
    "        #     else:\n",
    "        #         wait_H += 1\n",
    "        #         print(f\"  H 無改善 wait_H={wait_H}/{PATIENCE_H}\")\n",
    "\n",
    "        #     if wait_H >= PATIENCE_H:  # ← 不再 break！\n",
    "        #         print(\"🔸 H 早停 → 切到 Pcv-phase\")\n",
    "        #         joint_phase = True  # 切旗標\n",
    "        #         wait_Pcv = 0  # 重設計數\n",
    "        #         continue  # 直接下一個 epoch\n",
    "\n",
    "        # else:  # Pcv-phase\n",
    "        #     if val_loss_Pcv < best_loss_Pcv - MIN_DELTA and val_loss_H < best_loss_H * 1.05 - MIN_DELTA:\n",
    "        #         best_loss_H = val_loss_H\n",
    "        #         best_loss_Pcv = val_loss_Pcv\n",
    "        #         best_epoch = epoch + 1\n",
    "        #         wait_Pcv = 0\n",
    "        #         torch.save(model.state_dict(), model_save_path)\n",
    "        #         print(f\"✅ Save best Pcv @ epoch {best_epoch}\")\n",
    "        #     else:\n",
    "        #         wait_Pcv += 1\n",
    "        #         print(f\"  Pcv 無改善 wait_Pcv={wait_Pcv}/{PATIENCE_PCV}\")\n",
    "\n",
    "        #     if wait_Pcv >= PATIENCE_PCV:  # 真正結束\n",
    "        #         print(\"🔸 Pcv 早停觸發，整體訓練結束\")\n",
    "        #         break\n",
    "\n",
    "        if not joint_phase:  # H-phase: 專心看 NRMSE_H\n",
    "            if avg_val_nrmse_H < best_nrmse_H:\n",
    "                print(\n",
    "                    f\"✅ NRMSE_H improved ({best_nrmse_H:.4f}% -> {avg_val_nrmse_H:.4f}%). Saving model...\"\n",
    "                )\n",
    "                best_nrmse_H = avg_val_nrmse_H\n",
    "                best_mape_Pcv = avg_val_mape_Pcv\n",
    "                best_epoch = epoch + 1\n",
    "                wait_H = 0\n",
    "                torch.save(model.state_dict(), model_save_path)\n",
    "            else:\n",
    "                wait_H += 1\n",
    "                print(\n",
    "                    f\"  NRMSE_H did not improve. Wait: {wait_H}/{PATIENCE_H}\")\n",
    "\n",
    "            if wait_H >= PATIENCE_H:\n",
    "                print(\"🔸 H-phase patience reached. Switching to Pcv-phase...\")\n",
    "                joint_phase = True\n",
    "                wait_Pcv = 0\n",
    "                continue\n",
    "\n",
    "        else:  # Pcv-phase: 專心看 MAPE_Pcv，但 H 不能太差\n",
    "            # 條件：MAPE 必須進步，且 NRMSE 不能比歷史最佳惡化超過 10% (相對值)\n",
    "            if avg_val_mape_Pcv < best_mape_Pcv and avg_val_nrmse_H < best_nrmse_H * 1.10:\n",
    "                print(\n",
    "                    f\"✅ MAPE_Pcv improved ({best_mape_Pcv:.4f}% -> {avg_val_mape_Pcv:.4f}%). Saving model...\"\n",
    "                )\n",
    "                best_nrmse_H = avg_val_nrmse_H\n",
    "                best_mape_Pcv = avg_val_mape_Pcv\n",
    "                best_epoch = epoch + 1\n",
    "                wait_Pcv = 0\n",
    "                torch.save(model.state_dict(), model_save_path)\n",
    "            else:\n",
    "                wait_Pcv += 1\n",
    "                print(\n",
    "                    f\"  MAPE_Pcv did not improve. Wait: {wait_Pcv}/{PATIENCE_PCV}\"\n",
    "                )\n",
    "\n",
    "            if wait_Pcv >= PATIENCE_PCV:\n",
    "                print(\"🔸 Pcv-phase patience reached. Training finished.\")\n",
    "                break\n",
    "\n",
    "        # ======================================================Early stop======================================================\n",
    "\n",
    "    print(f\"Training complete. Best model saved at {model_save_path}.\")\n",
    "    elapsed = time.perf_counter() - start_time  # ← 訓練結束，計算耗時\n",
    "    hrs = int(elapsed // 3600)\n",
    "    mins = int((elapsed % 3600) // 60)\n",
    "    secs = elapsed % 60\n",
    "    print(f\"訓練總耗時：{hrs} 小時 {mins} 分 {secs:.2f} 秒\")\n",
    "    logger.save_summary(best_epoch, best_val_loss, best_nrmse_H, best_mape_Pcv,\n",
    "                        model_save_path, elapsed)\n",
    "\n",
    "    hist_df = pd.DataFrame(history)\n",
    "\n",
    "    json_path = os.path.join(result_dir, \"training_history.json\")\n",
    "    hist_df.to_json(json_path, orient=\"records\", force_ascii=False, indent=2)\n",
    "    print(f\"✅ 已儲存訓練歷程到 {json_path}\")\n",
    "\n",
    "    # ==============================繪製 Train Loss 與 Validation Loss 圖==============================\n",
    "    # plt.figure(figsize=(10, 5))\n",
    "    # plt.plot(\n",
    "    #     range(1,\n",
    "    #           len(train_losses) + 1),\n",
    "    #     train_losses,\n",
    "    #     label=\"Train Loss\",\n",
    "    # )\n",
    "    # plt.plot(range(1,\n",
    "    #                len(val_losses) + 1),\n",
    "    #          val_losses,\n",
    "    #          label=\"Validation Loss\")\n",
    "\n",
    "    # plt.xlabel(\"Epochs\")\n",
    "    # plt.ylabel(\"Loss\")\n",
    "    # plt.title(\"Training & Validation Loss Curve\")\n",
    "    # plt.legend()\n",
    "    # plt.grid(alpha=0.5)\n",
    "    # if save_figure == True:\n",
    "    #     # 將圖表保存為 SVG 格式\n",
    "    #     figure_save_path2 = os.path.join(figure_save_base_path,\n",
    "    #                                      \"Training_Validation_Loss_Curve.svg\")\n",
    "    #     plt.savefig(figure_save_path2)\n",
    "    # plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    # --- ① Train / Val Total loss ---\n",
    "    plt.plot(range(1,\n",
    "                   len(train_losses) + 1),\n",
    "             train_losses,\n",
    "             label=\"Train Total\")\n",
    "    plt.plot(range(1, len(val_losses) + 1), val_losses, label=\"Val Total\")\n",
    "\n",
    "    # ➜ ② 另外畫 H-loss、Pcv-loss  (透明度低一點)\n",
    "    plt.plot(range(1,\n",
    "                   len(history[\"loss_H\"]) + 1),\n",
    "             history[\"loss_H\"],\n",
    "             label=\"Val H\",\n",
    "             alpha=0.4,\n",
    "             ls=\"--\")\n",
    "    plt.plot(range(1,\n",
    "                   len(history[\"loss_Pcv\"]) + 1),\n",
    "             history[\"loss_Pcv\"],\n",
    "             label=\"Val Pcv\",\n",
    "             alpha=0.4,\n",
    "             ls=\"--\")\n",
    "\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training & Validation Loss Curve\")\n",
    "    plt.grid(alpha=0.5)\n",
    "\n",
    "    # ➜ ③ 標出最佳模型 epoch\n",
    "    plt.axvline(best_epoch, ls=\":\", lw=1, c=\"k\", label=f\"best @ {best_epoch}\")\n",
    "\n",
    "    plt.legend()\n",
    "\n",
    "    # ➜ ④ 永久存圖\n",
    "    fig_loss_path = os.path.join(result_dir, \"loss_curve_final.svg\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fig_loss_path)\n",
    "    if not save_figure:\n",
    "        plt.close()  # 不顯示直接關\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "    # ==============================繪製 Train Loss 與 Validation Loss 圖 END==============================\n",
    "\n",
    "    # ===================================使用最佳模型來產生驗證結果=============================\n",
    "    model.load_state_dict(torch.load(model_save_path))  # 載入最佳模型\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, features, amps, s0, target_H, target_Pcv in valid_loader:\n",
    "            inputs, features, amps, s0, target_H, target_Pcv = inputs.to(\n",
    "                device), features.to(device), amps.to(device), s0.to(\n",
    "                    device), target_H.to(device), target_Pcv.to(device)\n",
    "\n",
    "            outputs_H, outputs_Pcv = model(inputs, features, amps, s0)\n",
    "            break  # 只使用一批驗證數據進行可視化\n",
    "\n",
    "    # 選取對應資料（index tensor 要先轉 list 才能 index numpy）\n",
    "    outputs_np = outputs_H[fixed_idx, -downsample:, 0].detach().cpu().numpy()\n",
    "    targets_np = target_H[fixed_idx, -downsample:, 0].detach().cpu().numpy()\n",
    "    B_seq_np = inputs[fixed_idx, -downsample:, 0].detach().cpu().numpy()\n",
    "\n",
    "    # 設定圖表\n",
    "    plt.figure()\n",
    "    for i in range(train_show_sample):  # 每一批數據繪製一個圖表\n",
    "        plt.plot(outputs_np[i], B_seq_np[i], label=f\"Pred: Sample {i+1}\")\n",
    "        plt.plot(targets_np[i],\n",
    "                 B_seq_np[i],\n",
    "                 label=f\"Target: Sample {i+1}\",\n",
    "                 alpha=0.7)\n",
    "\n",
    "        # 添加標題和標籤\n",
    "        plt.title(f\"Best Model - Predicted vs Target\")\n",
    "        plt.xlabel(\"H(A/m)\")\n",
    "        plt.ylabel(\"B(T)\")\n",
    "        plt.grid(alpha=0.5)\n",
    "        plt.legend()\n",
    "\n",
    "        if save_figure == True:\n",
    "            figure_save_path3 = os.path.join(\n",
    "                figure_save_base_path,\n",
    "                f\"Best Model Predicted vs Target Sample.svg\")  # 定義模型保存檔名\n",
    "            plt.savefig(figure_save_path3)\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # ===================================使用最佳模型來產生驗證結果 END============================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Python用\n",
    "    # BASE_DIR = Path(__file__).resolve().parent\n",
    "    # os.chdir(BASE_DIR)\n",
    "    # print(\"👉 Switch CWD to script folder:\", os.getcwd())\n",
    "\n",
    "    data_B, data_F, data_T, data_H, data_Pcv, data_Hdc, data_N, data_Duty_P, data_Duty_N = load_dataset(\n",
    "        material)\n",
    "\n",
    "    GLOBAL_B_MAX = np.abs(data_B).max()\n",
    "    GLOBAL_H_MAX = np.abs(data_H).max()\n",
    "\n",
    "    train_loader, valid_loader, norm = get_dataloader(data_B, data_F, data_T,\n",
    "                                                      data_H, data_N, data_Hdc,\n",
    "                                                      data_Duty_P, data_Duty_N,\n",
    "                                                      data_Pcv, GLOBAL_B_MAX,\n",
    "                                                      GLOBAL_H_MAX)\n",
    "\n",
    "    # ---- 印第一個 batch 檢查 ----\n",
    "    # # inputs, features, s0, target_H, target_Pcv = next(iter(train_loader))\n",
    "    # inputs, features, amps, s0, target_H, target_Pcv = next(iter(train_loader))\n",
    "\n",
    "    # print(\"=== Batch shape check ===\")\n",
    "    # print(f\"inputs      : {inputs.shape}\")  # (batch, seq_len, 4)\n",
    "    # print(f\"features    : {features.shape}\")  # (batch, 4)\n",
    "    # print(f\"s0          : {s0.shape}\")  # (batch, operator_size)\n",
    "    # print(f\"target_H    : {target_H.shape}\")  # (batch, seq_len, 1)\n",
    "    # # print(f\"target_Pcv  : {target_Pcv.shape}\")  # (batch, 1)\n",
    "    # print()\n",
    "\n",
    "    # # 選一筆樣本看看數值範圍\n",
    "    # idx = 0\n",
    "    # print(\"範例 inputs[0] (前 3 個時間點):\")\n",
    "    # print(inputs[idx, :3, :])  # B, ΔB, dB/dt, d²B/dt² (已歸一化到 ~[-1,1])\n",
    "    # print(\"範例 features[0]:\", features[idx])  # F, T, Hdc, N (已 z-score)\n",
    "    # print(\"範例 s0[0]:\", s0[idx, :5])  # 前 5 個 Preisach operator 狀態\n",
    "    # print(\"範例 target_H[0] (前 3 點):\", target_H[idx, :3, 0])\n",
    "    # # print(\"範例 target_Pcv[0]:\", target_Pcv[idx, 0])\n",
    "\n",
    "    # 產生 Logger（放在 train_model 前）\n",
    "\n",
    "    logger = TrainLogger(\n",
    "        exp_name=f\"{material}_{note}_{timestamp}\",\n",
    "        config_dict={\n",
    "            k: getattr(Config, k)\n",
    "            for k in dir(Config)\n",
    "            if not k.startswith('__') and not callable(getattr(Config, k))\n",
    "        },\n",
    "        result_dir=result_dir)\n",
    "    feature_names = [\"F\", \"T\", \"Hdc\", \"N\", \"Pcv\"]\n",
    "    logger.save_norm_params(norm, feature_names)\n",
    "\n",
    "    train_model(norm, train_loader, valid_loader, logger)  # logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B shape: (1000, 1040)\n",
      "H shape: (1000, 1040)\n",
      "F shape: (1000,)\n",
      "T shape: (1000,)\n",
      "Hdc shape: (1000,)\n",
      "N shape: (1000,)\n",
      "Duty Pos shape: (1000,)\n",
      "Duty Neg shape: (1000,)\n",
      "Pcv shape: (1000,)\n",
      "0.F, 1.T, 2.Hdc, 3.N, 4.Pcv\n",
      "\"CH467160\": [\n",
      "    [2.0, 1.0],\n",
      "    [25.0, 1.0],\n",
      "    [1287.258056640625, 705.1074829101562],\n",
      "    [14.199999809265137, 4.453069686889648],\n",
      "    [1.6449116468429565, 0.7523635029792786],\n",
      "]\n",
      "✅ Normalization parameters saved to results\\20250807_uesed_for_PFC_test5_CH467160_Add_NRMSE_MAPE\\norm_params.json\n",
      "=== Start Train  ===\n",
      "\n",
      "          \n",
      "          \n",
      "                                                    ⠀⠀⠀⠀⢀⡤⠖⠋⠉⠉⠉⠉⠙⠲⣦⣀⠀⠀⠀⠀⠀\n",
      "                                                    ⠀⠀⠀⡴⠋⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠻⣦⡀⠀⠀⠀\n",
      "                                                    ⠀⠀⡼⢁⡠⢼⠁⠀⢱⢄⣀⠀⠀⠀⠀⠀⠎⢿⡄⠀⠀\n",
      "                                                    ⠀⣸⠁⠀⣧⣼⠀⠀⣧⣼⠉⠀⠀⠀⠀⠀⠐⢬⣷⠀⠀\n",
      "                                                    ⡼⣿⢀⠀⣿⡟⠀⠀⣿⣿⠀⠀⠀⠀⠀⠀⠀⠀⢹⣧⠀\n",
      "                  我好想畢業                         ⣇⢹⠀⠁⠈⠀⠉⠃⠈⠃⠀⠀⠀⠀⠀⠀⠀⠀⡰⢸⡇\n",
      "                                                    ⠙⢿⣧⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⣏⣈⣉⣤⠿⠁\n",
      "                                                    ⠀⣠⣾⣿⠤⡀⠀⠀⠀⠀⠀⢀⣤⣶⣿⣿⣿⣿⣅⠀⠀\n",
      "                                                    ⢰⣧⣿⣿⣿⣦⣉⡐⠒⠒⢲⣿⣿⣿⣿⣿⣿⣶⣿⣧⠀\n",
      "                                                    ⠘⠿⢿⣿⣿⣿⡿⠿⠛⠿⠿⠿⣿⣿⣿⣿⣿⣿⡿⠟⠀\n",
      "                                                    ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠉⠉⠁⠀⠀⠀⠀\n",
      "\n",
      "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⣾⣷⣄⠀⠀⠀⣀⣤⣤⣤⡀⠀⠀⠀⠀⠀⠀⠀\n",
      "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣶⠏⠀⠀⣿⠀⢀⡾⠛⠋⠀⣾⣿⠀⠀⠀⠀⠀⠀⠀\n",
      "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡀⡏⠀⠀⠀⣿⢀⣾⠁⠀⣰⠆⢹⡿⠀⠀⠀⠀⠀⠀⠀\n",
      "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠃⣧⠀⠀⢠⡟⢸⡇⠀⣰⠟⠀⣼⠃⠀⠀⠀⠀⠀⠀⠀\n",
      "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣀⣹⣆⢀⣸⣇⣸⠃⢠⡏⠀⣸⠋⠀⠀⠀⠀⠀⠀⠀⠀\n",
      "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⣀⣤⣴⣶⣶⣶⠾⠟⠛⠉⠉⠉⠈⠉⠉⠛⠁⢾⠁⣴⠇⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
      "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣀⣤⣤⣶⣶⠾⠟⠛⠛⣻⣿⣙⡁⠀⠀⢾⣶⣾⣷⣿⣶⣄⠀⠀⠀⠀⠰⢿⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
      "⠀⠀⠀⠀⠀⠀⢀⣀⣀⣀⣠⣴⣶⣶⠾⠟⠛⠉⠉⠉⠀⠀⠀⠀⠀⣿⣻⣟⣻⣿⡦⠀⠘⣿⣿⣛⡿⢶⡇⠀⠀⠀⠀⠀⠀⢻⣆⠀⠀⠀⠀⠀⠀⠀⠀\n",
      "⣠⣶⣶⣶⣾⣿⣿⣿⣿⣿⣿⣿⣧⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⡟⠙⣿⣿⡗⠀⠀⠿⠉⣿⣿⣿⣶⠀⠀⠀⠀⠀⠀⠈⢿⠀⠀⠀⠀⠀⠀⠀⠀\n",
      "⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⠳⣄⣿⡿⠁⠀⠀⠘⢦⣿⣿⠇⠟⠁⠀⠀⠀⠀⠀⠀⣸⡇⠀⠀⠀⠀⠀⠀⠀\n",
      "⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠁⡇⠀⠀⠀⠀⠀⠀⠀\n",
      "⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢰⣇⠀⠀⠀⠀⠀⠀⠀\n",
      "⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⣿⠀⠀⠀⠀⠀⠀⠀\n",
      "⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡏⠀⠀⠀⠀⠀⠀⠀\n",
      "⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀\n",
      "⢻⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡟⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣇⡇⠀⠀⠀⠀⠀⠀⠀\n",
      "⠀⠻⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢤⣤⡀⠀⠀⠀⠀⠀⠀⠀⣿⡇⠀⠀⠀⠀⠀⠀⠀\n",
      "⠀⠀⠈⠙⢿⣿⣿⣿⣿⣿⣿⠟⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣀⣤⡾⠟⠛⠆⠀⠀⠀⠀⠀⢀⢻⡇⠀⠀⠀⠀⠀⠀⠀\n",
      "⠀⠀⠀⠀⠀⠈⠙⠿⣿⣭⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⣤⣴⣶⠾⠟⠋⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⣾⠇⠀⠀⠀⠀⠀⠀⠀\n",
      "⠀⠀⠀⠀⠀⠀⠀⠀⠈⠉⠙⠛⠷⠶⢶⣶⣦⣤⣴⡆⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣠⣌⣿⠀⠀⠀⠀⠀⠀⠀⠀\n",
      "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⣿⡄⠀⠀⠀⠀⠀⠀⠀⠀⠙⠛⠛⠛⠃⠀⠀⠀⠀⠀⠀⠀⣤⣴⣾⣿⣿⣿⣓⠀⠀⠀⠀⠀⠀⠀\n",
      "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣼⣿⣷⣦⣄⣀⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⣀⣠⣤⣶⣾⣟⣯⣽⠟⠋⠀⠉⠳⣄⠀⠀⠀⠀⠀\n",
      "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣾⢇⠀⠉⠛⠷⣮⣍⣩⡍⢻⡟⠉⣉⢹⡏⠉⣿⣹⣷⣦⣿⠿⠟⠉⠀⠀⠀⠀⠀⠀⠙⣆⠀⠀⠀⠀\n",
      "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣰⠏⢸⠇⠀⠀⠀⠀⠀⠉⠉⠛⠛⠛⠛⠛⠛⠛⠋⠉⠉⠀⠀⠀⠀⠀⢠⣠⡶⠀⠀⠀⠀⠘⣧⠀⠀⠀\n",
      "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢰⡿⠀⣸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠟⠁⠀⠀⠀⠀⠀⠘⣆⠀⠀\n",
      "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣾⠃⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣾⣇⡀⠀⠀⠀⠀⠀⠀⢹⡆⠀\n",
      "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⣾⠀⣾⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⢿⣥⢠⣤⠼⠇⠀⠀⠘⣿⡄\n",
      "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⣽⡄⠈⢿⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⣿⠿⠾⠷⠄⠀⠀⠀⢀⣿⠁\n",
      "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⣧⠀⠸⣷⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣤⣾⠋⠀⠀⠀⠀⠀⠀⢰⣾⡿⠀\n",
      "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠙⣦⣠⣿⣿⣶⣶⣤⣤⣄⣀⣀⣀⣀⠀⠀⠀⠀⠀⠀⠀⣀⣀⣠⣴⣿⣇⠀⠀⠀⠀⠀⠀⠀⣸⡟⠀⠀\n",
      "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⢻⣿⠀⠉⠛⢿⣿⣯⣿⡟⢿⠻⣿⢻⣿⢿⣿⣿⣿⣿⣿⠿⠟⠹⣟⢷⣄⠀⠀⠀⢀⣼⠟⠀⠀⠀\n",
      "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⣿⣄⠀⠀⠘⢷⣌⡻⠿⣿⣛⣿⣟⣛⣛⣋⣉⣉⣉⣀⡀⠀⠀⠈⠻⢿⣷⣶⣶⢛⣧⠀⠀⠀⠀\n",
      "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⣏⠀⠀⠀⠀⠹⢯⣟⣛⢿⣿⣽⣅⣀⡀⠀⣀⡀⠀⠀⠀⠠⢦⣀⠰⡦⠀⢸⠀⣏⠀⠀⠀⠀\n",
      "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢿⡀⠀⠀⠀⠀⠀⠀⠈⠉⢻⣿⡟⠛⠉⠉⠁⠀⠀⠀⠀⠀⠀⠈⠛⠷⠀⣸⠀⣿⡀⠀⠀⠀\n",
      "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⣧⠀⠀⠀⠀⠀⠀⠀⠀⠘⣿⣇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⠀⣿⡇⠀⠀⠀\n",
      "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠸⢿⠀⠀⢦⡀⡀⠀⠀⠀⠀⢹⣿⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⡄⡏⠀⠀⠀⠀\n",
      "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⡄⠀⠈⠳⣝⠦⢄⠀⠀⠀⣟⣷⠀⠀⠀⣷⣄⠀⠀⠀⠀⠀⠀⠀⠀⣿⡇⡇⠀⠀⠀⠀\n",
      "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣄⣷⡀⠀⠀⠈⠙⠂⠀⠀⠀⢸⣿⡄⠀⠀⠘⢦⡙⢦⡀⠀⠀⠀⠀⢰⣷⣷⡇⠀⠀⠀⠀\n",
      "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢻⡿⢧⣤⣀⡀⠀⠀⠀⠀⠀⠀⢿⣷⣄⠀⠀⠀⠁⠋⠀⠀⠀⠀⠀⢸⣿⣿⣇⠀⠀⠀⠀\n",
      "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⣷⡀⠈⠉⠛⠛⠛⠛⠛⠛⠛⠛⢿⡍⠛⠳⠶⣶⣤⣤⣤⣤⣤⣤⠼⠟⡟⢿⡇⠀⠀⠀\n",
      "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⣷⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⣧⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠰⣾⡇⠀⠀⠀\n",
      "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣠⣤⣴⣿⣷⣶⣶⣶⣶⣶⣶⣦⣀⣀⣀⣻⡀⠀⠀⠀⣀⣀⠀⡀⠀⠀⠀⢀⣼⣿⠇⠀⠀⠀\n",
      "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣴⠟⠉⠁⠀⠀⠈⠻⣿⡆⢹⣯⣽⣿⣿⠟⠋⠙⣿⣶⣿⣿⣿⣿⣾⣿⣿⣿⣟⠋⠉⣇⠀⠀⠀⠀\n",
      "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⡇⠀⠀⠀⡀⠀⠀⠀⠈⢻⣆⣿⠀⠀⠀⢁⣶⣿⠿⠟⠛⠷⣶⣽⣿⣿⣻⣏⠙⠃⣴⢻⠀⠀⠀⠀\n",
      "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠸⣷⣀⠀⠀⠉⠀⠀⠀⠀⠀⢹⣿⠀⣀⣴⣿⠋⠀⠀⠀⠀⠀⠀⠉⠻⣿⣧⣿⢀⣰⣿⣿⠀⠀⠀⠀\n",
      "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⢿⣶⣶⣤⣤⣤⣤⣤⣤⣾⣿⣟⣿⣿⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⣿⣅⣾⢿⣵⠇⠀⠀⠀⠀\n",
      "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠉⠛⠛⠛⠛⠛⠛⠛⠛⠉⠉⠉⠁⢹⣜⠷⠦⠤⠤⠤⠤⠤⠴⠶⠛⣉⣱⠿⠁⠀⠀⠀⠀⠀\n",
      "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠛⠿⠷⣦⣤⣤⣄⣠⣤⣤⡶⠟⠁⠀⠀⠀⠀⠀⠀⠀\n",
      "                        \n",
      "                        \n",
      "                我是Chill guy，這是我的碩論，我真的不會，但沒關係，大不了休學，我超爛\n",
      "                 \n",
      "    \n",
      "Number of parameters:  13431\n",
      "Epoch 1 | Learning Rate: 0.019800\n",
      "---\n",
      "Epoch 1 | LR: 0.019800 | alpha: 0.001\n",
      "  Losses -> Train: 0.167052 | Val: 0.037850 | Val_H: 0.036839 | Val_Pcv: 1.046875\n",
      "  Metrics -> NRMSE_H: 138.1638% | MAPE_Pcv: 263.2744%\n",
      "  (Best @ Epoch 0 | Best NRMSE_H: inf% | Best MAPE_Pcv: inf%)\n",
      "✅ NRMSE_H improved (inf% -> 138.1638%). Saving model...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[25], line 53\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     50\u001b[0m feature_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHdc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mN\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPcv\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     51\u001b[0m logger\u001b[38;5;241m.\u001b[39msave_norm_params(norm, feature_names)\n\u001b[1;32m---> 53\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogger\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[24], line 145\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(norm, train_loader, valid_loader, logger)\u001b[0m\n\u001b[0;32m    142\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautocast(device_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 145\u001b[0m     outputs_H, outputs_Pcv \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mamps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    146\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43ms0\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 模型的輸出\u001b[39;00m\n\u001b[0;32m    147\u001b[0m     loss_H \u001b[38;5;241m=\u001b[39m criterion_H(outputs_H, target_H)  \u001b[38;5;66;03m# 使用真實的 H(t) 計算損失\u001b[39;00m\n\u001b[0;32m    148\u001b[0m     loss_Pcv \u001b[38;5;241m=\u001b[39m criterion_Pcv(outputs_Pcv, target_Pcv)\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[19], line 68\u001b[0m, in \u001b[0;36mMMINet.forward\u001b[1;34m(self, x, var, amps, s0, n_init)\u001b[0m\n\u001b[0;32m     65\u001b[0m dnn1_in \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrnn1_hx, var), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# H hysteresis prediction\u001b[39;00m\n\u001b[1;32m---> 68\u001b[0m H_hyst_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdnn1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdnn1_in\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# DNN2 input (B,dB/dt,T,F)\u001b[39;00m\n\u001b[0;32m     71\u001b[0m rnn2_in \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((x2[:, t, :], var), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
