{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Network Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step0: Import Package & Hyperparameter Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¸…ç©ºæ‰€æœ‰è®Šæ•¸\n",
    "%reset -f\n",
    "# # å¼·åˆ¶ Python å›æ”¶è¨˜æ†¶é«”\n",
    "# import gc\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Package\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook ç’°å¢ƒï¼Œè·³éåˆ‡æ›ç›®éŒ„\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from datetime import datetime\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    os.chdir(os.path.dirname(os.path.abspath(__file__)))\n",
    "except NameError:\n",
    "    print(\"Notebook ç’°å¢ƒï¼Œè·³éåˆ‡æ›ç›®éŒ„\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Unified Hyperparameter Configuration\n",
    "class Config:\n",
    "    SEED = 1\n",
    "    NUM_EPOCHS = 1500\n",
    "    BATCH_SIZE = 256\n",
    "    LEARNING_RATE = 0.002  #è«–æ–‡æä¾›\n",
    "    LR_SCHEDULER_GAMMA = 0.99  #è«–æ–‡æä¾›\n",
    "    DECAY_EPOCH = 200\n",
    "    DECAY_RATIO = 0.5\n",
    "    EARLY_STOPPING_PATIENCE = 100\n",
    "    HIDDEN_SIZE = 30\n",
    "    OPERATOR_SIZE = 30\n",
    "    MAXOUT_H = 1\n",
    "\n",
    "\n",
    "# Reproducibility\n",
    "random.seed(Config.SEED)\n",
    "np.random.seed(Config.SEED)\n",
    "torch.manual_seed(Config.SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Material & Number of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "material = \"CH467160\"\n",
    "fix_way = \"uesed_for_PFC_test4\"\n",
    "note = \"optimizer_test\"\n",
    "note_detail = \"åŠ å…¥å­¸ç¿’ç‡èª¿åº¦å™¨\"\n",
    "downsample = 1024\n",
    "save_figure = True\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d\")\n",
    "\n",
    "# è¨“ç·´æƒ…æ³æ³\n",
    "plot_interval = 150\n",
    "train_show_sample = 1\n",
    "\n",
    "result_dir = os.path.join(\"results\",\n",
    "                          f\"{timestamp}_{fix_way}_{material}_{note}\")\n",
    "os.makedirs(result_dir, exist_ok=True)\n",
    "\n",
    "# å®šç¾©ä¿å­˜æ¨¡å‹çš„è·¯å¾‘\n",
    "model_save_dir = result_dir\n",
    "model_save_path = os.path.join(\n",
    "    model_save_dir, f\"{material}_{fix_way}_{note}_{timestamp}.pt\")  # å®šç¾©æ¨¡å‹ä¿å­˜æª”å\n",
    "\n",
    "figure_save_base_path = result_dir\n",
    "\n",
    "# Select device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1: Data processing and data loader generate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Preprocess data into a data loader\n",
    "def get_dataloader(data_B,\n",
    "                   data_F,\n",
    "                   data_T,\n",
    "                   data_H,\n",
    "                   data_N,\n",
    "                   data_Hdc,\n",
    "                   data_Duty_P,\n",
    "                   data_Duty_N,\n",
    "                   data_Pcv,\n",
    "                   global_B_max,\n",
    "                   global_H_max,\n",
    "                   n_init=16):\n",
    "\n",
    "    # Data pre-process\n",
    "\n",
    "    # â”€â”€ 0. å…¨åŸŸè¨­å®š/é™éšè¨­å®š â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    eps = 1e-8  # é˜²æ­¢é™¤ä»¥ 0\n",
    "    if downsample == 1024:\n",
    "        seq_length = 1024  # å–®ç­†æ³¢å½¢é»æ•¸ (ä¸å† down-sample)\n",
    "    else:\n",
    "        seq_length = downsample\n",
    "        cols = np.linspace(0, 1023, seq_length, dtype=int)\n",
    "        data_B = data_B[:, cols]\n",
    "        data_H = data_H[:, cols]\n",
    "\n",
    "    # â”€â”€ 1. æ³¢å½¢æ‹¼æ¥ (è£œ n_init é»ä½œåˆå§‹ç£åŒ–) â”€â”€â”€â”€\n",
    "    data_length = seq_length + n_init\n",
    "    data_B = np.hstack((data_B[:, -n_init:], data_B))  # (batch, data_length)\n",
    "    data_H = np.hstack((data_H[:, -n_init:], data_H))\n",
    "\n",
    "    print(\"B shape:\", data_B.shape)\n",
    "    print(\"H shape:\", data_H.shape)\n",
    "    print(\"F shape:\", data_F.shape)\n",
    "    print(\"T shape:\", data_T.shape)\n",
    "    print(\"Hdc shape:\", data_Hdc.shape)\n",
    "    print(\"N shape:\", data_N.shape)\n",
    "    print(\"Duty Pos shape:\", data_Duty_P.shape)\n",
    "    print(\"Duty Neg shape:\", data_Duty_N.shape)\n",
    "    print(\"Pcv shape:\", data_Pcv.shape)\n",
    "\n",
    "    # â”€â”€ 2. è½‰æˆ Tensor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    B = torch.from_numpy(data_B).view(-1, data_length, 1).float()  # (B,N,1)\n",
    "    H = torch.from_numpy(data_H).view(-1, data_length, 1).float()\n",
    "    F = torch.log10(torch.from_numpy(data_F).view(-1, 1).float())  # ç´”é‡\n",
    "    T = torch.from_numpy(data_T).view(-1, 1).float()\n",
    "    Hdc = torch.from_numpy(data_Hdc).view(-1, 1).float()\n",
    "    N = torch.from_numpy(data_N).view(-1, 1).float()\n",
    "    Duty_P = torch.from_numpy(data_Duty_P).view(-1, 1).float()\n",
    "    Duty_N = torch.from_numpy(data_Duty_N).view(-1, 1).float()\n",
    "    Pcv = torch.log10(torch.from_numpy(data_Pcv).view(-1, 1).float())\n",
    "\n",
    "    # â”€â”€ 3. æ¯ç­†æ¨£æœ¬å„è‡ªæ‰¾æœ€å¤§å¹…å€¼ (per-profile scale) â”€\n",
    "    # scale_B = torch.max(torch.abs(B), dim=1,\n",
    "    #                     keepdim=True).values + eps  # (B,1,1)\n",
    "    # scale_H = torch.max(torch.abs(H), dim=1, keepdim=True).values + eps\n",
    "\n",
    "    # â”€â”€ 4. å…ˆè¨ˆç®—å°æ•¸ï¼Œå†é™¤ä»¥ scale_B â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    dB = torch.diff(B, dim=1, prepend=B[:, :1])\n",
    "    dB_dt = dB * (seq_length * F.view(-1, 1, 1))  # çœŸå¯¦æ–œç‡\n",
    "    # d2B = torch.diff(dB, dim=1, prepend=dB[:, :1])\n",
    "    # d2B_dt = d2B * (seq_length * F.view(-1, 1, 1))\n",
    "\n",
    "    # â”€â”€ 5. å½¢æˆæ¨¡å‹è¼¸å…¥ (å·²ç¶“ç¸®æ”¾åˆ° [-1,1]) â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # in_B = B / scale_B\n",
    "    # out_H = H / scale_H  # é æ¸¬ç›®æ¨™\n",
    "    # in_dB_dt = dB_dt / scale_B\n",
    "    # å¾ŒçºŒç™¼ç¾d2Bç„¡æ”¹å–„æº–ç¢ºåº¦(å¯èƒ½è¦å¤šæ³¢å½¢ç¨®é¡æ‰æœ‰æ•ˆå¹«åŠ©)ï¼Œå…ˆä»¥è¼¸å…¥0ä»£å…¥\n",
    "    # in_d2B_dt = d2B_dt / scale_B\n",
    "\n",
    "    # *ä¿®æ­£æˆä½¿ç”¨å…¨åŸŸæœ€å¤§å¹…å€¼ (ver.250806)\n",
    "    in_B = B / global_B_max\n",
    "    out_H = H / global_H_max\n",
    "    in_dB_dt = dB_dt / global_B_max\n",
    "    in_d2B_dt = torch.zeros_like(in_dB_dt)\n",
    "\n",
    "    # â”€â”€ 6. ç´”é‡ç‰¹å¾µï¼šè¨ˆç®— z-score åƒæ•¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    def safe_mean_std(tensor, eps=1e-8):\n",
    "        m = torch.mean(tensor).item()\n",
    "        s = torch.std(tensor).item()\n",
    "        return [m, 1.0 if s < eps else s]\n",
    "\n",
    "    #  Compute normalization parameters (å‡å€¼ & æ¨™æº–å·®)**\n",
    "    norm = [\n",
    "        safe_mean_std(F),\n",
    "        safe_mean_std(T),\n",
    "        safe_mean_std(Hdc),\n",
    "        safe_mean_std(N),\n",
    "        safe_mean_std(Pcv)\n",
    "    ]\n",
    "\n",
    "    # ç”¨ä¾†åštestå›ºå®šæ¨™æº–åŒ–åƒæ•¸çš„\n",
    "    print(\"0.F, 1.T, 2.Hdc, 3.N, 4.Pcv\")\n",
    "    material_name = f\"{material}\"\n",
    "    print(f'\"{material_name}\": [')\n",
    "    for param in norm:\n",
    "        print(f\"    {param},\")\n",
    "    print(\"]\")\n",
    "\n",
    "    # Data Normalization\n",
    "    in_F = (F - norm[0][0]) / norm[0][1]  # F\n",
    "    in_T = (T - norm[1][0]) / norm[1][1]  # T\n",
    "    in_Hdc = (Hdc - norm[2][0]) / norm[2][1]  # Hdc\n",
    "    in_N = (N - norm[3][0]) / norm[3][1]  # N\n",
    "    in_Pcv = (Pcv - norm[4][0]) / norm[4][1]  # Pcv\n",
    "    in_Duty_P = Duty_P  # Duty Pos\n",
    "    in_Duty_N = Duty_N  # Duty Neg\n",
    "\n",
    "    # #   â†’ æ–¹ä¾¿æ¨è«–å¾©åŸï¼Œä¿ç•™ scale_B, scale_H ç•¶ä½œé¡å¤–ç´”é‡\n",
    "    # aux_features = torch.cat(\n",
    "    #     (in_F, in_T, in_Hdc, in_N, in_Duty_P, in_Duty_N, in_Pcv,\n",
    "    #      scale_B.squeeze(-1), scale_H.squeeze(-1)),\n",
    "    #     dim=1)\n",
    "\n",
    "    # â”€â”€ 7. ç”¢ç”Ÿåˆå§‹ Preisach operator ç‹€æ…‹ s0 â”€â”€â”€â”€â”€â”€\n",
    "    max_B, _ = torch.max(in_B, dim=1)\n",
    "    min_B, _ = torch.min(in_B, dim=1)\n",
    "    # s0 = get_operator_init(in_B[:, 0] - dB[:, 0] / scale_B.squeeze(-1),\n",
    "    #                        dB / scale_B, max_B, min_B)\n",
    "\n",
    "    s0 = get_operator_init(in_B[:, 0] - dB[:, 0] / global_B_max.squeeze(-1),\n",
    "                           dB / global_B_max, max_B, min_B)\n",
    "\n",
    "    # â”€â”€ 8. çµ„åˆ Dataset â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # wave_inputs = torch.cat(\n",
    "    #     (\n",
    "    #         in_B,  # â‘  B\n",
    "    #         dB / scale_B,  # â‘¡ Î”B\n",
    "    #         in_dB_dt,  # â‘¢ dB/dt\n",
    "    #         in_d2B_dt),\n",
    "    #     dim=2)  # â‘£ dÂ²B/dtÂ²   â†’ (B,L,4)\n",
    "\n",
    "    # amps = torch.cat((scale_B.squeeze(-1), scale_H.squeeze(-1)),\n",
    "    #                 dim=1)  # (B,2)\n",
    "\n",
    "    wave_inputs = torch.cat(\n",
    "        (\n",
    "            in_B,  # â‘  B\n",
    "            dB / global_B_max,  # â‘¡ Î”B\n",
    "            in_dB_dt,  # â‘¢ dB/dt\n",
    "            in_d2B_dt),\n",
    "        dim=2)  # â‘£ dÂ²B/dtÂ²   â†’ (B,L,4)\n",
    "\n",
    "    aux_features = torch.cat((in_F, in_T, in_Hdc, in_N, in_Duty_P, in_Duty_N),\n",
    "                             dim=1)  # (B,4)\n",
    "\n",
    "    amp_B = torch.full((len(B), 1), global_B_max, dtype=torch.float32)\n",
    "    amp_H = torch.full((len(B), 1), global_H_max, dtype=torch.float32)\n",
    "    amps = torch.cat((amp_B, amp_H), dim=1)  # ä»çµ¦ RNN2 ç”¨\n",
    "\n",
    "    # é€™è£¡æŠŠ Pcvï¼ˆå·² z-scoreï¼‰å–®ç¨æ‹¿å‡ºä¾†ç•¶å¦ä¸€å€‹ label\n",
    "    target_Pcv = in_Pcv  # (B,1)\n",
    "\n",
    "    full_dataset = torch.utils.data.TensorDataset(\n",
    "        wave_inputs,  # 0  â†’ æ¨¡å‹åºåˆ—è¼¸å…¥\n",
    "        aux_features,  # 1  â†’ 4 å€‹ç´”é‡\n",
    "        amps,  # 2  â†’ å¹…å€¼ä¿‚æ•¸\n",
    "        s0,  # 3  â†’ Preisach åˆå§‹ç‹€æ…‹\n",
    "        out_H,  # 4  â†’ ç›®æ¨™ H  (å·² scale_H)\n",
    "        target_Pcv)  # 5  â†’ ç›®æ¨™ Pcv (å·² z-score)\n",
    "\n",
    "    # â”€â”€ 9. Train / Valid split & DataLoader â”€â”€â”€â”€â”€â”€â”€\n",
    "    train_size = int(0.8 * len(full_dataset))\n",
    "    valid_size = len(full_dataset) - train_size\n",
    "    train_set, valid_set = torch.utils.data.random_split(\n",
    "        full_dataset, [train_size, valid_size],\n",
    "        generator=torch.Generator().manual_seed(Config.SEED))\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_set,\n",
    "                                               batch_size=Config.BATCH_SIZE,\n",
    "                                               shuffle=True,\n",
    "                                               num_workers=0,\n",
    "                                               pin_memory=True,\n",
    "                                               collate_fn=filter_input)\n",
    "\n",
    "    valid_loader = torch.utils.data.DataLoader(valid_set,\n",
    "                                               batch_size=Config.BATCH_SIZE,\n",
    "                                               shuffle=False,\n",
    "                                               num_workers=0,\n",
    "                                               pin_memory=True,\n",
    "                                               collate_fn=filter_input)\n",
    "\n",
    "    return train_loader, valid_loader, norm\n",
    "\n",
    "\n",
    "# %% Predict the operator state at t0\n",
    "def get_operator_init(B1,\n",
    "                      dB,\n",
    "                      Bmax,\n",
    "                      Bmin,\n",
    "                      max_out_H=Config.MAXOUT_H,\n",
    "                      operator_size=Config.OPERATOR_SIZE):\n",
    "    \"\"\"Compute the initial state of hysteresis operators\"\"\"\n",
    "    s0 = torch.zeros((dB.shape[0], operator_size))\n",
    "    operator_thre = torch.from_numpy(\n",
    "        np.linspace(max_out_H / operator_size, max_out_H,\n",
    "                    operator_size)).view(1, -1)\n",
    "\n",
    "    for i in range(dB.shape[0]):\n",
    "        for j in range(operator_size):\n",
    "            r = operator_thre[0, j]\n",
    "            if (Bmax[i] >= r) or (Bmin[i] <= -r):\n",
    "                if dB[i, 0] >= 0:\n",
    "                    if B1[i] > Bmin[i] + 2 * r:\n",
    "                        s0[i, j] = r\n",
    "                    else:\n",
    "                        s0[i, j] = B1[i] - (r + Bmin[i])\n",
    "                else:\n",
    "                    if B1[i] < Bmax[i] - 2 * r:\n",
    "                        s0[i, j] = -r\n",
    "                    else:\n",
    "                        s0[i, j] = B1[i] + (r - Bmax[i])\n",
    "    return s0\n",
    "\n",
    "\n",
    "def filter_input(batch):\n",
    "    inputs, features, amps, s0, target_H, target_Pcv = zip(*batch)\n",
    "\n",
    "    inputs = torch.stack(inputs)\n",
    "    features = torch.stack(features)\n",
    "    amps = torch.stack(amps)\n",
    "    s0 = torch.stack(s0)\n",
    "    target_H = torch.stack(target_H)[:, -downsample:, :]  # ä¿ç•™å…¨é•·\n",
    "    target_Pcv = torch.stack(target_Pcv)  # (B,1)\n",
    "\n",
    "    return inputs, features, amps, s0, target_H, target_Pcv\n",
    "\n",
    "\n",
    "# æº«åº¦é »ç‡ä¸è®ŠåŠ å…¥å¾®å°çš„ epsilon\n",
    "def safe_mean_std(tensor, eps=1e-8):\n",
    "    m_tensor = torch.mean(tensor)  # é‚„æ˜¯ Tensor\n",
    "    s_tensor = torch.std(tensor)  # é‚„æ˜¯ Tensor\n",
    "\n",
    "    m_val = m_tensor.item()  # ç¬¬ä¸€æ¬¡è½‰æˆ float\n",
    "    s_val = s_tensor.item()\n",
    "    if s_val < eps:\n",
    "        s_val = 1.0\n",
    "    return [m_val, s_val]  # ç›´æ¥å›å‚³ float\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2: Define Network Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Magnetization mechansim-determined neural network\n",
    "\"\"\"\n",
    "    Parameters:\n",
    "    - hidden_size: number of eddy current slices (RNN neuron)\n",
    "    - operator_size: number of operators\n",
    "    - input_size: number of inputs (1.B 2.dB 3.dB/dt 4.d2B/dt)\n",
    "    - var_size: number of supplenmentary variables (1.F 2.T 3.Hdc 4.N 5.Duty_P 6.Duty_N)        \n",
    "    - output_size: number of outputs (1.H)\n",
    "    \n",
    "    åªå…ˆæŠŠd2B/dtè€ƒé‡åœ¨EddyCellè£¡é¢\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class MMINet(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            norm,\n",
    "            hidden_size=Config.HIDDEN_SIZE,\n",
    "            operator_size=Config.OPERATOR_SIZE,\n",
    "            input_size=4,  # Add d2B(250203)\n",
    "            var_size=6,\n",
    "            output_size=1):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.var_size = var_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.operator_size = operator_size\n",
    "        self.norm = norm  #*é€™è£¡æ”¹æˆå¾å¤–éƒ¨å‚³å…¥ norm(250203)\n",
    "\n",
    "        self.rnn1 = StopOperatorCell(self.operator_size)\n",
    "        self.dnn1 = nn.Linear(self.operator_size + self.var_size, 1)\n",
    "        # var_size (F T Hdc N Duty_P Duty_N ) + 3 (B, dB/dt, d2B/dt)\n",
    "        self.rnn2 = EddyCell(var_size + 3, self.hidden_size, output_size)\n",
    "        self.dnn2 = nn.Linear(self.hidden_size, 1)\n",
    "        self.rnn2_hx = None\n",
    "        # var_size=6: 1.F 2.T 3.Hdc 4.N 5.Duty_P 6.Duty_N + 1 for P_prelim\n",
    "        self.loss_mlp = nn.Sequential(nn.Linear(self.var_size + 1, 128),\n",
    "                                      nn.ReLU(), nn.Linear(128, 64), nn.ReLU(),\n",
    "                                      nn.Linear(64, 32), nn.ReLU(),\n",
    "                                      nn.Linear(32, 1))\n",
    "\n",
    "    def forward(self, x, var, amps, s0, n_init=16):\n",
    "        \"\"\"\n",
    "        Parameters: \n",
    "        - x(batch,seq,input_size): Input features (1.B, 2.dB, 3.dB/dt)  \n",
    "        - var(batch,var_size): Supplementary inputs (1.F 2.T 3.Hdc 4.N 5.Duty_P 6.Duty_N) \n",
    "        - s0(batch,1): Operator inital states\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)  # Batch size\n",
    "        seq_size = x.size(1)  # Ser\n",
    "        self.rnn1_hx = s0\n",
    "\n",
    "        # !Initialize DNN2 input (1.B 2.dB/dt 3.d2B)\n",
    "        # x2 = torch.cat((x[:, :, 0:1], x[:, :, 2:3]), dim=2)\n",
    "        # !é¸å– B, dB/dt, d2B/dt\n",
    "        x2 = torch.cat((x[:, :, 0:1], x[:, :, 2:4]), dim=2)\n",
    "\n",
    "        for t in range(seq_size):\n",
    "            # RNN1 input (dB,state)\n",
    "            self.rnn1_hx = self.rnn1(x[:, t, 1:2], self.rnn1_hx)\n",
    "\n",
    "            # DNN1 input (rnn1_hx,F,T,Hdc,N)\n",
    "            dnn1_in = torch.cat((self.rnn1_hx, var), dim=1)\n",
    "\n",
    "            # H hysteresis prediction\n",
    "            H_hyst_pred = self.dnn1(dnn1_in)\n",
    "\n",
    "            # DNN2 input (B,dB/dt,T,F)\n",
    "            rnn2_in = torch.cat((x2[:, t, :], var), dim=1)\n",
    "\n",
    "            # Initialize second rnn state\n",
    "            if t == 0:\n",
    "                H_eddy_init = x[:, t, 0:1] - H_hyst_pred\n",
    "                buffer = x.new_ones(x.size(0), self.hidden_size)\n",
    "                self.rnn2_hx = Variable(\n",
    "                    (buffer / torch.sum(self.dnn2.weight, dim=1)) *\n",
    "                    H_eddy_init)\n",
    "\n",
    "            #rnn2_in = torch.cat((rnn2_in,H_hyst_pred),dim=1)\n",
    "            self.rnn2_hx = self.rnn2(rnn2_in, self.rnn2_hx)\n",
    "\n",
    "            # H eddy prediction\n",
    "            H_eddy = self.dnn2(self.rnn2_hx)\n",
    "\n",
    "            # H total\n",
    "            H_total = (H_hyst_pred + H_eddy).view(batch_size, 1,\n",
    "                                                  self.output_size)\n",
    "            if t == 0:\n",
    "                output = H_total\n",
    "            else:\n",
    "                output = torch.cat((output, H_total), dim=1)\n",
    "\n",
    "        H = (output[:, n_init:, :])\n",
    "\n",
    "        amp_B = amps[:, 0:1]  # (batch,1)\n",
    "        amp_H = amps[:, 1:2]  # (batch,1)\n",
    "        B_amp = x[:, n_init:, 0:1] * amp_B.unsqueeze(1)\n",
    "        H_amp = output[:, n_init:, :] * amp_H.unsqueeze(1)\n",
    "        P_prelim = torch.trapz(H_amp, B_amp, axis=1) * (10**(\n",
    "            var[:, 0:1] * self.norm[0][1] + self.norm[0][0]))\n",
    "        Pcv_log = torch.log10(P_prelim.clamp(min=1e-12))\n",
    "        Pcv = (Pcv_log - self.norm[4][0]) / self.norm[4][1]\n",
    "        mlp_input = torch.cat((var, Pcv), dim=1)  # (batch, 5)\n",
    "        s = self.loss_mlp(mlp_input)\n",
    "        Pcv_mlp = Pcv + s\n",
    "\n",
    "        return H, Pcv_mlp\n",
    "\n",
    "\n",
    "class StopOperatorCell():\n",
    "\n",
    "    def __init__(self, operator_size):\n",
    "        self.operator_thre = torch.from_numpy(\n",
    "            np.linspace(Config.MAXOUT_H / operator_size, Config.MAXOUT_H,\n",
    "                        operator_size)).view(1, -1)\n",
    "\n",
    "    def sslu(self, X):\n",
    "        a = torch.ones_like(X)\n",
    "        return torch.max(-a, torch.min(a, X))\n",
    "\n",
    "    def __call__(self, dB, state):\n",
    "        r = self.operator_thre.to(dB.device)\n",
    "        output = self.sslu((dB + state) / r) * r\n",
    "        return output.float()\n",
    "\n",
    "\n",
    "class EddyCell(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size=1):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.x2h = nn.Linear(input_size, hidden_size, bias=False)\n",
    "        self.h2h = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        hidden = self.x2h(x) + self.h2h(hidden)\n",
    "        hidden = torch.sigmoid(hidden)\n",
    "        return hidden\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3: Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def load_dataset(material, base_path=\"./Data/\"):\n",
    "\n",
    "    in_file1 = f\"{base_path}{material}/train/B_Field.csv\"\n",
    "    in_file2 = f\"{base_path}{material}/train/Frequency.csv\"\n",
    "    in_file3 = f\"{base_path}{material}/train/Temperature.csv\"\n",
    "    in_file4 = f\"{base_path}{material}/train/H_Field.csv\"\n",
    "    in_file5 = f\"{base_path}{material}/train/Volumetric_Loss.csv\"\n",
    "    in_file6 = f\"{base_path}{material}/train/Hdc.csv\"\n",
    "    in_file7 = f\"{base_path}{material}/train/Turns.csv\"\n",
    "    in_file8 = f\"{base_path}{material}/train/Duty_P.csv\"\n",
    "    in_file9 = f\"{base_path}{material}/train/Duty_N.csv\"\n",
    "\n",
    "    data_B = np.genfromtxt(in_file1, delimiter=',')  # N x 1024\n",
    "    data_F = np.genfromtxt(in_file2, delimiter=',')  # N x 1\n",
    "    data_T = np.genfromtxt(in_file3, delimiter=',')  # N x 1\n",
    "    data_H = np.genfromtxt(in_file4, delimiter=',')  # N x 1024\n",
    "    data_Pcv = np.genfromtxt(in_file5, delimiter=',')  # N x 1\n",
    "    data_Hdc = np.genfromtxt(in_file6, delimiter=',')  # N x 1\n",
    "    data_N = np.genfromtxt(in_file7, delimiter=',')  # N x 1\n",
    "    data_Duty_P = np.genfromtxt(in_file8, delimiter=',')  # N x 1\n",
    "    data_Duty_N = np.genfromtxt(in_file9, delimiter=',')  # N x 1\n",
    "\n",
    "    return data_B, data_F, data_T, data_H, data_Pcv, data_Hdc, data_N, data_Duty_P, data_Duty_N\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainLogger:\n",
    "\n",
    "    def __init__(self, exp_name, config_dict, result_dir):\n",
    "        self.exp_name = exp_name\n",
    "        self.result_dir = result_dir\n",
    "        self.config = config_dict\n",
    "        os.makedirs(self.result_dir, exist_ok=True)\n",
    "\n",
    "        self._save_config()\n",
    "        self._write_metadata()\n",
    "\n",
    "    def _save_config(self):\n",
    "        with open(os.path.join(self.result_dir, \"config.json\"), \"w\") as f:\n",
    "            json.dump(self.config, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    def _write_metadata(self):\n",
    "        metadata = {\n",
    "            \"experiment_name\": self.exp_name,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "        with open(os.path.join(self.result_dir, \"meta.json\"), \"w\") as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "\n",
    "    def save_norm_params(self, norm, feature_names=[\"F\", \"T\", \"Hdc\", \"Pcv\"]):\n",
    "        \"\"\"\n",
    "        å°‡æ¨™æº–åŒ–åƒæ•¸å­˜æˆï¼š\n",
    "        {\n",
    "          \"CH467160\": [\n",
    "             [mean_F, std_F],\n",
    "             [mean_T, std_T],\n",
    "             [mean_Hdc, std_Hdc],\n",
    "             [mean_N, std_N],\n",
    "             [mean_Pcv, std_Pcv],\n",
    "          ]\n",
    "        }\n",
    "        \"\"\"\n",
    "        # å¾ exp_name å‰åŠæ®µå–å‡º material\n",
    "        material_key = self.exp_name.split('_')[0]\n",
    "\n",
    "        # ç›´æ¥æŠŠ norm (list of [mean, std]) ç•¶æˆ value\n",
    "        output = {material_key: norm}\n",
    "\n",
    "        # å¯«æª”\n",
    "        with open(os.path.join(self.result_dir, \"norm_params.json\"), \"w\") as f:\n",
    "            json.dump(output, f, indent=4, ensure_ascii=False)\n",
    "        print(\n",
    "            f\"âœ… Normalization parameters saved to {os.path.join(self.result_dir, 'norm_params.json')}\"\n",
    "        )\n",
    "\n",
    "    def save_summary(self, best_epoch, best_val_loss, best_loss_H,\n",
    "                     best_loss_Pcv, model_save_path, elapsed):\n",
    "        summary = {\n",
    "            \"exp_name\": self.exp_name,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"duration_sec\": elapsed,\n",
    "            \"config\": self.config,\n",
    "            \"best_model\": {\n",
    "                \"path\": model_save_path,\n",
    "                \"epoch\": best_epoch,\n",
    "                \"val_loss\": best_val_loss,\n",
    "                \"loss_H\": best_loss_H,\n",
    "                \"loss_Pcv\": best_loss_Pcv\n",
    "            },\n",
    "            \"note\": note,\n",
    "            \"note detail\": note_detail\n",
    "        }\n",
    "        with open(os.path.join(self.result_dir, \"summary.json\"), \"w\") as f:\n",
    "            json.dump(summary, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(norm, train_loader, valid_loader, logger):\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "    model = MMINet(norm=norm).to(device)\n",
    "    print(\"=== Start Train  ===\")\n",
    "    print(r\"\"\"\n",
    "          \n",
    "          \n",
    "â €â €â €â €â¢€â¡¤â –â ‹â ‰â ‰â ‰â ‰â ™â ²â£¦â£€â €â €â €â €â €\n",
    "â €â €â €â¡´â ‹â €â €â €â €â €â €â €â €â €â ˆâ »â£¦â¡€â €â €â €\n",
    "â €â €â¡¼â¢â¡ â¢¼â â €â¢±â¢„â£€â €â €â €â €â €â â¢¿â¡„â €â €\n",
    "â €â£¸â â €â£§â£¼â €â €â£§â£¼â ‰â €â €â €â €â €â â¢¬â£·â €â €\n",
    "â¡¼â£¿â¢€â €â£¿â¡Ÿâ €â €â£¿â£¿â €â €â €â €â €â €â €â €â¢¹â£§â €\n",
    "â£‡â¢¹â €â â ˆâ €â ‰â ƒâ ˆâ ƒâ €â €â €â €â €â €â €â €â¡°â¢¸â¡‡\n",
    "â ™â¢¿â£§â¡€â €â €â €â €â €â €â €â €â €â €â ˆâ£â£ˆâ£‰â£¤â ¿â \n",
    "â €â£ â£¾â£¿â ¤â¡€â €â €â €â €â €â¢€â£¤â£¶â£¿â£¿â£¿â£¿â£…â €â €\n",
    "â¢°â£§â£¿â£¿â£¿â£¦â£‰â¡â ’â ’â¢²â£¿â£¿â£¿â£¿â£¿â£¿â£¶â£¿â£§â €\n",
    "â ˜â ¿â¢¿â£¿â£¿â£¿â¡¿â ¿â ›â ¿â ¿â ¿â£¿â£¿â£¿â£¿â£¿â£¿â¡¿â Ÿâ €\n",
    "â €â €â €â €â €â €â €â €â €â €â €â €â €â €â ‰â ‰â â €â €â €â €\n",
    "\n",
    "â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¢€â£´â¡›â ‰â¢¯â£’â¢¤â¡€â €â €â €â €\n",
    "â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¢ â¡–â¢¡â¡â¡„â¢€â¡€â ˆâ¢™â¢®â¡³â¡„â €â €\n",
    "â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡ â £â €â ‚â €â ˆâ €â €â ˆâ£ˆâ ·â¢‰â ƒâ €â €\n",
    "â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡°â ‹â¡â¡€â €â €â €â €â €â£â ¼â¢â¡â €â €â €\n",
    "â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â£ â £â¢â â ˆâ †â£€â¡ˆâ €â ²â¢ƒâ¢ â â €â €â €â €\n",
    "â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â â ‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â£°â¡…â ‚â „â ‚â ˆâ â¢ â “â¢¢â ±â£¨â ƒâ €â €â €â €â €\n",
    "â €â €â €â €â €â €â €â €â €â €â €â£ â£´â ¤â£¤â¢¤â£¤â£€â €â €â €â €â €â €â €â €â €â €â €â €â €â €â£¼â —â €â €â €â €â ‚â ˆâ €â ˆâ „â£¼â â €â €â €â €â €â €\n",
    "â €â €â €â €â €â €â €â €â €â¢ â£¾â¡Ÿâ£¬â ›â£­â¢«â¡â£¶â¢³â£¦â¡„â €â €â €â €â €â €â €â €â €â €â¡â ‚â €â €â €â €â¢¢â €â €â €â €â£¾â ƒâ €â €â €â €â €â €â €\n",
    "â €â €â €â €â €â €â €â €â €â£¾â¡“â ¤â â â ±â¡ˆâ¡œâ¢¥â¢»â£¹â¢»â ·â¡¶â£¶â¡¶â¢¶â£¶â£¦â£¤â¡€â£´â Ÿâ €â €â €â €â €â €â €â â  â¢©â â €â €â €â €â €â €â €â €\n",
    "â €â €â €â €â €â €â €â €â£¼â¡§â ™â¢¢â â¡ˆâ €â €â ˆâ €â â ‹â „â¢Šâ ‘â „â ˜â¡€â¢â¡â£¿â¡Ÿâ ƒâ €â €â €â €â €â €â €â €â €â£°â â €â €â €â €â €â €â €â €â €\n",
    "â €â €â €â €â €â €â¢€â£¾â¡¿â¢¤â¡â¢‚â ¡â €â €â €â €â €â €â¡ˆâ â¢â Šâ ´â “â â¡œâ£¾â â €â €â €â €â €â €â €â €â €â €â¢ â â €â €â €â €â €â €â €â €â €â €\n",
    "â €â €â €â €â¢€â£´â£¿â¢Ÿâ¡½â£‚â –â¡„â¢‚â¢€â£´â ¼â¡â¡¿â£›â¡â ›â ›â ¾â£¦â£¤â£¾â£½â¡â €â €â €â €â €â €â €â €â €â¢€â ‚â¡Ÿâ €â €â €â €â €â €â €â €â €â €â €\n",
    "â €â €â €â¢ â£¾â£â ³â¢â£–â ¹â£â¡œâ£§â¡¿â ¡â¢â ±â ‘â ¨â ˆâ â €â €â ˆâ ‰â£»â¡â â¡€â €â €â €â €â €â €â €â €â¢â£ºâ â €â €â €â €â €â €â €â €â €â €â €\n",
    "â €â €â €â¢¸â¡—â¡®â£™â ¦â¡¸â¡™â£¼â¡¿â¢‹â €â â €â €â €â €â €â €â €â €â €â¢€â¡Ÿâ  â â €â €â €â €â €â €â €â €â €â¢ â¡â €â €â €â €â €â €â €â €â €â €â €â €\n",
    "â €â €â €â¢¸â£‡â¡³â¡µâ£Šâ •â£©â¡Ÿâ €â €â €â €â €â €â €â €â €â €â €â €â €â¡¾â â â €â €â €â €â €â €â €â €â¢€â €â£²â â €â €â €â €â €â €â €â €â €â €â €â €\n",
    "â €â €â €â¢¸â£¿â£Ÿâ£¡â¢¦â£½â¡Ÿâ â â €â¢€â  â €â €â €â €â €â €â €â €â¢ â¡‡â ˜â €â €â €â €â €â €â €â €â €â£¸â¢¸â ƒâ£€â£€â¡€â €â €â €â €â €â €â €â €â €â €\n",
    "â €â €â €â¢¸â¡¿â â¡‰â£¤â£¿â ¡â¢ˆâ ¤â¢â ‚â €â €â €â €â €â €â €â €â €â£â¢¡â €â¢€â €â €â €â €â €â €â €â£´â¢‹â£¿â ¾â ƒâ ’â ˜â ³â£¶â£„â €â €â €â €â €â €â €\n",
    "â €â €â¢ â£â£µâ ¶â£¿â£¿â¢â¡â ¤â ’â  â â ‚â €â €â €â €â €â €â €â¢°â¡â¢†â ‚â „â €â €â €â €â €â €â ©â¢„â£¿â ‹â â €â €â â â ¡â¡˜â£¿â €â €â €â €â €â €\n",
    "â €â£°â£¿â¡Ÿâ¢¡â£¾â¢¿â£Œâ ³â¡˜â ¤â ‰â „â €â €â €â €â €â €â €â €â €â¡¾â¡‘â Œâ¡â ˆâ „â €â ‚â¢â â¢Œâ¢‚â£¿â¢ƒâ ‰â €â €â  â¢ˆâ  â¡‘â¢†â¢½â €â €â €â €â €â €\n",
    "â¢€â£¿â¡Ÿâ¢¡â£¿â¡Ÿâ¢®â¢Œâ¢£â â „â â €â €â €â €â €â €â €â €â €â£°â¢â ±â ˆâ „â ¡â €â Œâ¡â¢€â ‹â¢¤â£«â ‡â ‚â €â €â €â â¡€â ¤â ±â£ˆâ¡Ÿâ €â €â €â €â €â €\n",
    "â¢¸â£¿â£‡â£³â¡¯â¡â¢®â¡˜â¢„â ƒâ Œâ €â €â €â €â €â €â €â €â¢€â£¼â â¡Œâ ¢â â Œâ  â¢â ‚â ”â¡ˆâ œâ£²â â €â €â €â €â  â â¢€â¢€â ‚â£µâ¡·â ¤â¢¤â¡¤â£€â €â €\n",
    "â¢¸â£¿â£Ÿâ¡¶â¡¹â£â ¦â¡‘â¡ˆâ „â €â €â €â €â €â €â €â €â ¤â£¹â £â¡˜â  â â Œâ €â¡â¢‚â Œâ¡ â¢‰â£¼â ‡â €â €â €â €â €â ‚â¢ˆâ €â¡ â ˜â£´â â ˆâ €â ˆâ ºâ¢µâ¡†\n",
    "â¢¸â£¿â£¿â¢¶â¡¹â¢†â ³â „â¡â €â €â €â €â €â €â €â €â €â â  â â „â¡â Œâ €â â „â Šâ ¤â¡â¢§â¡¾â â €â €â €â €â €â €â „â â  â¢¹â¡â €â €â¡€â „â¡‘â¢ â¡‡\n",
    "â ¸â£¿â£¿â£â¡³â¢â¡šâ¢€â €â €â €â €â €â €â €â¡€â €â €â¢€â ‚â£â ‚â €â €â €â Œâ¡â¢‰â â¡Œâ¢ºâ¡‡â €â €â €â €â €â €â €â €â Šâ¢°â¡Ÿâ €â €â €â €â¢‚â â¢¯â¡œ\n",
    "â €â¢»â£¿â£¯â£—â¡ªâ¢â ‚â „â¡€â €â €â¢€â €â „â¡€â  â â  â¡ˆâ „â Œâ¡â €â â  â ˆâ „â¢‚â œâ£¹â „â €â â €â €â£€â â¢‚â â¢ â¡¿â €â €â €â €â¡€â Œâ¡ˆâ µâ¡‡\n",
    "â €â¢¸â£¿â£¿â£¿â£·â¡ â¢Œâ¡â¢€â â „â ‚â Œâ¡€â €â €â „â ¡â¢â ¨â â €â¡â ˆâ „â ¡â¢ˆâ †â¡¹â¢œâ¡€â ‚â €â €â¡â  â ˆâ¢ƒâ €â£¾â €â €â €â €â €â  â¢€â “â¡¸â¡‡\n",
    "â €â ˆâ£¿â£¿â£¿â£¿â£¿â£¶â£Œâ£¦â¡˜â¡¬â£‘â¢¢â â¡€â ‚â Œâ¡ â â ‚â â „â „â â Œâ¡â¢‚â ¬â¡‘â¢†â €â  â¢€â ‚â ¡â¢€â ‚â Œâ£¸â §â €â €â €â  â¢€â â¢¢â ˜â£¼â \n",
    "â €â €â¢¿â£¿â£¿â£¿â£¿â£¿â£¿â¡¼â£§â¢§â££â œâ£„â £â¡˜â ¤â¡ â €â €â ˜â¡€â €â „â£€â¢ƒâ¡˜â¢¤â ›â „â ˜â  â¢„â ƒâ ¤â €â „â¡€â£¿â €â €â €â  â €â €â¡˜â „â£»â¢»â €\n",
    "â €â €â¢¸â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¯â£¿â£³â¢¿â£¤â¡“â¡œâ¡â €â €â¡„â¢ƒâ „â  â¡€â¢„â ¢â¢œâ¢¢â¡‰â¡â¢ˆâ â¡ˆâ¡˜â ¤â¢‰â¡â¢¡â â¡â  â â ‚â¡€â ¡â¢â¢¨â¢´â â €\n",
    "â €â €â €â¢¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£·â£¯â£Ÿâ£¿â£½â£³â£â¢¶â£±â£Œâ ‚â Œâ¢â °â£ˆâ ²â£â ¢â¡â â €â €â â €â¢€â ‚â¢„â £â¡˜â  â¢â ‚â¡â  â¢â¢¢â¢â¡¿â €â €\n",
    "â €â €â €â ˆâ¢¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¾â£¿â£¿â£¿â£¿â£¿â£¾â£¿â£¶â£®â£¶â£¼â¢³â£â¡±â¢Œâ ¢â¡â „â£â €â ‚â¢Œâ ‚â¢†â â ‚â „â¡‚â ¥â¢‘â¡ˆâ¢²â¡¾â ƒâ €â €\n",
    "â €â €â €â €â ˆâ¢»â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£·â£¿â£¿â£¾â£µâ£¯â£¶â£â¡²â¢Œâ¡˜â¢¡â ‚â¢â  â Œâ¡â  â ˜â¡„â¢£â¢¼â¡¿â â €â €â €\n",
    "â €â €â €â €â €â €â ‰â »â¢¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¯â£¿â£½â£¶â£­â¢¦â£‰â ¦â¡˜â¢„â ¢â£‘â¢£â¡œâ£±â â €â €â €â €â €\n",
    "â €â €â €â €â €â €â €â €â €â €â ‰â ™â ¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£½â£¯â£·â£»â¢¯â£Ÿâ£¿â£¿â£·â£Ÿâ£¾â£µâ£®â£µâ£ºâ£¦â ¿â ƒâ €â €â €â €â €â €\n",
    "â €â €â €â €â €â €â €â €â €â €â €â €â €â €â ‰â ›â ¿â ¿â ¿â ¿â ¿â ¿â ¿â ¿â ›â ‹â ™â ›â ›â ›â »â ¿â ¿â ¿â£¿â£¿â£¿â£¿â¡¿â Ÿâ ›â ‰\n",
    "                        \n",
    "                        GO FUCK YOU, GO FUCK YOU\n",
    "                        \n",
    "                        \n",
    "\n",
    "                        \n",
    "    \"\"\")\n",
    "    print(\"Number of parameters: \", count_parameters(model))\n",
    "\n",
    "    criterion_H = nn.MSELoss()\n",
    "    criterion_Pcv = nn.MSELoss()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=Config.LEARNING_RATE)\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(\n",
    "        optimizer, gamma=Config.LR_SCHEDULER_GAMMA)\n",
    "\n",
    "    # Loss è¨˜éŒ„\n",
    "    best_val_loss = float('inf')\n",
    "    best_loss_Pcv = float('inf')\n",
    "    best_loss_H = float('inf')\n",
    "\n",
    "    # Early stopping ç´€éŒ„\n",
    "    # patience_counter = 0 # å–®ç´”å›ºå®šçš„æ—©åœè¨ˆæ•¸å™¨\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    fixed_idx = None\n",
    "\n",
    "    best_loss_H = float('inf')\n",
    "    best_loss_Pcv = float('inf')\n",
    "    wait_H = wait_Pcv = 0\n",
    "    MIN_DELTA = 1e-6  # ä½é€²æ­¥é–€æª»:é©—è­‰æå¤±åœ¨å¾ŒæœŸå¸¸å¡åœ¨å°æ•¸é»å¾Œå¹¾ä½ä¾†å›æŠ–å‹•ï¼›è‹¥ä¸è¨­é–€æª»ï¼Œæ¨¡å‹å¯èƒ½å› å¾®å°é›œè¨Šä¸€ç›´é‡ç½®ç­‰å¾…è¨ˆæ•¸ï¼Œæ°¸é è§¸ç™¼ä¸äº†æ—©åœ\n",
    "    PATIENCE_H = 150\n",
    "    PATIENCE_PCV = 150\n",
    "    joint_phase = False\n",
    "\n",
    "    # ä¿å­˜æ¯å€‹ epoch çš„æ™‚é–“\n",
    "    epoch_times = []\n",
    "    # Logger ç´€éŒ„\n",
    "    best_epoch = 0\n",
    "    history = {\n",
    "        \"epoch\": [],\n",
    "        \"train_loss\": [],\n",
    "        \"val_loss\": [],\n",
    "        \"loss_H\": [],\n",
    "        \"loss_Pcv\": []\n",
    "    }\n",
    "\n",
    "    for epoch in range(Config.NUM_EPOCHS):\n",
    "\n",
    "        t0 = time.perf_counter()\n",
    "        alpha = (epoch + 1) / Config.NUM_EPOCHS\n",
    "\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "\n",
    "        for inputs, features, amps, s0, target_H, target_Pcv in train_loader:\n",
    "\n",
    "            inputs, features, amps, s0, target_H, target_Pcv = inputs.to(\n",
    "                device), features.to(device), amps.to(device), s0.to(\n",
    "                    device), target_H.to(device), target_Pcv.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.autocast(device_type=\"cuda\"):\n",
    "                outputs_H, outputs_Pcv = model(inputs, features, amps,\n",
    "                                               s0)  # æ¨¡å‹çš„è¼¸å‡º\n",
    "                loss_H = criterion_H(outputs_H, target_H)  # ä½¿ç”¨çœŸå¯¦çš„ H(t) è¨ˆç®—æå¤±\n",
    "                loss_Pcv = criterion_Pcv(outputs_Pcv, target_Pcv)\n",
    "\n",
    "                loss = (1 - alpha) * loss_H + alpha * loss_Pcv\n",
    "                # alpha = 0.5\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        scheduler.step()  # æ›´æ–°å­¸ç¿’ç‡\n",
    "        print(\n",
    "            f\"Epoch {epoch+1} | Learning Rate: {scheduler.get_last_lr()[0]:.6f}\"\n",
    "        )\n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)  # **è¨˜éŒ„ Train Loss**\n",
    "\n",
    "        # ------------------------------vaildation------------------------------\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_loss_H = 0.0\n",
    "        val_loss_Pcv = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, features, amps, s0, target_H, target_Pcv in valid_loader:\n",
    "                inputs, features, amps, s0, target_H, target_Pcv = inputs.to(\n",
    "                    device), features.to(device), amps.to(device), s0.to(\n",
    "                        device), target_H.to(device), target_Pcv.to(device)\n",
    "\n",
    "                outputs_H, outputs_Pcv = model(inputs, features, amps,\n",
    "                                               s0)  # æ¨¡å‹çš„è¼¸å‡º\n",
    "                loss_H = criterion_H(outputs_H, target_H)  # ä½¿ç”¨çœŸå¯¦çš„ H(t) è¨ˆç®—æå¤±\n",
    "                loss_Pcv = criterion_Pcv(outputs_Pcv, target_Pcv)\n",
    "\n",
    "                alpha = (epoch + 1) / Config.NUM_EPOCHS\n",
    "                # alpha = 0.5\n",
    "                loss = (1 - alpha) * loss_H + alpha * loss_Pcv\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                val_loss_H += loss_H.item()\n",
    "                val_loss_Pcv += loss_Pcv.item()\n",
    "\n",
    "        # æ±‚é©—è­‰é›†å¹³å‡\n",
    "        val_loss_H /= len(valid_loader)\n",
    "        val_loss_Pcv /= len(valid_loader)\n",
    "        val_loss /= len(valid_loader)\n",
    "        val_losses.append(val_loss)  # **è¨˜éŒ„ Validation Loss**\n",
    "\n",
    "        # â”€â”€â”€ è¨ˆç®—ä¸¦è¼¸å‡ºé€™å€‹ epoch çš„è€—æ™‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        te = time.perf_counter() - t0\n",
    "        epoch_times.append(te)\n",
    "        print(f\"---\")\n",
    "        print(f\"alpha: {alpha:.3f}\")\n",
    "        print(\n",
    "            f\"Epoch {epoch+1}, loss_H: {val_loss_H :.6f}, loss_Pcv: {val_loss_Pcv:.6f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Epoch {epoch+1} | Train Loss: {train_loss:.6f} | Val Loss: {val_loss:.6f} | Time: {te:.2f}s\"\n",
    "        )\n",
    "        print(\n",
    "            f\"(Best Epoch {best_epoch} | best H: {best_loss_H:.6f}| best Pcv: {best_loss_Pcv:.6f}| val_loss : {val_loss:.6f})\"\n",
    "        )\n",
    "\n",
    "        # â”€â”€â”€ è¨˜éŒ„è¨“ç·´æ­·å² â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        history[\"epoch\"].append(epoch + 1)\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"loss_H\"].append(val_loss_H)\n",
    "        history[\"loss_Pcv\"].append(val_loss_Pcv)\n",
    "\n",
    "        # ======================================================ç¹ªè£½è¨“ç·´æƒ…æ³======================================================\n",
    "\n",
    "        if (epoch + 1) % plot_interval == 0:\n",
    "\n",
    "            # ç¬¬ä¸€æ¬¡ç”¢ç”Ÿå›ºå®šçš„éš¨æ©Ÿç´¢å¼•\n",
    "            if fixed_idx is None:\n",
    "                batch_size_fix = 3\n",
    "                fixed_idx = torch.randperm(batch_size_fix)[:train_show_sample]\n",
    "\n",
    "            # # -------------------------è¨­å®šåœ–è¡¨H(t)æ¯”è¼ƒ---------------------------------------\n",
    "\n",
    "            # outputs = [fixed_idx, :downsample,\n",
    "            #  0].detach().cpu().numpy()\n",
    "            # targets_np = target_H[fixed_idx, :downsample,\n",
    "            #                       0].detach().cpu().numpy()\n",
    "\n",
    "            # plt.figure(figsize=(12, 6))\n",
    "\n",
    "            # for i in range(outputs.shape[0]):  # æ¯ä¸€æ‰¹æ•¸æ“šç¹ªè£½ä¸€å€‹åœ–è¡¨\n",
    "            #     plt.plot(outputs[i, :, 0],\n",
    "            #              label=f\"Pred: Sample {i+1}\",\n",
    "            #              linestyle='--',\n",
    "            #              marker='o')\n",
    "            #     plt.plot(targets[i, :, 0],\n",
    "            #              label=f\"Target: Sample {i+1}\",\n",
    "            #              linestyle='-',\n",
    "            #              marker='x')\n",
    "\n",
    "            # # æ·»åŠ æ¨™é¡Œå’Œæ¨™ç±¤\n",
    "            # plt.title(f\"Compare - Epoch {epoch + 1}\", fontsize=16)\n",
    "            # plt.xlabel(\"Index\", fontsize=14)\n",
    "            # plt.ylabel(\"Value\", fontsize=14)\n",
    "            # plt.legend(loc=\"upper right\", fontsize=12)\n",
    "            # plt.grid(alpha=0.5)\n",
    "\n",
    "            # # é¡¯ç¤ºåœ–è¡¨\n",
    "            # plt.show()\n",
    "            # # -------------------------è¨­å®šåœ–è¡¨H(t)æ¯”è¼ƒ çµæŸ---------------------------------------\n",
    "\n",
    "            # # -------------------------è¨­å®šåœ–è¡¨B-Hæ¯”è¼ƒ---------------------------------------\n",
    "            # å–å°æ‡‰ sample\n",
    "            outputs_np = outputs_H[fixed_idx, -downsample:,\n",
    "                                   0].detach().cpu().numpy()\n",
    "            targets_np = target_H[fixed_idx, -downsample:,\n",
    "                                  0].detach().cpu().numpy()\n",
    "            B_seq_np = inputs[fixed_idx, -downsample:,\n",
    "                              0].detach().cpu().numpy()\n",
    "\n",
    "            # è¨­å®šåœ–è¡¨\n",
    "            plt.figure()\n",
    "\n",
    "            for i in range(train_show_sample):  # æ¯ä¸€æ‰¹æ•¸æ“šç¹ªè£½ä¸€å€‹åœ–è¡¨\n",
    "                plt.plot(outputs_np[i],\n",
    "                         B_seq_np[i],\n",
    "                         label=f\"Pred: Sample {i+1}\",\n",
    "                         markersize=1)\n",
    "\n",
    "                plt.plot(targets_np[i],\n",
    "                         B_seq_np[i],\n",
    "                         label=f\"Target: Sample {i+1}\",\n",
    "                         alpha=0.5)\n",
    "\n",
    "            # æ·»åŠ æ¨™é¡Œå’Œæ¨™ç±¤\n",
    "            plt.title(f\"Compare - Epoch {epoch + 1}\")\n",
    "            plt.xlabel(\"Index\")\n",
    "            plt.ylabel(\"Value\")\n",
    "            plt.grid(alpha=0.5)\n",
    "            plt.legend()\n",
    "            if save_figure == True:\n",
    "                figure_save_path1 = os.path.join(\n",
    "                    figure_save_base_path,\n",
    "                    f\"Compare_Epoch {epoch + 1}.svg\")  # å®šç¾©æ¨¡å‹ä¿å­˜æª”å\n",
    "                plt.savefig(figure_save_path1)\n",
    "            plt.show()\n",
    "            # # -------------------------è¨­å®šåœ–è¡¨B-Hæ¯”è¼ƒ END---------------------------------------\n",
    "        # ======================================================ç¹ªè£½è¨“ç·´æƒ…æ³  END ======================================================\n",
    "\n",
    "        # ======================================================Early stop======================================================\n",
    "        # if val_loss < best_val_loss:\n",
    "        #     best_val_loss = val_loss\n",
    "        #     best_epoch = epoch + 1\n",
    "        #     best_loss_H = val_loss_H\n",
    "        #     best_loss_Pcv = val_loss_Pcv\n",
    "        #     torch.save(model.state_dict(), model_save_path)  # ä¿å­˜æœ€ä½³æ¨¡å‹\n",
    "        #     print(\n",
    "        #         f\"â†’Saving model at epoch {epoch+1} with validation loss {val_loss:.6f}...\"\n",
    "        #     )\n",
    "        #     patience_counter = 0\n",
    "        # else:\n",
    "        #     patience_counter += 1\n",
    "        #     print(\n",
    "        #         f\"  ç„¡æ”¹å–„ï¼Œpatience_counter = {patience_counter}/{Config.EARLY_STOPPING_PATIENCE}\"\n",
    "        #     )\n",
    "\n",
    "        # if patience_counter >= Config.EARLY_STOPPING_PATIENCE:\n",
    "        #     print(\"Early stopping triggered.\")\n",
    "        #     break\n",
    "\n",
    "        # --- Hè·ŸPcvä¸€å€‹çµæŸå°±çµæŸ ---\n",
    "        # joint_phase = (alpha >= SWITCH_ALPHA)  # åˆ¤æ–·ç¾åœ¨åœ¨å“ªä¸€æ®µ\n",
    "\n",
    "        # if not joint_phase:  # â‘  åªçœ‹ H\n",
    "        #     if val_loss_H < best_loss_H - MIN_DELTA:\n",
    "        #         best_loss_H = val_loss_H\n",
    "        #         best_epoch = epoch + 1\n",
    "        #         wait_H = 0\n",
    "        #         torch.save(model.state_dict(), model_save_path)\n",
    "        #         print(f\"âœ… Save best H @ epoch {epoch+1}\")\n",
    "        #     else:\n",
    "        #         wait_H += 1\n",
    "        #         print(f\"  H ç„¡æ”¹å–„ï¼Œwait_H={wait_H}/{PATIENCE_H}\")\n",
    "        #     if wait_H >= PATIENCE_H:\n",
    "        #         print(\"ğŸ”¸ Early-Stop (H) è§¸ç™¼\")\n",
    "        #         break\n",
    "        # else:  # â‘¡ åªçœ‹ Pcv\n",
    "        #     if val_loss_Pcv < best_loss_Pcv - MIN_DELTA:\n",
    "        #         best_loss_Pcv = val_loss_Pcv\n",
    "        #         best_epoch = epoch + 1\n",
    "        #         wait_Pcv = 0\n",
    "        #         torch.save(model.state_dict(), model_save_path)\n",
    "        #         print(f\"âœ… Save best Pcv @ epoch {epoch+1}\")\n",
    "        #     else:\n",
    "        #         wait_Pcv += 1\n",
    "        #         print(f\"  Pcv ç„¡æ”¹å–„ï¼Œwait_Pcv={wait_Pcv}/{PATIENCE_PCV}\")\n",
    "        #     if wait_Pcv >= PATIENCE_PCV:\n",
    "        #         print(\"ğŸ”¸ Early-Stop (Pcv) è§¸ç™¼\")\n",
    "        #         break\n",
    "\n",
    "        # --- HçµæŸæ¥è‘—Pcv ---\n",
    "        if not joint_phase:  # H-phase\n",
    "            if val_loss_H < best_loss_H - MIN_DELTA:\n",
    "                best_loss_H = val_loss_H\n",
    "                best_loss_Pcv = val_loss_Pcv\n",
    "                best_epoch = epoch + 1\n",
    "                wait_H = 0\n",
    "                torch.save(model.state_dict(), model_save_path)\n",
    "                print(f\"âœ… Save best H @ epoch {best_epoch}\")\n",
    "            else:\n",
    "                wait_H += 1\n",
    "                print(f\"  H ç„¡æ”¹å–„ wait_H={wait_H}/{PATIENCE_H}\")\n",
    "\n",
    "            if wait_H >= PATIENCE_H:  # â† ä¸å† breakï¼\n",
    "                print(\"ğŸ”¸ H æ—©åœ â†’ åˆ‡åˆ° Pcv-phase\")\n",
    "                joint_phase = True  # åˆ‡æ——æ¨™\n",
    "                wait_Pcv = 0  # é‡è¨­è¨ˆæ•¸\n",
    "                best_loss_Pcv = float('inf')  # é‡æ–°è¨ˆæœ€ä½³ Pcv\n",
    "                continue  # ç›´æ¥ä¸‹ä¸€å€‹ epoch\n",
    "\n",
    "        else:  # Pcv-phase\n",
    "            if val_loss_Pcv < best_loss_Pcv - MIN_DELTA:\n",
    "                best_loss_H = val_loss_H\n",
    "                best_loss_Pcv = val_loss_Pcv\n",
    "                best_epoch = epoch + 1\n",
    "                wait_Pcv = 0\n",
    "                torch.save(model.state_dict(), model_save_path)\n",
    "                print(f\"âœ… Save best Pcv @ epoch {best_epoch}\")\n",
    "            else:\n",
    "                wait_Pcv += 1\n",
    "                print(f\"  Pcv ç„¡æ”¹å–„ wait_Pcv={wait_Pcv}/{PATIENCE_PCV}\")\n",
    "\n",
    "            if wait_Pcv >= PATIENCE_PCV:  # çœŸæ­£çµæŸ\n",
    "                print(\"ğŸ”¸ Pcv æ—©åœè§¸ç™¼ï¼Œæ•´é«”è¨“ç·´çµæŸ\")\n",
    "                break\n",
    "\n",
    "        # ======================================================Early stop======================================================\n",
    "\n",
    "    print(f\"Training complete. Best model saved at {model_save_path}.\")\n",
    "    elapsed = time.perf_counter() - start_time  # â† è¨“ç·´çµæŸï¼Œè¨ˆç®—è€—æ™‚\n",
    "    hrs = int(elapsed // 3600)\n",
    "    mins = int((elapsed % 3600) // 60)\n",
    "    secs = elapsed % 60\n",
    "    print(f\"è¨“ç·´ç¸½è€—æ™‚ï¼š{hrs} å°æ™‚ {mins} åˆ† {secs:.2f} ç§’\")\n",
    "    logger.save_summary(best_epoch, best_val_loss, best_loss_H, best_loss_Pcv,\n",
    "                        model_save_path, elapsed)\n",
    "\n",
    "    hist_df = pd.DataFrame(history)\n",
    "\n",
    "    json_path = os.path.join(result_dir, \"training_history.json\")\n",
    "    hist_df.to_json(json_path, orient=\"records\", force_ascii=False, indent=2)\n",
    "    print(f\"âœ… å·²å„²å­˜è¨“ç·´æ­·ç¨‹åˆ° {json_path}\")\n",
    "\n",
    "    # ==============================ç¹ªè£½ Train Loss èˆ‡ Validation Loss åœ–==============================\n",
    "    # plt.figure(figsize=(10, 5))\n",
    "    # plt.plot(\n",
    "    #     range(1,\n",
    "    #           len(train_losses) + 1),\n",
    "    #     train_losses,\n",
    "    #     label=\"Train Loss\",\n",
    "    # )\n",
    "    # plt.plot(range(1,\n",
    "    #                len(val_losses) + 1),\n",
    "    #          val_losses,\n",
    "    #          label=\"Validation Loss\")\n",
    "\n",
    "    # plt.xlabel(\"Epochs\")\n",
    "    # plt.ylabel(\"Loss\")\n",
    "    # plt.title(\"Training & Validation Loss Curve\")\n",
    "    # plt.legend()\n",
    "    # plt.grid(alpha=0.5)\n",
    "    # if save_figure == True:\n",
    "    #     # å°‡åœ–è¡¨ä¿å­˜ç‚º SVG æ ¼å¼\n",
    "    #     figure_save_path2 = os.path.join(figure_save_base_path,\n",
    "    #                                      \"Training_Validation_Loss_Curve.svg\")\n",
    "    #     plt.savefig(figure_save_path2)\n",
    "    # plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    # --- â‘  Train / Val Total loss ---\n",
    "    plt.plot(range(1,\n",
    "                   len(train_losses) + 1),\n",
    "             train_losses,\n",
    "             label=\"Train Total\")\n",
    "    plt.plot(range(1, len(val_losses) + 1), val_losses, label=\"Val Total\")\n",
    "\n",
    "    # âœ â‘¡ å¦å¤–ç•« H-lossã€Pcv-loss  (é€æ˜åº¦ä½ä¸€é»)\n",
    "    plt.plot(range(1,\n",
    "                   len(history[\"loss_H\"]) + 1),\n",
    "             history[\"loss_H\"],\n",
    "             label=\"Val H\",\n",
    "             alpha=0.4,\n",
    "             ls=\"--\")\n",
    "    plt.plot(range(1,\n",
    "                   len(history[\"loss_Pcv\"]) + 1),\n",
    "             history[\"loss_Pcv\"],\n",
    "             label=\"Val Pcv\",\n",
    "             alpha=0.4,\n",
    "             ls=\"--\")\n",
    "\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training & Validation Loss Curve\")\n",
    "    plt.grid(alpha=0.5)\n",
    "\n",
    "    # âœ â‘¢ æ¨™å‡ºæœ€ä½³æ¨¡å‹ epoch\n",
    "    plt.axvline(best_epoch, ls=\":\", lw=1, c=\"k\", label=f\"best @ {best_epoch}\")\n",
    "\n",
    "    plt.legend()\n",
    "\n",
    "    # âœ â‘£ æ°¸ä¹…å­˜åœ–\n",
    "    fig_loss_path = os.path.join(result_dir, \"loss_curve_final.svg\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fig_loss_path)\n",
    "    if not save_figure:\n",
    "        plt.close()  # ä¸é¡¯ç¤ºç›´æ¥é—œ\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "    # ==============================ç¹ªè£½ Train Loss èˆ‡ Validation Loss åœ– END==============================\n",
    "\n",
    "    # ===================================ä½¿ç”¨æœ€ä½³æ¨¡å‹ä¾†ç”¢ç”Ÿé©—è­‰çµæœ=============================\n",
    "    model.load_state_dict(torch.load(model_save_path))  # è¼‰å…¥æœ€ä½³æ¨¡å‹\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, features, amps, s0, target_H, target_Pcv in valid_loader:\n",
    "            inputs, features, amps, s0, target_H, target_Pcv = inputs.to(\n",
    "                device), features.to(device), amps.to(device), s0.to(\n",
    "                    device), target_H.to(device), target_Pcv.to(device)\n",
    "\n",
    "            outputs_H, outputs_Pcv = model(inputs, features, amps, s0)\n",
    "            break  # åªä½¿ç”¨ä¸€æ‰¹é©—è­‰æ•¸æ“šé€²è¡Œå¯è¦–åŒ–\n",
    "\n",
    "    # é¸å–å°æ‡‰è³‡æ–™ï¼ˆindex tensor è¦å…ˆè½‰ list æ‰èƒ½ index numpyï¼‰\n",
    "    outputs_np = outputs_H[fixed_idx, -downsample:, 0].detach().cpu().numpy()\n",
    "    targets_np = target_H[fixed_idx, -downsample:, 0].detach().cpu().numpy()\n",
    "    B_seq_np = inputs[fixed_idx, -downsample:, 0].detach().cpu().numpy()\n",
    "\n",
    "    # è¨­å®šåœ–è¡¨\n",
    "    plt.figure()\n",
    "    for i in range(train_show_sample):  # æ¯ä¸€æ‰¹æ•¸æ“šç¹ªè£½ä¸€å€‹åœ–è¡¨\n",
    "        plt.plot(outputs_np[i], B_seq_np[i], label=f\"Pred: Sample {i+1}\")\n",
    "        plt.plot(targets_np[i],\n",
    "                 B_seq_np[i],\n",
    "                 label=f\"Target: Sample {i+1}\",\n",
    "                 alpha=0.7)\n",
    "\n",
    "        # æ·»åŠ æ¨™é¡Œå’Œæ¨™ç±¤\n",
    "        plt.title(f\"Best Model - Predicted vs Target\")\n",
    "        plt.xlabel(\"H(A/m)\")\n",
    "        plt.ylabel(\"B(T)\")\n",
    "        plt.grid(alpha=0.5)\n",
    "        plt.legend()\n",
    "\n",
    "        if save_figure == True:\n",
    "            figure_save_path3 = os.path.join(\n",
    "                figure_save_base_path,\n",
    "                f\"Best Model Predicted vs Target Sample.svg\")  # å®šç¾©æ¨¡å‹ä¿å­˜æª”å\n",
    "            plt.savefig(figure_save_path3)\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # ===================================ä½¿ç”¨æœ€ä½³æ¨¡å‹ä¾†ç”¢ç”Ÿé©—è­‰çµæœ END============================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Train!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Pythonç”¨\n",
    "    # BASE_DIR = Path(__file__).resolve().parent\n",
    "    # os.chdir(BASE_DIR)\n",
    "    # print(\"ğŸ‘‰ Switch CWD to script folder:\", os.getcwd())\n",
    "\n",
    "    data_B, data_F, data_T, data_H, data_Pcv, data_Hdc, data_N, data_Duty_P, data_Duty_N = load_dataset(\n",
    "        material)\n",
    "\n",
    "    GLOBAL_B_MAX = np.abs(data_B).max()\n",
    "    GLOBAL_H_MAX = np.abs(data_H).max()\n",
    "\n",
    "    train_loader, valid_loader, norm = get_dataloader(data_B, data_F, data_T,\n",
    "                                                      data_H, data_N, data_Hdc,\n",
    "                                                      data_Duty_P, data_Duty_N,\n",
    "                                                      data_Pcv, GLOBAL_B_MAX,\n",
    "                                                      GLOBAL_H_MAX)\n",
    "\n",
    "    # ---- å°ç¬¬ä¸€å€‹ batch æª¢æŸ¥ ----\n",
    "    # # inputs, features, s0, target_H, target_Pcv = next(iter(train_loader))\n",
    "    # inputs, features, amps, s0, target_H, target_Pcv = next(iter(train_loader))\n",
    "\n",
    "    # print(\"=== Batch shape check ===\")\n",
    "    # print(f\"inputs      : {inputs.shape}\")  # (batch, seq_len, 4)\n",
    "    # print(f\"features    : {features.shape}\")  # (batch, 4)\n",
    "    # print(f\"s0          : {s0.shape}\")  # (batch, operator_size)\n",
    "    # print(f\"target_H    : {target_H.shape}\")  # (batch, seq_len, 1)\n",
    "    # # print(f\"target_Pcv  : {target_Pcv.shape}\")  # (batch, 1)\n",
    "    # print()\n",
    "\n",
    "    # # é¸ä¸€ç­†æ¨£æœ¬çœ‹çœ‹æ•¸å€¼ç¯„åœ\n",
    "    # idx = 0\n",
    "    # print(\"ç¯„ä¾‹ inputs[0] (å‰ 3 å€‹æ™‚é–“é»):\")\n",
    "    # print(inputs[idx, :3, :])  # B, Î”B, dB/dt, dÂ²B/dtÂ² (å·²æ­¸ä¸€åŒ–åˆ° ~[-1,1])\n",
    "    # print(\"ç¯„ä¾‹ features[0]:\", features[idx])  # F, T, Hdc, N (å·² z-score)\n",
    "    # print(\"ç¯„ä¾‹ s0[0]:\", s0[idx, :5])  # å‰ 5 å€‹ Preisach operator ç‹€æ…‹\n",
    "    # print(\"ç¯„ä¾‹ target_H[0] (å‰ 3 é»):\", target_H[idx, :3, 0])\n",
    "    # # print(\"ç¯„ä¾‹ target_Pcv[0]:\", target_Pcv[idx, 0])\n",
    "\n",
    "    # ç”¢ç”Ÿ Loggerï¼ˆæ”¾åœ¨ train_model å‰ï¼‰\n",
    "\n",
    "    logger = TrainLogger(\n",
    "        exp_name=f\"{material}_{note}_{timestamp}\",\n",
    "        config_dict={\n",
    "            k: getattr(Config, k)\n",
    "            for k in dir(Config)\n",
    "            if not k.startswith('__') and not callable(getattr(Config, k))\n",
    "        },\n",
    "        result_dir=result_dir)\n",
    "    feature_names = [\"F\", \"T\", \"Hdc\", \"N\", \"Pcv\"]\n",
    "    logger.save_norm_params(norm, feature_names)\n",
    "\n",
    "    train_model(norm, train_loader, valid_loader, logger)  # logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B shape: (2418, 1040)\n",
      "H shape: (2418, 1040)\n",
      "F shape: (2418,)\n",
      "T shape: (2418,)\n",
      "Hdc shape: (2418,)\n",
      "N shape: (2418,)\n",
      "Duty Pos shape: (2418,)\n",
      "Duty Neg shape: (2418,)\n",
      "Pcv shape: (2418,)\n",
      "0.F, 1.T, 2.Hdc, 3.N, 4.Pcv\n",
      "\"CH467160\": [\n",
      "    [2.0, 1.0],\n",
      "    [25.0, 1.0],\n",
      "    [1245.2510986328125, 698.5667114257812],\n",
      "    [14.20181941986084, 4.4452033042907715],\n",
      "    [1.6664491891860962, 0.7449064254760742],\n",
      "]\n",
      "âœ… Normalization parameters saved to results\\20250806_uesed_for_PFC_test4_CH467160_optimizer_test\\norm_params.json\n",
      "=== Start Train  ===\n",
      "\n",
      "â£¿â£¿â£¿â£¿â£¿â£¿â£¿â ¿â ‹â£‰â£‰â£‰â£‰â£‰â£‰â¡â ‰â¢‰â¡‰â ‰â ‰â ‰â£‰â£‰â£‰â£‰â£™â ›â ›â ›â ¿â¢¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿\n",
      "â£¿â£¿â£¿â£¿â£¿â¡¿â ƒâ£´â£¿â£¿â£¿â¢Ÿâ£«â ­â£–â£›â£»â£¯â£­â£­â£½â£›â£›â£“â£’â£’â£’â£’â£’â£’â£’â£¢â£„â¡‰â »â¢¿â£¿â£¿â£¿\n",
      "â£¿â£¿â£¿â£¿â¡Ÿâ €â£¾â£¿â¡Ÿâ¢«â¡–â¢«â ’â£­â£µâ£¶â£¶â£¾â£¶â¡†â£¿â£¿â£¿â£¿â£¿â£­â¢©â£­â£¿â£­â¡â¢»â£¿â£¿â£¶â €â¢»â£¿â£¿\n",
      "â£¿â£¿â£¿â¡¿â â£¼â£¿â£¿â£¾â£¯â£–â£µâ¡¾â ¿â ›â ›â ›â ›â ¿â£¿â£¸â£¿â£¿â£¿â£¿â¡‡â£¾â£¿â£¿â£¿â£¿â£¦â£¿â£¿â£¿â¡‡â ¸â£¿â£¿\n",
      "â£¿â¡¿â ‹â €â¢šâ£›â£¿â£¿â£Ÿâ ›â£¿â â €â €â €â €â ˜â ³â¢¦â¡„â ™â£¿â£¿â¡¿â ¿â Ÿâ ‹â â €â €â €â¢™â£Ÿâ¡¿â ¿â¡¿â¢¤â¡‰â »\n",
      "â â¢ â¢¢â¡¿â ‹â£â£¤â¢ â£‰â ‘â ºâ ¿â ¿â Ÿâ¢‰â£´â£¿â£¶â£¦â£ â£¶â£¿â£¿â£·â£¦â €â£´â£¶â£¿â£¿â£¿â£¿â Ÿâ ›â ›â ¬â¡±â¡â¡€\n",
      "â €â£¿â£Ÿâ €â¡¾â¡¿â ‹â ˆâ ™â ¿â£·â£¶â£¶â£¾â£¿â£¿â£¿â¢¿â ¿â ›â¢»â£¿â£¿â£¿â£¿â£„â¡™â »â£¿â£§â£„â£€â£´â ‹â£·â£¶â¡‡â¡¿â¡‡\n",
      "â €â¢¿â¢¹â¡„â¢±â£¶â¡„â ˜â¢¿â£¶â£„â ˆâ ™â ›â ¶â ¶â£¶â£¾â €â¢â¢‰â£‰â£›â£¿â£¿â£¿â ‡â£€â£¨â£›â¡»â£¿â Ÿâ €â ¹â¡¯â¢â¡â¢€\n",
      "â£·â ˆâ ·â£‰â£¿â£¿â£¿â¡€â €â ‰â ¹â €â¢¹â£·â£¶â£†â£€â ˆâ â ¹â ¿â¢¿â£¿â£¿â£€â£€â£¾â£¿â£¿â ¿â â €â¢€â ˆâ €â£¿â¡¿â €â£¾\n",
      "â£¿â£·â£„â â¢¿â£¿â£¿â£¿â£„â ™â£¶â €â£€â ˆâ ‰â ›â â¢ â£·â£¶â£¶â£¤â €â£¤â£¤â¡„â¢€â£¤â£¤â¡„â¢°â¡‡â ˜â †â €â£¿â¡‡â¢¸â£¿\n",
      "â£¿â£¿â£¿â£§â ˆâ¢¿â£¿â£¿â£¿â£§â£ˆâ €â¢¿â£¿â£¶â£¦â €â£€â¡€â ˆâ ‰â ‰â €â ‰â ‰â â ˆâ ‰â ‰â €â €â €â €â €â €â£¿â¡‡â¢¸â£¿\n",
      "â£¿â£¿â£¿â£¿â£·â¡ˆâ »â£¿â£¿â£¿â£¿â£·â£„â¡‰â »â â¢ â£¿â£¿â£¿â£¶â €â£¦â£¤â£¤â €â£€â£€â €â£ â „â¢ â €â „â£¸â£¿â¡‡â¢¸â£¿\n",
      "â£¿â£¿â£¿â£¿â£¿â£¿â£¶â£ˆâ ‘â ®â£“â ¾â£â¡»â ¶â£¤â£„â£‰â¡™â ›â ›â €â »â ¿â Ÿâ €â ¿â ‹â â ›â €â£‰â£¡â£´â£¿â£¿â¡‡â¢¸â£¿\n",
      "â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£·â£¦â£Œâ¡™â ³â ¯â£â£²â ­â£Ÿâ£»â ¿â¢¿â£¿â£¿â£—â£’â£šâ£â£¿â£¿â£¿â£¿â ¿â£«â£¾â¡¿â£¹â£§â ˜â£¿\n",
      "â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£§â£„â¡€â ›â ¿â£§â£¤â£Ÿâ£›â£ƒâ¡¤â ¤â ¤â¢¤â£¤â£¤â£¤â£¤â£¤â£¿â¡¿â¢›â£¤â£¿â£¿â €â£¿\n",
      "â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£·â£¦â£¤â£‰â ™â »â ¿â ¿â£¿â£¿â£¿â£¿â£¿â£¿â£¶â£¶â£¶â£¿â£¿â£¿â¡¿â €â£¼â£¿\n",
      "â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¶â£¶â£¦â£¤â£¤â£‰â£‰â£™â ›â¡›â¢›â¡›â£›â£‰â£¡â£´â£¾â£¿â£¿ \n",
      "\n",
      "\n",
      "    â €â €â €â €â €â €â €â €â €â  â¡„â¢ â¡„â£ â ¤â¢¤â¡€â£¤â €â¢ â¡„â €â €â €â €â €â €â €â €â €\n",
      "â €â €â €â €â €â €â €â €â €â €â¢¹â¡â €â£¿â €â¢¨â¡‡â£¿â €â¢¸â ‡â €â €â €â €â €â €â €â €â €\n",
      "â €â €â €â €â €â €â €â €â €â €â ˆâ â €â ˆâ ‰â ‰â €â ˆâ ‰â ‰â €â €â €â €â €â €â €â €â €â €\n",
      "â €â €â €â €â¢ â ¤â£„â¢ â¡„â €â£¤â¢€â£ â¢¤â¡€â¢ â¡„â €â¡„â¢ â¡€â €â¢ â£¤â£¤â¡€â €â €â €â €\n",
      "â €â €â €â €â£™â ²â¢¦â¢¸â¡—â ’â£¿â¢¸â¡â €â£¿â¢¸â¡‡â €â¡‡â¢¸â¡‡â €â¢¸â¡‡â €â¡·â €â €â €â €\n",
      "â €â €â €â €â ˆâ ›â ‹â ˆâ ƒâ €â ‹â €â ™â ›â â €â ™â ›â â ˜â ›â ›â ˜â ›â ‹â â €â €â €â €\n",
      "â €â €â €â €â €â €â €â €â €â¢€â¡€â¢€â¡€â¢€â €â¡€â €â €â£€â €â €â €â €â €â €â €â €â €â €â €\n",
      "â €â €â €â €â €â €â €â €â €â¢¸â¡·â£¯â €â¢¸â €â¡‡â €â €â£¿â €â €â €â €â €â €â €â €â €â €â €\n",
      "â €â €â €â €â €â €â €â €â €â ˜â ƒâ ˆâ “â ˜â €â ›â ’â ‚â ›â ’â ‚â €â €â €â €â €â €â €â €â €\n",
      "â €â£€â €â¢€â¡€â¢€â£€â¡€â €â¡€â €â£€â¢€â£€â£€â¡€â¢€â£€â£€â €â£€â£€â¡€â£€â €â €â¢€â£€â£€â €\n",
      "â €â ˜â¢·â â¢°â¡â €â¢¹â „â¡‡â €â£¿â¢¸â£§â¢¤â¡‡â ˜â ¦â£¬â¡€â¡§â ¤â „â£¿â €â €â¢¸â ¤â „â €\n",
      "â €â €â ˜â €â €â ›â ’â ‹â €â ›â ’â ‹â ˜â ƒâ €â “â ˜â ²â šâ â “â ’â ‚â ›â ’â ’â ˜â €â €â €\n",
      "â €â €â €â €â €â €â €â €â €â¡€â €â¢€â €â¢€â£€â €â¢€â €â¢€â €â¢€â €â €â €â €â €â €â €â €â €\n",
      "â €â €â €â €â €â €â €â €â €â¡¿â¢†â£¸â¢ â¡â ‰â¢¹â¡ˆâ£‡â¡¼â£‡â£¼â €â €â €â €â €â €â €â €â €\n",
      "â €â €â €â €â €â €â €â €â €â ‡â ˆâ ¿â €â ³â ¶â â €â ¹â ‡â ¸â ‡â €â €â €â €â €â €â €â €â €\n",
      "â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â â „â „â „â „â „â „â „â „â ™â¢¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿\n",
      "â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â „â „â¢€â£€â£€â£€â¡€â „â¢€â£ â¡”â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿\n",
      "â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£°â¢¿â£¿â£¿â£¿â£¿â£¿â£¿â£·â¡†â¢ â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿\n",
      "â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â¡â£»â£Ÿâ£¿â£¿â¡¿â£Ÿâ£›â£¿â¡ƒâ¢¸â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿\n",
      "â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£§â£¿â£¾â£¿â£·â£¿â£·â£¿â£¿â£¿â£·â£½â£¹â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿\n",
      "â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â¡Ÿâ£Ÿâ£¿â£¿â ºâ£Ÿâ£»â£¿â£¿â£¿â¡â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿\n",
      "â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â¢¿â¡â »â µâ ¿â ¿â¢¿â£¿â£¿â¢³â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿\n",
      "â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¯â£§â ˆâ£›â£›â£¿â£¿â¡¿â£¡â£â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿\n",
      "â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â¡§â „â ™â ›â ›â¢â£´â£¿â£¿â£·â£¿â¢¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿\n",
      "â£¿â£¿â£¿â£¿â£¿â£¿â¡¿â Ÿâ ‰â „â „â¢ â „â£€â£ â£¾â£¿â£¿â¡¿â Ÿâ â „â ˆâ ›â¢¿â£¿â£¿â£¿â£¿â£¿\n",
      "â£¿â£¿â£¿â£¿â¡Ÿâ ‰â „â „â¢€â  â â ’â â ¾â ¿â¢Ÿâ ‹â â „â¢€â£€â  â â „â ‚â ˆâ »â¢¿â£¿â£¿\n",
      "â£¿â£¿â£¿â ‹â â „â¢€â¡ˆâ „â „â „â „â „â „â „â „â â ’â ‰â „â¢ â£¶â „â „â „â „â „â ˆâ «â¢¿\n",
      "â£¿â£¿â¡Ÿâ „â¢”â †â¡€â „â ˆâ¢€â „â „â „â „â „â „â „â¢„â¡€â „â ˆâ¡â¢ â ’â „â „â „â „â¢€â£‚\n",
      "â£¿â£¿â â¡€â „â „â¢‡â „â „â¢ˆâ †â „â „â¢€â ”â ‰â â ‰â ‰â £â£–â ‰â¡‚â¡”â ‚â „â¢€â ”â â „\n",
      "â£¿â¡¿â „â „â „â „â¢°â ¹â£—â£ºâ ¤â „â °â¡â „â „â „â „â „â „â ˜â¢¯â¡¶â¢Ÿâ¡ â °â „â „â „â „\n",
      "\n",
      "    \n",
      "Number of parameters:  12631\n",
      "Epoch 1 | Learning Rate: 0.001980\n",
      "---\n",
      "alpha: 0.001\n",
      "Epoch 1, loss_H: 0.042976, loss_Pcv: 1.339622\n",
      "Epoch 1 | Train Loss: 0.112613 | Val Loss: 0.043841 | Time: 5.18s\n",
      "(Best Epoch 0 | best H: inf| best Pcv: inf| val_loss : 0.043841)\n",
      "âœ… Save best H @ epoch 1\n",
      "Epoch 2 | Learning Rate: 0.001960\n",
      "---\n",
      "alpha: 0.001\n",
      "Epoch 2, loss_H: 0.024745, loss_Pcv: 0.789951\n",
      "Epoch 2 | Train Loss: 0.036751 | Val Loss: 0.025765 | Time: 5.24s\n",
      "(Best Epoch 1 | best H: 0.042976| best Pcv: 1.339622| val_loss : 0.025765)\n",
      "âœ… Save best H @ epoch 2\n",
      "Epoch 3 | Learning Rate: 0.001941\n",
      "---\n",
      "alpha: 0.002\n",
      "Epoch 3, loss_H: 0.014509, loss_Pcv: 61.675949\n",
      "Epoch 3 | Train Loss: 0.046597 | Val Loss: 0.137832 | Time: 5.15s\n",
      "(Best Epoch 2 | best H: 0.024745| best Pcv: 0.789951| val_loss : 0.137832)\n",
      "âœ… Save best H @ epoch 3\n",
      "Epoch 4 | Learning Rate: 0.001921\n",
      "---\n",
      "alpha: 0.003\n",
      "Epoch 4, loss_H: 0.011045, loss_Pcv: 14.582913\n",
      "Epoch 4 | Train Loss: 0.112009 | Val Loss: 0.049903 | Time: 5.06s\n",
      "(Best Epoch 3 | best H: 0.014509| best Pcv: 61.675949| val_loss : 0.049903)\n",
      "âœ… Save best H @ epoch 4\n",
      "Epoch 5 | Learning Rate: 0.001902\n",
      "---\n",
      "alpha: 0.003\n",
      "Epoch 5, loss_H: 0.009603, loss_Pcv: 10.180180\n",
      "Epoch 5 | Train Loss: 0.028145 | Val Loss: 0.043505 | Time: 4.98s\n",
      "(Best Epoch 4 | best H: 0.011045| best Pcv: 14.582913| val_loss : 0.043505)\n",
      "âœ… Save best H @ epoch 5\n",
      "Epoch 6 | Learning Rate: 0.001883\n",
      "---\n",
      "alpha: 0.004\n",
      "Epoch 6, loss_H: 0.008833, loss_Pcv: 2.581678\n",
      "Epoch 6 | Train Loss: 0.023544 | Val Loss: 0.019124 | Time: 4.87s\n",
      "(Best Epoch 5 | best H: 0.009603| best Pcv: 10.180180| val_loss : 0.019124)\n",
      "âœ… Save best H @ epoch 6\n",
      "Epoch 7 | Learning Rate: 0.001864\n",
      "---\n",
      "alpha: 0.005\n",
      "Epoch 7, loss_H: 0.008510, loss_Pcv: 0.770811\n",
      "Epoch 7 | Train Loss: 0.017346 | Val Loss: 0.012067 | Time: 5.31s\n",
      "(Best Epoch 6 | best H: 0.008833| best Pcv: 2.581678| val_loss : 0.012067)\n",
      "âœ… Save best H @ epoch 7\n",
      "Epoch 8 | Learning Rate: 0.001845\n",
      "---\n",
      "alpha: 0.005\n",
      "Epoch 8, loss_H: 0.008181, loss_Pcv: 0.636384\n",
      "Epoch 8 | Train Loss: 0.013175 | Val Loss: 0.011531 | Time: 5.20s\n",
      "(Best Epoch 7 | best H: 0.008510| best Pcv: 0.770811| val_loss : 0.011531)\n",
      "âœ… Save best H @ epoch 8\n",
      "Epoch 9 | Learning Rate: 0.001827\n",
      "---\n",
      "alpha: 0.006\n",
      "Epoch 9, loss_H: 0.008089, loss_Pcv: 0.577557\n",
      "Epoch 9 | Train Loss: 0.011950 | Val Loss: 0.011506 | Time: 5.50s\n",
      "(Best Epoch 8 | best H: 0.008181| best Pcv: 0.636384| val_loss : 0.011506)\n",
      "âœ… Save best H @ epoch 9\n",
      "Epoch 10 | Learning Rate: 0.001809\n",
      "---\n",
      "alpha: 0.007\n",
      "Epoch 10, loss_H: 0.008139, loss_Pcv: 0.520254\n",
      "Epoch 10 | Train Loss: 0.011243 | Val Loss: 0.011553 | Time: 5.15s\n",
      "(Best Epoch 9 | best H: 0.008089| best Pcv: 0.577557| val_loss : 0.011553)\n",
      "  H ç„¡æ”¹å–„ wait_H=1/150\n",
      "Epoch 11 | Learning Rate: 0.001791\n",
      "---\n",
      "alpha: 0.007\n",
      "Epoch 11, loss_H: 0.007861, loss_Pcv: 0.601880\n",
      "Epoch 11 | Train Loss: 0.011501 | Val Loss: 0.012217 | Time: 4.95s\n",
      "(Best Epoch 9 | best H: 0.008089| best Pcv: 0.577557| val_loss : 0.012217)\n",
      "âœ… Save best H @ epoch 11\n",
      "Epoch 12 | Learning Rate: 0.001773\n",
      "---\n",
      "alpha: 0.008\n",
      "Epoch 12, loss_H: 0.007742, loss_Pcv: 0.576336\n",
      "Epoch 12 | Train Loss: 0.011836 | Val Loss: 0.012291 | Time: 5.06s\n",
      "(Best Epoch 11 | best H: 0.007861| best Pcv: 0.601880| val_loss : 0.012291)\n",
      "âœ… Save best H @ epoch 12\n",
      "Epoch 13 | Learning Rate: 0.001755\n",
      "---\n",
      "alpha: 0.009\n",
      "Epoch 13, loss_H: 0.007724, loss_Pcv: 0.548922\n",
      "Epoch 13 | Train Loss: 0.011917 | Val Loss: 0.012414 | Time: 5.23s\n",
      "(Best Epoch 12 | best H: 0.007742| best Pcv: 0.576336| val_loss : 0.012414)\n",
      "âœ… Save best H @ epoch 13\n",
      "Epoch 14 | Learning Rate: 0.001737\n",
      "---\n",
      "alpha: 0.009\n",
      "Epoch 14, loss_H: 0.007644, loss_Pcv: 0.577851\n",
      "Epoch 14 | Train Loss: 0.012193 | Val Loss: 0.012966 | Time: 5.14s\n",
      "(Best Epoch 13 | best H: 0.007724| best Pcv: 0.548922| val_loss : 0.012966)\n",
      "âœ… Save best H @ epoch 14\n",
      "Epoch 15 | Learning Rate: 0.001720\n",
      "---\n",
      "alpha: 0.010\n",
      "Epoch 15, loss_H: 0.007575, loss_Pcv: 0.577602\n",
      "Epoch 15 | Train Loss: 0.012693 | Val Loss: 0.013275 | Time: 5.23s\n",
      "(Best Epoch 14 | best H: 0.007644| best Pcv: 0.577851| val_loss : 0.013275)\n",
      "âœ… Save best H @ epoch 15\n",
      "Epoch 16 | Learning Rate: 0.001703\n",
      "---\n",
      "alpha: 0.011\n",
      "Epoch 16, loss_H: 0.007580, loss_Pcv: 0.531730\n",
      "Epoch 16 | Train Loss: 0.012562 | Val Loss: 0.013171 | Time: 4.98s\n",
      "(Best Epoch 15 | best H: 0.007575| best Pcv: 0.577602| val_loss : 0.013171)\n",
      "  H ç„¡æ”¹å–„ wait_H=1/150\n",
      "Epoch 17 | Learning Rate: 0.001686\n",
      "---\n",
      "alpha: 0.011\n",
      "Epoch 17, loss_H: 0.007444, loss_Pcv: 0.526595\n",
      "Epoch 17 | Train Loss: 0.012609 | Val Loss: 0.013327 | Time: 4.86s\n",
      "(Best Epoch 15 | best H: 0.007575| best Pcv: 0.577602| val_loss : 0.013327)\n",
      "âœ… Save best H @ epoch 17\n",
      "Epoch 18 | Learning Rate: 0.001669\n",
      "---\n",
      "alpha: 0.012\n",
      "Epoch 18, loss_H: 0.007326, loss_Pcv: 0.562917\n",
      "Epoch 18 | Train Loss: 0.012924 | Val Loss: 0.013993 | Time: 4.95s\n",
      "(Best Epoch 17 | best H: 0.007444| best Pcv: 0.526595| val_loss : 0.013993)\n",
      "âœ… Save best H @ epoch 18\n",
      "Epoch 19 | Learning Rate: 0.001652\n",
      "---\n",
      "alpha: 0.013\n",
      "Epoch 19, loss_H: 0.007262, loss_Pcv: 0.580348\n",
      "Epoch 19 | Train Loss: 0.013588 | Val Loss: 0.014521 | Time: 5.22s\n",
      "(Best Epoch 18 | best H: 0.007326| best Pcv: 0.562917| val_loss : 0.014521)\n",
      "âœ… Save best H @ epoch 19\n",
      "Epoch 20 | Learning Rate: 0.001636\n",
      "---\n",
      "alpha: 0.013\n",
      "Epoch 20, loss_H: 0.007243, loss_Pcv: 0.505525\n",
      "Epoch 20 | Train Loss: 0.013599 | Val Loss: 0.013887 | Time: 5.17s\n",
      "(Best Epoch 19 | best H: 0.007262| best Pcv: 0.580348| val_loss : 0.013887)\n",
      "âœ… Save best H @ epoch 20\n",
      "Epoch 21 | Learning Rate: 0.001619\n",
      "---\n",
      "alpha: 0.014\n",
      "Epoch 21, loss_H: 0.007288, loss_Pcv: 0.467286\n",
      "Epoch 21 | Train Loss: 0.012887 | Val Loss: 0.013728 | Time: 5.27s\n",
      "(Best Epoch 20 | best H: 0.007243| best Pcv: 0.505525| val_loss : 0.013728)\n",
      "  H ç„¡æ”¹å–„ wait_H=1/150\n",
      "Epoch 22 | Learning Rate: 0.001603\n",
      "---\n",
      "alpha: 0.015\n",
      "Epoch 22, loss_H: 0.006948, loss_Pcv: 0.515518\n",
      "Epoch 22 | Train Loss: 0.013395 | Val Loss: 0.014407 | Time: 5.05s\n",
      "(Best Epoch 20 | best H: 0.007243| best Pcv: 0.505525| val_loss : 0.014407)\n",
      "âœ… Save best H @ epoch 22\n",
      "Epoch 23 | Learning Rate: 0.001587\n",
      "---\n",
      "alpha: 0.015\n",
      "Epoch 23, loss_H: 0.006930, loss_Pcv: 0.567466\n",
      "Epoch 23 | Train Loss: 0.014407 | Val Loss: 0.015525 | Time: 5.04s\n",
      "(Best Epoch 22 | best H: 0.006948| best Pcv: 0.515518| val_loss : 0.015525)\n",
      "âœ… Save best H @ epoch 23\n",
      "Epoch 24 | Learning Rate: 0.001571\n",
      "---\n",
      "alpha: 0.016\n",
      "Epoch 24, loss_H: 0.006737, loss_Pcv: 0.563657\n",
      "Epoch 24 | Train Loss: 0.014992 | Val Loss: 0.015648 | Time: 5.30s\n",
      "(Best Epoch 23 | best H: 0.006930| best Pcv: 0.567466| val_loss : 0.015648)\n",
      "âœ… Save best H @ epoch 24\n",
      "Epoch 25 | Learning Rate: 0.001556\n",
      "---\n",
      "alpha: 0.017\n",
      "Epoch 25, loss_H: 0.006734, loss_Pcv: 0.535452\n",
      "Epoch 25 | Train Loss: 0.014764 | Val Loss: 0.015546 | Time: 5.11s\n",
      "(Best Epoch 24 | best H: 0.006737| best Pcv: 0.563657| val_loss : 0.015546)\n",
      "âœ… Save best H @ epoch 25\n",
      "Epoch 26 | Learning Rate: 0.001540\n",
      "---\n",
      "alpha: 0.017\n",
      "Epoch 26, loss_H: 0.006884, loss_Pcv: 0.429163\n",
      "Epoch 26 | Train Loss: 0.014364 | Val Loss: 0.014204 | Time: 5.36s\n",
      "(Best Epoch 25 | best H: 0.006734| best Pcv: 0.535452| val_loss : 0.014204)\n",
      "  H ç„¡æ”¹å–„ wait_H=1/150\n",
      "Epoch 27 | Learning Rate: 0.001525\n",
      "---\n",
      "alpha: 0.018\n",
      "Epoch 27, loss_H: 0.006652, loss_Pcv: 0.466605\n",
      "Epoch 27 | Train Loss: 0.013538 | Val Loss: 0.014932 | Time: 5.09s\n",
      "(Best Epoch 25 | best H: 0.006734| best Pcv: 0.535452| val_loss : 0.014932)\n",
      "âœ… Save best H @ epoch 27\n",
      "Epoch 28 | Learning Rate: 0.001509\n",
      "---\n",
      "alpha: 0.019\n",
      "Epoch 28, loss_H: 0.006444, loss_Pcv: 0.533084\n",
      "Epoch 28 | Train Loss: 0.014313 | Val Loss: 0.016275 | Time: 4.96s\n",
      "(Best Epoch 27 | best H: 0.006652| best Pcv: 0.466605| val_loss : 0.016275)\n",
      "âœ… Save best H @ epoch 28\n",
      "Epoch 29 | Learning Rate: 0.001494\n",
      "---\n",
      "alpha: 0.019\n",
      "Epoch 29, loss_H: 0.006399, loss_Pcv: 0.526414\n",
      "Epoch 29 | Train Loss: 0.015323 | Val Loss: 0.016452 | Time: 5.03s\n",
      "(Best Epoch 28 | best H: 0.006444| best Pcv: 0.533084| val_loss : 0.016452)\n",
      "âœ… Save best H @ epoch 29\n",
      "Epoch 30 | Learning Rate: 0.001479\n",
      "---\n",
      "alpha: 0.020\n",
      "Epoch 30, loss_H: 0.006279, loss_Pcv: 0.516577\n",
      "Epoch 30 | Train Loss: 0.015344 | Val Loss: 0.016485 | Time: 5.33s\n",
      "(Best Epoch 29 | best H: 0.006399| best Pcv: 0.526414| val_loss : 0.016485)\n",
      "âœ… Save best H @ epoch 30\n",
      "Epoch 31 | Learning Rate: 0.001465\n",
      "---\n",
      "alpha: 0.021\n",
      "Epoch 31, loss_H: 0.006150, loss_Pcv: 0.527101\n",
      "Epoch 31 | Train Loss: 0.015557 | Val Loss: 0.016916 | Time: 5.21s\n",
      "(Best Epoch 30 | best H: 0.006279| best Pcv: 0.516577| val_loss : 0.016916)\n",
      "âœ… Save best H @ epoch 31\n",
      "Epoch 32 | Learning Rate: 0.001450\n",
      "---\n",
      "alpha: 0.021\n",
      "Epoch 32, loss_H: 0.006088, loss_Pcv: 0.514208\n",
      "Epoch 32 | Train Loss: 0.015833 | Val Loss: 0.016928 | Time: 5.27s\n",
      "(Best Epoch 31 | best H: 0.006150| best Pcv: 0.527101| val_loss : 0.016928)\n",
      "âœ… Save best H @ epoch 32\n",
      "Epoch 33 | Learning Rate: 0.001435\n",
      "---\n",
      "alpha: 0.022\n",
      "Epoch 33, loss_H: 0.006068, loss_Pcv: 0.526818\n",
      "Epoch 33 | Train Loss: 0.015953 | Val Loss: 0.017524 | Time: 5.08s\n",
      "(Best Epoch 32 | best H: 0.006088| best Pcv: 0.514208| val_loss : 0.017524)\n",
      "âœ… Save best H @ epoch 33\n",
      "Epoch 34 | Learning Rate: 0.001421\n",
      "---\n",
      "alpha: 0.023\n",
      "Epoch 34, loss_H: 0.005887, loss_Pcv: 0.540530\n",
      "Epoch 34 | Train Loss: 0.016376 | Val Loss: 0.018006 | Time: 4.97s\n",
      "(Best Epoch 33 | best H: 0.006068| best Pcv: 0.526818| val_loss : 0.018006)\n",
      "âœ… Save best H @ epoch 34\n",
      "Epoch 35 | Learning Rate: 0.001407\n",
      "---\n",
      "alpha: 0.023\n",
      "Epoch 35, loss_H: 0.005810, loss_Pcv: 0.520386\n",
      "Epoch 35 | Train Loss: 0.016444 | Val Loss: 0.017817 | Time: 5.26s\n",
      "(Best Epoch 34 | best H: 0.005887| best Pcv: 0.540530| val_loss : 0.017817)\n",
      "âœ… Save best H @ epoch 35\n",
      "Epoch 36 | Learning Rate: 0.001393\n",
      "---\n",
      "alpha: 0.024\n",
      "Epoch 36, loss_H: 0.005797, loss_Pcv: 0.620242\n",
      "Epoch 36 | Train Loss: 0.016791 | Val Loss: 0.020544 | Time: 5.15s\n",
      "(Best Epoch 35 | best H: 0.005810| best Pcv: 0.520386| val_loss : 0.020544)\n",
      "âœ… Save best H @ epoch 36\n",
      "Epoch 37 | Learning Rate: 0.001379\n",
      "---\n",
      "alpha: 0.025\n",
      "Epoch 37, loss_H: 0.005627, loss_Pcv: 0.518672\n",
      "Epoch 37 | Train Loss: 0.017344 | Val Loss: 0.018282 | Time: 5.19s\n",
      "(Best Epoch 36 | best H: 0.005797| best Pcv: 0.620242| val_loss : 0.018282)\n",
      "âœ… Save best H @ epoch 37\n",
      "Epoch 38 | Learning Rate: 0.001365\n",
      "---\n",
      "alpha: 0.025\n",
      "Epoch 38, loss_H: 0.005528, loss_Pcv: 0.536493\n",
      "Epoch 38 | Train Loss: 0.017040 | Val Loss: 0.018979 | Time: 5.29s\n",
      "(Best Epoch 37 | best H: 0.005627| best Pcv: 0.518672| val_loss : 0.018979)\n",
      "âœ… Save best H @ epoch 38\n",
      "Epoch 39 | Learning Rate: 0.001351\n",
      "---\n",
      "alpha: 0.026\n",
      "Epoch 39, loss_H: 0.005474, loss_Pcv: 0.640520\n",
      "Epoch 39 | Train Loss: 0.017946 | Val Loss: 0.021985 | Time: 5.12s\n",
      "(Best Epoch 38 | best H: 0.005528| best Pcv: 0.536493| val_loss : 0.021985)\n",
      "âœ… Save best H @ epoch 39\n",
      "Epoch 40 | Learning Rate: 0.001338\n",
      "---\n",
      "alpha: 0.027\n",
      "Epoch 40, loss_H: 0.005403, loss_Pcv: 0.548510\n",
      "Epoch 40 | Train Loss: 0.018514 | Val Loss: 0.019886 | Time: 5.11s\n",
      "(Best Epoch 39 | best H: 0.005474| best Pcv: 0.640520| val_loss : 0.019886)\n",
      "âœ… Save best H @ epoch 40\n",
      "Epoch 41 | Learning Rate: 0.001325\n",
      "---\n",
      "alpha: 0.027\n",
      "Epoch 41, loss_H: 0.005338, loss_Pcv: 0.492604\n",
      "Epoch 41 | Train Loss: 0.017694 | Val Loss: 0.018657 | Time: 5.44s\n",
      "(Best Epoch 40 | best H: 0.005403| best Pcv: 0.548510| val_loss : 0.018657)\n",
      "âœ… Save best H @ epoch 41\n",
      "Epoch 42 | Learning Rate: 0.001311\n",
      "---\n",
      "alpha: 0.028\n",
      "Epoch 42, loss_H: 0.005163, loss_Pcv: 0.511805\n",
      "Epoch 42 | Train Loss: 0.017690 | Val Loss: 0.019349 | Time: 5.28s\n",
      "(Best Epoch 41 | best H: 0.005338| best Pcv: 0.492604| val_loss : 0.019349)\n",
      "âœ… Save best H @ epoch 42\n",
      "Epoch 43 | Learning Rate: 0.001298\n",
      "---\n",
      "alpha: 0.029\n",
      "Epoch 43, loss_H: 0.005060, loss_Pcv: 0.524434\n",
      "Epoch 43 | Train Loss: 0.018401 | Val Loss: 0.019949 | Time: 5.34s\n",
      "(Best Epoch 42 | best H: 0.005163| best Pcv: 0.511805| val_loss : 0.019949)\n",
      "âœ… Save best H @ epoch 43\n",
      "Epoch 44 | Learning Rate: 0.001285\n",
      "---\n",
      "alpha: 0.029\n",
      "Epoch 44, loss_H: 0.005019, loss_Pcv: 0.480214\n",
      "Epoch 44 | Train Loss: 0.018084 | Val Loss: 0.018958 | Time: 5.01s\n",
      "(Best Epoch 43 | best H: 0.005060| best Pcv: 0.524434| val_loss : 0.018958)\n",
      "âœ… Save best H @ epoch 44\n",
      "Epoch 45 | Learning Rate: 0.001272\n",
      "---\n",
      "alpha: 0.030\n",
      "Epoch 45, loss_H: 0.005084, loss_Pcv: 0.413591\n",
      "Epoch 45 | Train Loss: 0.017822 | Val Loss: 0.017339 | Time: 4.94s\n",
      "(Best Epoch 44 | best H: 0.005019| best Pcv: 0.480214| val_loss : 0.017339)\n",
      "  H ç„¡æ”¹å–„ wait_H=1/150\n",
      "Epoch 46 | Learning Rate: 0.001260\n",
      "---\n",
      "alpha: 0.031\n",
      "Epoch 46, loss_H: 0.005999, loss_Pcv: 0.313932\n",
      "Epoch 46 | Train Loss: 0.015101 | Val Loss: 0.015443 | Time: 5.13s\n",
      "(Best Epoch 44 | best H: 0.005019| best Pcv: 0.480214| val_loss : 0.015443)\n",
      "  H ç„¡æ”¹å–„ wait_H=2/150\n",
      "Epoch 47 | Learning Rate: 0.001247\n",
      "---\n",
      "alpha: 0.031\n",
      "Epoch 47, loss_H: 0.004700, loss_Pcv: 0.514427\n",
      "Epoch 47 | Train Loss: 0.016639 | Val Loss: 0.020672 | Time: 5.16s\n",
      "(Best Epoch 44 | best H: 0.005019| best Pcv: 0.480214| val_loss : 0.020672)\n",
      "âœ… Save best H @ epoch 47\n",
      "Epoch 48 | Learning Rate: 0.001235\n",
      "---\n",
      "alpha: 0.032\n",
      "Epoch 48, loss_H: 0.005355, loss_Pcv: 0.826245\n",
      "Epoch 48 | Train Loss: 0.024771 | Val Loss: 0.031624 | Time: 5.13s\n",
      "(Best Epoch 47 | best H: 0.004700| best Pcv: 0.514427| val_loss : 0.031624)\n",
      "  H ç„¡æ”¹å–„ wait_H=1/150\n",
      "Epoch 49 | Learning Rate: 0.001222\n",
      "---\n",
      "alpha: 0.033\n",
      "Epoch 49, loss_H: 0.004725, loss_Pcv: 0.567184\n",
      "Epoch 49 | Train Loss: 0.027096 | Val Loss: 0.023098 | Time: 5.10s\n",
      "(Best Epoch 47 | best H: 0.004700| best Pcv: 0.514427| val_loss : 0.023098)\n",
      "  H ç„¡æ”¹å–„ wait_H=2/150\n",
      "Epoch 50 | Learning Rate: 0.001210\n",
      "---\n",
      "alpha: 0.033\n",
      "Epoch 50, loss_H: 0.004748, loss_Pcv: 0.553175\n",
      "Epoch 50 | Train Loss: 0.020547 | Val Loss: 0.023029 | Time: 5.09s\n",
      "(Best Epoch 47 | best H: 0.004700| best Pcv: 0.514427| val_loss : 0.023029)\n",
      "  H ç„¡æ”¹å–„ wait_H=3/150\n",
      "Epoch 51 | Learning Rate: 0.001198\n",
      "---\n",
      "alpha: 0.034\n",
      "Epoch 51, loss_H: 0.004727, loss_Pcv: 0.436329\n",
      "Epoch 51 | Train Loss: 0.018292 | Val Loss: 0.019402 | Time: 5.02s\n",
      "(Best Epoch 47 | best H: 0.004700| best Pcv: 0.514427| val_loss : 0.019402)\n",
      "  H ç„¡æ”¹å–„ wait_H=4/150\n",
      "Epoch 52 | Learning Rate: 0.001186\n",
      "---\n",
      "alpha: 0.035\n",
      "Epoch 52, loss_H: 0.004382, loss_Pcv: 0.485245\n",
      "Epoch 52 | Train Loss: 0.017364 | Val Loss: 0.021052 | Time: 5.37s\n",
      "(Best Epoch 47 | best H: 0.004700| best Pcv: 0.514427| val_loss : 0.021052)\n",
      "âœ… Save best H @ epoch 52\n",
      "Epoch 53 | Learning Rate: 0.001174\n",
      "---\n",
      "alpha: 0.035\n",
      "Epoch 53, loss_H: 0.004252, loss_Pcv: 0.511454\n",
      "Epoch 53 | Train Loss: 0.019035 | Val Loss: 0.022173 | Time: 5.23s\n",
      "(Best Epoch 52 | best H: 0.004382| best Pcv: 0.485245| val_loss : 0.022173)\n",
      "âœ… Save best H @ epoch 53\n",
      "Epoch 54 | Learning Rate: 0.001162\n",
      "---\n",
      "alpha: 0.036\n",
      "Epoch 54, loss_H: 0.004063, loss_Pcv: 0.488842\n",
      "Epoch 54 | Train Loss: 0.019350 | Val Loss: 0.021515 | Time: 5.31s\n",
      "(Best Epoch 53 | best H: 0.004252| best Pcv: 0.511454| val_loss : 0.021515)\n",
      "âœ… Save best H @ epoch 54\n",
      "Epoch 55 | Learning Rate: 0.001151\n",
      "---\n",
      "alpha: 0.037\n",
      "Epoch 55, loss_H: 0.003976, loss_Pcv: 0.484689\n",
      "Epoch 55 | Train Loss: 0.020077 | Val Loss: 0.021602 | Time: 5.24s\n",
      "(Best Epoch 54 | best H: 0.004063| best Pcv: 0.488842| val_loss : 0.021602)\n",
      "âœ… Save best H @ epoch 55\n",
      "Epoch 56 | Learning Rate: 0.001139\n",
      "---\n",
      "alpha: 0.037\n",
      "Epoch 56, loss_H: 0.003883, loss_Pcv: 0.495171\n",
      "Epoch 56 | Train Loss: 0.019845 | Val Loss: 0.022225 | Time: 4.93s\n",
      "(Best Epoch 55 | best H: 0.003976| best Pcv: 0.484689| val_loss : 0.022225)\n",
      "âœ… Save best H @ epoch 56\n",
      "Epoch 57 | Learning Rate: 0.001128\n",
      "---\n",
      "alpha: 0.038\n",
      "Epoch 57, loss_H: 0.003780, loss_Pcv: 0.487571\n",
      "Epoch 57 | Train Loss: 0.020231 | Val Loss: 0.022165 | Time: 5.14s\n",
      "(Best Epoch 56 | best H: 0.003883| best Pcv: 0.495171| val_loss : 0.022165)\n",
      "âœ… Save best H @ epoch 57\n",
      "Epoch 58 | Learning Rate: 0.001117\n",
      "---\n",
      "alpha: 0.039\n",
      "Epoch 58, loss_H: 0.003705, loss_Pcv: 0.490443\n",
      "Epoch 58 | Train Loss: 0.020612 | Val Loss: 0.022526 | Time: 5.28s\n",
      "(Best Epoch 57 | best H: 0.003780| best Pcv: 0.487571| val_loss : 0.022526)\n",
      "âœ… Save best H @ epoch 58\n",
      "Epoch 59 | Learning Rate: 0.001105\n",
      "---\n",
      "alpha: 0.039\n",
      "Epoch 59, loss_H: 0.003610, loss_Pcv: 0.496668\n",
      "Epoch 59 | Train Loss: 0.021180 | Val Loss: 0.023004 | Time: 4.92s\n",
      "(Best Epoch 58 | best H: 0.003705| best Pcv: 0.490443| val_loss : 0.023004)\n",
      "âœ… Save best H @ epoch 59\n",
      "Epoch 60 | Learning Rate: 0.001094\n",
      "---\n",
      "alpha: 0.040\n",
      "Epoch 60, loss_H: 0.003512, loss_Pcv: 0.490873\n",
      "Epoch 60 | Train Loss: 0.021793 | Val Loss: 0.023006 | Time: 5.03s\n",
      "(Best Epoch 59 | best H: 0.003610| best Pcv: 0.496668| val_loss : 0.023006)\n",
      "âœ… Save best H @ epoch 60\n",
      "Epoch 61 | Learning Rate: 0.001083\n",
      "---\n",
      "alpha: 0.041\n",
      "Epoch 61, loss_H: 0.003443, loss_Pcv: 0.586489\n",
      "Epoch 61 | Train Loss: 0.020812 | Val Loss: 0.027154 | Time: 4.94s\n",
      "(Best Epoch 60 | best H: 0.003512| best Pcv: 0.490873| val_loss : 0.027154)\n",
      "âœ… Save best H @ epoch 61\n",
      "Epoch 62 | Learning Rate: 0.001073\n",
      "---\n",
      "alpha: 0.041\n",
      "Epoch 62, loss_H: 0.003337, loss_Pcv: 0.479217\n",
      "Epoch 62 | Train Loss: 0.022252 | Val Loss: 0.023007 | Time: 4.96s\n",
      "(Best Epoch 61 | best H: 0.003443| best Pcv: 0.586489| val_loss : 0.023007)\n",
      "âœ… Save best H @ epoch 62\n",
      "Epoch 63 | Learning Rate: 0.001062\n",
      "---\n",
      "alpha: 0.042\n",
      "Epoch 63, loss_H: 0.003277, loss_Pcv: 0.472084\n",
      "Epoch 63 | Train Loss: 0.022309 | Val Loss: 0.022966 | Time: 5.23s\n",
      "(Best Epoch 62 | best H: 0.003337| best Pcv: 0.479217| val_loss : 0.022966)\n",
      "âœ… Save best H @ epoch 63\n",
      "Epoch 64 | Learning Rate: 0.001051\n",
      "---\n",
      "alpha: 0.043\n",
      "Epoch 64, loss_H: 0.003222, loss_Pcv: 0.543969\n",
      "Epoch 64 | Train Loss: 0.021362 | Val Loss: 0.026294 | Time: 5.22s\n",
      "(Best Epoch 63 | best H: 0.003277| best Pcv: 0.472084| val_loss : 0.026294)\n",
      "âœ… Save best H @ epoch 64\n",
      "Epoch 65 | Learning Rate: 0.001041\n",
      "---\n",
      "alpha: 0.043\n",
      "Epoch 65, loss_H: 0.003053, loss_Pcv: 0.528656\n",
      "Epoch 65 | Train Loss: 0.022792 | Val Loss: 0.025829 | Time: 5.22s\n",
      "(Best Epoch 64 | best H: 0.003222| best Pcv: 0.543969| val_loss : 0.025829)\n",
      "âœ… Save best H @ epoch 65\n",
      "Epoch 66 | Learning Rate: 0.001030\n",
      "---\n",
      "alpha: 0.044\n",
      "Epoch 66, loss_H: 0.002984, loss_Pcv: 0.482576\n",
      "Epoch 66 | Train Loss: 0.024959 | Val Loss: 0.024086 | Time: 5.05s\n",
      "(Best Epoch 65 | best H: 0.003053| best Pcv: 0.528656| val_loss : 0.024086)\n",
      "âœ… Save best H @ epoch 66\n",
      "Epoch 67 | Learning Rate: 0.001020\n",
      "---\n",
      "alpha: 0.045\n",
      "Epoch 67, loss_H: 0.002982, loss_Pcv: 0.481544\n",
      "Epoch 67 | Train Loss: 0.022693 | Val Loss: 0.024358 | Time: 4.98s\n",
      "(Best Epoch 66 | best H: 0.002984| best Pcv: 0.482576| val_loss : 0.024358)\n",
      "âœ… Save best H @ epoch 67\n",
      "Epoch 68 | Learning Rate: 0.001010\n",
      "---\n",
      "alpha: 0.045\n",
      "Epoch 68, loss_H: 0.002860, loss_Pcv: 0.499524\n",
      "Epoch 68 | Train Loss: 0.022190 | Val Loss: 0.025376 | Time: 5.01s\n",
      "(Best Epoch 67 | best H: 0.002982| best Pcv: 0.481544| val_loss : 0.025376)\n",
      "âœ… Save best H @ epoch 68\n",
      "Epoch 69 | Learning Rate: 0.001000\n",
      "---\n",
      "alpha: 0.046\n",
      "Epoch 69, loss_H: 0.002731, loss_Pcv: 0.511610\n",
      "Epoch 69 | Train Loss: 0.022802 | Val Loss: 0.026139 | Time: 5.18s\n",
      "(Best Epoch 68 | best H: 0.002860| best Pcv: 0.499524| val_loss : 0.026139)\n",
      "âœ… Save best H @ epoch 69\n",
      "Epoch 70 | Learning Rate: 0.000990\n",
      "---\n",
      "alpha: 0.047\n",
      "Epoch 70, loss_H: 0.002631, loss_Pcv: 0.480338\n",
      "Epoch 70 | Train Loss: 0.022738 | Val Loss: 0.024924 | Time: 5.15s\n",
      "(Best Epoch 69 | best H: 0.002731| best Pcv: 0.511610| val_loss : 0.024924)\n",
      "âœ… Save best H @ epoch 70\n",
      "Epoch 71 | Learning Rate: 0.000980\n",
      "---\n",
      "alpha: 0.047\n",
      "Epoch 71, loss_H: 0.002661, loss_Pcv: 0.443325\n",
      "Epoch 71 | Train Loss: 0.022267 | Val Loss: 0.023519 | Time: 5.22s\n",
      "(Best Epoch 70 | best H: 0.002631| best Pcv: 0.480338| val_loss : 0.023519)\n",
      "  H ç„¡æ”¹å–„ wait_H=1/150\n",
      "Epoch 72 | Learning Rate: 0.000970\n",
      "---\n",
      "alpha: 0.048\n",
      "Epoch 72, loss_H: 0.002519, loss_Pcv: 0.461874\n",
      "Epoch 72 | Train Loss: 0.022523 | Val Loss: 0.024568 | Time: 5.11s\n",
      "(Best Epoch 70 | best H: 0.002631| best Pcv: 0.480338| val_loss : 0.024568)\n",
      "âœ… Save best H @ epoch 72\n",
      "Epoch 73 | Learning Rate: 0.000960\n",
      "---\n",
      "alpha: 0.049\n",
      "Epoch 73, loss_H: 0.002690, loss_Pcv: 0.784984\n",
      "Epoch 73 | Train Loss: 0.021594 | Val Loss: 0.040762 | Time: 4.92s\n",
      "(Best Epoch 72 | best H: 0.002519| best Pcv: 0.461874| val_loss : 0.040762)\n",
      "  H ç„¡æ”¹å–„ wait_H=1/150\n",
      "Epoch 74 | Learning Rate: 0.000951\n",
      "---\n",
      "alpha: 0.049\n",
      "Epoch 74, loss_H: 0.010818, loss_Pcv: 1.093201\n",
      "Epoch 74 | Train Loss: 0.049771 | Val Loss: 0.064215 | Time: 5.27s\n",
      "(Best Epoch 72 | best H: 0.002519| best Pcv: 0.461874| val_loss : 0.064215)\n",
      "  H ç„¡æ”¹å–„ wait_H=2/150\n",
      "Epoch 75 | Learning Rate: 0.000941\n",
      "---\n",
      "alpha: 0.050\n",
      "Epoch 75, loss_H: 0.009202, loss_Pcv: 0.965878\n",
      "Epoch 75 | Train Loss: 0.056775 | Val Loss: 0.057036 | Time: 5.41s\n",
      "(Best Epoch 72 | best H: 0.002519| best Pcv: 0.461874| val_loss : 0.057036)\n",
      "  H ç„¡æ”¹å–„ wait_H=3/150\n",
      "Epoch 76 | Learning Rate: 0.000932\n",
      "---\n",
      "alpha: 0.051\n",
      "Epoch 76, loss_H: 0.005490, loss_Pcv: 0.961351\n",
      "Epoch 76 | Train Loss: 0.053764 | Val Loss: 0.053920 | Time: 5.20s\n",
      "(Best Epoch 72 | best H: 0.002519| best Pcv: 0.461874| val_loss : 0.053920)\n",
      "  H ç„¡æ”¹å–„ wait_H=4/150\n",
      "Epoch 77 | Learning Rate: 0.000922\n",
      "---\n",
      "alpha: 0.051\n",
      "Epoch 77, loss_H: 0.004481, loss_Pcv: 0.559758\n",
      "Epoch 77 | Train Loss: 0.041711 | Val Loss: 0.032985 | Time: 5.23s\n",
      "(Best Epoch 72 | best H: 0.002519| best Pcv: 0.461874| val_loss : 0.032985)\n",
      "  H ç„¡æ”¹å–„ wait_H=5/150\n",
      "Epoch 78 | Learning Rate: 0.000913\n",
      "---\n",
      "alpha: 0.052\n",
      "Epoch 78, loss_H: 0.003452, loss_Pcv: 0.415128\n",
      "Epoch 78 | Train Loss: 0.025334 | Val Loss: 0.024859 | Time: 4.96s\n",
      "(Best Epoch 72 | best H: 0.002519| best Pcv: 0.461874| val_loss : 0.024859)\n",
      "  H ç„¡æ”¹å–„ wait_H=6/150\n",
      "Epoch 79 | Learning Rate: 0.000904\n",
      "---\n",
      "alpha: 0.053\n",
      "Epoch 79, loss_H: 0.002713, loss_Pcv: 0.358163\n",
      "Epoch 79 | Train Loss: 0.021630 | Val Loss: 0.021433 | Time: 5.08s\n",
      "(Best Epoch 72 | best H: 0.002519| best Pcv: 0.461874| val_loss : 0.021433)\n",
      "  H ç„¡æ”¹å–„ wait_H=7/150\n",
      "Epoch 80 | Learning Rate: 0.000895\n",
      "---\n",
      "alpha: 0.053\n",
      "Epoch 80, loss_H: 0.002381, loss_Pcv: 0.425529\n",
      "Epoch 80 | Train Loss: 0.021479 | Val Loss: 0.024949 | Time: 5.35s\n",
      "(Best Epoch 72 | best H: 0.002519| best Pcv: 0.461874| val_loss : 0.024949)\n",
      "âœ… Save best H @ epoch 80\n",
      "Epoch 81 | Learning Rate: 0.000886\n",
      "---\n",
      "alpha: 0.054\n",
      "Epoch 81, loss_H: 0.002785, loss_Pcv: 0.244617\n",
      "Epoch 81 | Train Loss: 0.020313 | Val Loss: 0.015844 | Time: 5.46s\n",
      "(Best Epoch 80 | best H: 0.002381| best Pcv: 0.425529| val_loss : 0.015844)\n",
      "  H ç„¡æ”¹å–„ wait_H=1/150\n",
      "Epoch 82 | Learning Rate: 0.000877\n",
      "---\n",
      "alpha: 0.055\n",
      "Epoch 82, loss_H: 0.002546, loss_Pcv: 0.347256\n",
      "Epoch 82 | Train Loss: 0.018200 | Val Loss: 0.021390 | Time: 5.22s\n",
      "(Best Epoch 80 | best H: 0.002381| best Pcv: 0.425529| val_loss : 0.021390)\n",
      "  H ç„¡æ”¹å–„ wait_H=2/150\n",
      "Epoch 83 | Learning Rate: 0.000868\n",
      "---\n",
      "alpha: 0.055\n",
      "Epoch 83, loss_H: 0.004457, loss_Pcv: 0.581483\n",
      "Epoch 83 | Train Loss: 0.017373 | Val Loss: 0.036386 | Time: 5.02s\n",
      "(Best Epoch 80 | best H: 0.002381| best Pcv: 0.425529| val_loss : 0.036386)\n",
      "  H ç„¡æ”¹å–„ wait_H=3/150\n",
      "Epoch 84 | Learning Rate: 0.000860\n",
      "---\n",
      "alpha: 0.056\n",
      "Epoch 84, loss_H: 0.012332, loss_Pcv: 0.407236\n",
      "Epoch 84 | Train Loss: 0.036253 | Val Loss: 0.034447 | Time: 4.96s\n",
      "(Best Epoch 80 | best H: 0.002381| best Pcv: 0.425529| val_loss : 0.034447)\n",
      "  H ç„¡æ”¹å–„ wait_H=4/150\n",
      "Epoch 85 | Learning Rate: 0.000851\n",
      "---\n",
      "alpha: 0.057\n",
      "Epoch 85, loss_H: 0.013312, loss_Pcv: 0.185397\n",
      "Epoch 85 | Train Loss: 0.027320 | Val Loss: 0.023063 | Time: 5.27s\n",
      "(Best Epoch 80 | best H: 0.002381| best Pcv: 0.425529| val_loss : 0.023063)\n",
      "  H ç„¡æ”¹å–„ wait_H=5/150\n",
      "Epoch 86 | Learning Rate: 0.000843\n",
      "---\n",
      "alpha: 0.057\n",
      "Epoch 86, loss_H: 0.010241, loss_Pcv: 0.086998\n",
      "Epoch 86 | Train Loss: 0.018987 | Val Loss: 0.014642 | Time: 5.27s\n",
      "(Best Epoch 80 | best H: 0.002381| best Pcv: 0.425529| val_loss : 0.014642)\n",
      "  H ç„¡æ”¹å–„ wait_H=6/150\n",
      "Epoch 87 | Learning Rate: 0.000834\n",
      "---\n",
      "alpha: 0.058\n",
      "Epoch 87, loss_H: 0.007666, loss_Pcv: 0.034876\n",
      "Epoch 87 | Train Loss: 0.012710 | Val Loss: 0.009244 | Time: 5.31s\n",
      "(Best Epoch 80 | best H: 0.002381| best Pcv: 0.425529| val_loss : 0.009244)\n",
      "  H ç„¡æ”¹å–„ wait_H=7/150\n",
      "Epoch 88 | Learning Rate: 0.000826\n",
      "---\n",
      "alpha: 0.059\n",
      "Epoch 88, loss_H: 0.005814, loss_Pcv: 0.050946\n",
      "Epoch 88 | Train Loss: 0.008863 | Val Loss: 0.008462 | Time: 5.01s\n",
      "(Best Epoch 80 | best H: 0.002381| best Pcv: 0.425529| val_loss : 0.008462)\n",
      "  H ç„¡æ”¹å–„ wait_H=8/150\n",
      "Epoch 89 | Learning Rate: 0.000818\n",
      "---\n",
      "alpha: 0.059\n",
      "Epoch 89, loss_H: 0.005768, loss_Pcv: 0.031185\n",
      "Epoch 89 | Train Loss: 0.008644 | Val Loss: 0.007276 | Time: 4.95s\n",
      "(Best Epoch 80 | best H: 0.002381| best Pcv: 0.425529| val_loss : 0.007276)\n",
      "  H ç„¡æ”¹å–„ wait_H=9/150\n",
      "Epoch 90 | Learning Rate: 0.000809\n",
      "---\n",
      "alpha: 0.060\n",
      "Epoch 90, loss_H: 0.005569, loss_Pcv: 0.029578\n",
      "Epoch 90 | Train Loss: 0.007420 | Val Loss: 0.007009 | Time: 5.11s\n",
      "(Best Epoch 80 | best H: 0.002381| best Pcv: 0.425529| val_loss : 0.007009)\n",
      "  H ç„¡æ”¹å–„ wait_H=10/150\n",
      "Epoch 91 | Learning Rate: 0.000801\n",
      "---\n",
      "alpha: 0.061\n",
      "Epoch 91, loss_H: 0.005273, loss_Pcv: 0.026333\n",
      "Epoch 91 | Train Loss: 0.007072 | Val Loss: 0.006551 | Time: 5.24s\n",
      "(Best Epoch 80 | best H: 0.002381| best Pcv: 0.425529| val_loss : 0.006551)\n",
      "  H ç„¡æ”¹å–„ wait_H=11/150\n",
      "Epoch 92 | Learning Rate: 0.000793\n",
      "---\n",
      "alpha: 0.061\n",
      "Epoch 92, loss_H: 0.004866, loss_Pcv: 0.027044\n",
      "Epoch 92 | Train Loss: 0.006838 | Val Loss: 0.006226 | Time: 5.27s\n",
      "(Best Epoch 80 | best H: 0.002381| best Pcv: 0.425529| val_loss : 0.006226)\n",
      "  H ç„¡æ”¹å–„ wait_H=12/150\n",
      "Epoch 93 | Learning Rate: 0.000785\n",
      "---\n",
      "alpha: 0.062\n",
      "Epoch 93, loss_H: 0.004627, loss_Pcv: 0.026151\n",
      "Epoch 93 | Train Loss: 0.006518 | Val Loss: 0.005962 | Time: 4.97s\n",
      "(Best Epoch 80 | best H: 0.002381| best Pcv: 0.425529| val_loss : 0.005962)\n",
      "  H ç„¡æ”¹å–„ wait_H=13/150\n",
      "Epoch 94 | Learning Rate: 0.000778\n",
      "---\n",
      "alpha: 0.063\n",
      "Epoch 94, loss_H: 0.004479, loss_Pcv: 0.024037\n",
      "Epoch 94 | Train Loss: 0.006110 | Val Loss: 0.005704 | Time: 5.27s\n",
      "(Best Epoch 80 | best H: 0.002381| best Pcv: 0.425529| val_loss : 0.005704)\n",
      "  H ç„¡æ”¹å–„ wait_H=14/150\n",
      "Epoch 95 | Learning Rate: 0.000770\n",
      "---\n",
      "alpha: 0.063\n",
      "Epoch 95, loss_H: 0.004253, loss_Pcv: 0.023937\n",
      "Epoch 95 | Train Loss: 0.005783 | Val Loss: 0.005500 | Time: 5.02s\n",
      "(Best Epoch 80 | best H: 0.002381| best Pcv: 0.425529| val_loss : 0.005500)\n",
      "  H ç„¡æ”¹å–„ wait_H=15/150\n",
      "Epoch 96 | Learning Rate: 0.000762\n",
      "---\n",
      "alpha: 0.064\n",
      "Epoch 96, loss_H: 0.004051, loss_Pcv: 0.023632\n",
      "Epoch 96 | Train Loss: 0.005719 | Val Loss: 0.005304 | Time: 5.37s\n",
      "(Best Epoch 80 | best H: 0.002381| best Pcv: 0.425529| val_loss : 0.005304)\n",
      "  H ç„¡æ”¹å–„ wait_H=16/150\n",
      "Epoch 97 | Learning Rate: 0.000754\n",
      "---\n",
      "alpha: 0.065\n",
      "Epoch 97, loss_H: 0.003862, loss_Pcv: 0.023809\n",
      "Epoch 97 | Train Loss: 0.005471 | Val Loss: 0.005152 | Time: 5.38s\n",
      "(Best Epoch 80 | best H: 0.002381| best Pcv: 0.425529| val_loss : 0.005152)\n",
      "  H ç„¡æ”¹å–„ wait_H=17/150\n",
      "Epoch 98 | Learning Rate: 0.000747\n",
      "---\n",
      "alpha: 0.065\n",
      "Epoch 98, loss_H: 0.003711, loss_Pcv: 0.023260\n",
      "Epoch 98 | Train Loss: 0.005288 | Val Loss: 0.004988 | Time: 5.17s\n",
      "(Best Epoch 80 | best H: 0.002381| best Pcv: 0.425529| val_loss : 0.004988)\n",
      "  H ç„¡æ”¹å–„ wait_H=18/150\n",
      "Epoch 99 | Learning Rate: 0.000739\n",
      "---\n",
      "alpha: 0.066\n",
      "Epoch 99, loss_H: 0.003585, loss_Pcv: 0.022725\n",
      "Epoch 99 | Train Loss: 0.005251 | Val Loss: 0.004848 | Time: 5.16s\n",
      "(Best Epoch 80 | best H: 0.002381| best Pcv: 0.425529| val_loss : 0.004848)\n",
      "  H ç„¡æ”¹å–„ wait_H=19/150\n",
      "Epoch 100 | Learning Rate: 0.000732\n",
      "---\n",
      "alpha: 0.067\n",
      "Epoch 100, loss_H: 0.003449, loss_Pcv: 0.022607\n",
      "Epoch 100 | Train Loss: 0.004882 | Val Loss: 0.004726 | Time: 5.11s\n",
      "(Best Epoch 80 | best H: 0.002381| best Pcv: 0.425529| val_loss : 0.004726)\n",
      "  H ç„¡æ”¹å–„ wait_H=20/150\n",
      "Epoch 101 | Learning Rate: 0.000725\n",
      "---\n",
      "alpha: 0.067\n",
      "Epoch 101, loss_H: 0.003309, loss_Pcv: 0.022680\n",
      "Epoch 101 | Train Loss: 0.004764 | Val Loss: 0.004613 | Time: 5.07s\n",
      "(Best Epoch 80 | best H: 0.002381| best Pcv: 0.425529| val_loss : 0.004613)\n",
      "  H ç„¡æ”¹å–„ wait_H=21/150\n",
      "Epoch 102 | Learning Rate: 0.000717\n",
      "---\n",
      "alpha: 0.068\n",
      "Epoch 102, loss_H: 0.003204, loss_Pcv: 0.022405\n",
      "Epoch 102 | Train Loss: 0.004637 | Val Loss: 0.004509 | Time: 5.20s\n",
      "(Best Epoch 80 | best H: 0.002381| best Pcv: 0.425529| val_loss : 0.004509)\n",
      "  H ç„¡æ”¹å–„ wait_H=22/150\n",
      "Epoch 103 | Learning Rate: 0.000710\n",
      "---\n",
      "alpha: 0.069\n",
      "Epoch 103, loss_H: 0.003086, loss_Pcv: 0.022361\n",
      "Epoch 103 | Train Loss: 0.004766 | Val Loss: 0.004410 | Time: 4.97s\n",
      "(Best Epoch 80 | best H: 0.002381| best Pcv: 0.425529| val_loss : 0.004410)\n",
      "  H ç„¡æ”¹å–„ wait_H=23/150\n",
      "Epoch 104 | Learning Rate: 0.000703\n",
      "---\n",
      "alpha: 0.069\n",
      "Epoch 104, loss_H: 0.003006, loss_Pcv: 0.022054\n",
      "Epoch 104 | Train Loss: 0.004439 | Val Loss: 0.004327 | Time: 4.93s\n",
      "(Best Epoch 80 | best H: 0.002381| best Pcv: 0.425529| val_loss : 0.004327)\n",
      "  H ç„¡æ”¹å–„ wait_H=24/150\n",
      "Epoch 105 | Learning Rate: 0.000696\n",
      "---\n",
      "alpha: 0.070\n",
      "Epoch 105, loss_H: 0.002905, loss_Pcv: 0.021959\n",
      "Epoch 105 | Train Loss: 0.004388 | Val Loss: 0.004239 | Time: 4.89s\n",
      "(Best Epoch 80 | best H: 0.002381| best Pcv: 0.425529| val_loss : 0.004239)\n",
      "  H ç„¡æ”¹å–„ wait_H=25/150\n",
      "Epoch 106 | Learning Rate: 0.000689\n",
      "---\n",
      "alpha: 0.071\n",
      "Epoch 106, loss_H: 0.002820, loss_Pcv: 0.021851\n",
      "Epoch 106 | Train Loss: 0.004305 | Val Loss: 0.004165 | Time: 4.95s\n",
      "(Best Epoch 80 | best H: 0.002381| best Pcv: 0.425529| val_loss : 0.004165)\n",
      "  H ç„¡æ”¹å–„ wait_H=26/150\n",
      "Epoch 107 | Learning Rate: 0.000682\n",
      "---\n",
      "alpha: 0.071\n",
      "Epoch 107, loss_H: 0.002744, loss_Pcv: 0.021664\n",
      "Epoch 107 | Train Loss: 0.004241 | Val Loss: 0.004094 | Time: 5.32s\n",
      "(Best Epoch 80 | best H: 0.002381| best Pcv: 0.425529| val_loss : 0.004094)\n",
      "  H ç„¡æ”¹å–„ wait_H=27/150\n",
      "Epoch 108 | Learning Rate: 0.000676\n",
      "---\n",
      "alpha: 0.072\n",
      "Epoch 108, loss_H: 0.002686, loss_Pcv: 0.021268\n",
      "Epoch 108 | Train Loss: 0.004129 | Val Loss: 0.004023 | Time: 5.33s\n",
      "(Best Epoch 80 | best H: 0.002381| best Pcv: 0.425529| val_loss : 0.004023)\n",
      "  H ç„¡æ”¹å–„ wait_H=28/150\n",
      "Epoch 109 | Learning Rate: 0.000669\n",
      "---\n",
      "alpha: 0.073\n",
      "Epoch 109, loss_H: 0.002600, loss_Pcv: 0.021262\n",
      "Epoch 109 | Train Loss: 0.004168 | Val Loss: 0.003956 | Time: 5.06s\n",
      "(Best Epoch 80 | best H: 0.002381| best Pcv: 0.425529| val_loss : 0.003956)\n",
      "  H ç„¡æ”¹å–„ wait_H=29/150\n",
      "Epoch 110 | Learning Rate: 0.000662\n",
      "---\n",
      "alpha: 0.073\n",
      "Epoch 110, loss_H: 0.002539, loss_Pcv: 0.021191\n",
      "Epoch 110 | Train Loss: 0.004016 | Val Loss: 0.003907 | Time: 5.10s\n",
      "(Best Epoch 80 | best H: 0.002381| best Pcv: 0.425529| val_loss : 0.003907)\n",
      "  H ç„¡æ”¹å–„ wait_H=30/150\n",
      "Epoch 111 | Learning Rate: 0.000655\n",
      "---\n",
      "alpha: 0.074\n",
      "Epoch 111, loss_H: 0.002469, loss_Pcv: 0.021072\n",
      "Epoch 111 | Train Loss: 0.004076 | Val Loss: 0.003846 | Time: 5.19s\n",
      "(Best Epoch 80 | best H: 0.002381| best Pcv: 0.425529| val_loss : 0.003846)\n",
      "  H ç„¡æ”¹å–„ wait_H=31/150\n",
      "Epoch 112 | Learning Rate: 0.000649\n",
      "---\n",
      "alpha: 0.075\n",
      "Epoch 112, loss_H: 0.002413, loss_Pcv: 0.020916\n",
      "Epoch 112 | Train Loss: 0.003983 | Val Loss: 0.003795 | Time: 4.99s\n",
      "(Best Epoch 80 | best H: 0.002381| best Pcv: 0.425529| val_loss : 0.003795)\n",
      "  H ç„¡æ”¹å–„ wait_H=32/150\n",
      "Epoch 113 | Learning Rate: 0.000642\n",
      "---\n",
      "alpha: 0.075\n",
      "Epoch 113, loss_H: 0.002354, loss_Pcv: 0.020855\n",
      "Epoch 113 | Train Loss: 0.003855 | Val Loss: 0.003748 | Time: 5.34s\n",
      "(Best Epoch 80 | best H: 0.002381| best Pcv: 0.425529| val_loss : 0.003748)\n",
      "âœ… Save best H @ epoch 113\n",
      "Epoch 114 | Learning Rate: 0.000636\n",
      "---\n",
      "alpha: 0.076\n",
      "Epoch 114, loss_H: 0.002305, loss_Pcv: 0.020551\n",
      "Epoch 114 | Train Loss: 0.003912 | Val Loss: 0.003692 | Time: 5.46s\n",
      "(Best Epoch 113 | best H: 0.002354| best Pcv: 0.020855| val_loss : 0.003692)\n",
      "âœ… Save best H @ epoch 114\n",
      "Epoch 115 | Learning Rate: 0.000630\n",
      "---\n",
      "alpha: 0.077\n",
      "Epoch 115, loss_H: 0.002244, loss_Pcv: 0.020796\n",
      "Epoch 115 | Train Loss: 0.003854 | Val Loss: 0.003666 | Time: 5.27s\n",
      "(Best Epoch 114 | best H: 0.002305| best Pcv: 0.020551| val_loss : 0.003666)\n",
      "âœ… Save best H @ epoch 115\n",
      "Epoch 116 | Learning Rate: 0.000623\n",
      "---\n",
      "alpha: 0.077\n",
      "Epoch 116, loss_H: 0.002214, loss_Pcv: 0.020216\n",
      "Epoch 116 | Train Loss: 0.003798 | Val Loss: 0.003606 | Time: 5.23s\n",
      "(Best Epoch 115 | best H: 0.002244| best Pcv: 0.020796| val_loss : 0.003606)\n",
      "âœ… Save best H @ epoch 116\n",
      "Epoch 117 | Learning Rate: 0.000617\n",
      "---\n",
      "alpha: 0.078\n",
      "Epoch 117, loss_H: 0.002153, loss_Pcv: 0.020480\n",
      "Epoch 117 | Train Loss: 0.003697 | Val Loss: 0.003583 | Time: 5.16s\n",
      "(Best Epoch 116 | best H: 0.002214| best Pcv: 0.020216| val_loss : 0.003583)\n",
      "âœ… Save best H @ epoch 117\n",
      "Epoch 118 | Learning Rate: 0.000611\n",
      "---\n",
      "alpha: 0.079\n",
      "Epoch 118, loss_H: 0.002122, loss_Pcv: 0.020090\n",
      "Epoch 118 | Train Loss: 0.003743 | Val Loss: 0.003535 | Time: 5.35s\n",
      "(Best Epoch 117 | best H: 0.002153| best Pcv: 0.020480| val_loss : 0.003535)\n",
      "âœ… Save best H @ epoch 118\n",
      "Epoch 119 | Learning Rate: 0.000605\n",
      "---\n",
      "alpha: 0.079\n",
      "Epoch 119, loss_H: 0.002078, loss_Pcv: 0.020116\n",
      "Epoch 119 | Train Loss: 0.003582 | Val Loss: 0.003509 | Time: 5.23s\n",
      "(Best Epoch 118 | best H: 0.002122| best Pcv: 0.020090| val_loss : 0.003509)\n",
      "âœ… Save best H @ epoch 119\n",
      "Epoch 120 | Learning Rate: 0.000599\n",
      "---\n",
      "alpha: 0.080\n",
      "Epoch 120, loss_H: 0.002049, loss_Pcv: 0.019802\n",
      "Epoch 120 | Train Loss: 0.003548 | Val Loss: 0.003470 | Time: 5.07s\n",
      "(Best Epoch 119 | best H: 0.002078| best Pcv: 0.020116| val_loss : 0.003470)\n",
      "âœ… Save best H @ epoch 120\n",
      "Epoch 121 | Learning Rate: 0.000593\n",
      "---\n",
      "alpha: 0.081\n",
      "Epoch 121, loss_H: 0.002005, loss_Pcv: 0.019864\n",
      "Epoch 121 | Train Loss: 0.003553 | Val Loss: 0.003445 | Time: 5.09s\n",
      "(Best Epoch 120 | best H: 0.002049| best Pcv: 0.019802| val_loss : 0.003445)\n",
      "âœ… Save best H @ epoch 121\n",
      "Epoch 122 | Learning Rate: 0.000587\n",
      "---\n",
      "alpha: 0.081\n",
      "Epoch 122, loss_H: 0.001974, loss_Pcv: 0.019659\n",
      "Epoch 122 | Train Loss: 0.003500 | Val Loss: 0.003412 | Time: 5.12s\n",
      "(Best Epoch 121 | best H: 0.002005| best Pcv: 0.019864| val_loss : 0.003412)\n",
      "âœ… Save best H @ epoch 122\n",
      "Epoch 123 | Learning Rate: 0.000581\n",
      "---\n",
      "alpha: 0.082\n",
      "Epoch 123, loss_H: 0.001947, loss_Pcv: 0.019428\n",
      "Epoch 123 | Train Loss: 0.003599 | Val Loss: 0.003380 | Time: 5.06s\n",
      "(Best Epoch 122 | best H: 0.001974| best Pcv: 0.019659| val_loss : 0.003380)\n",
      "âœ… Save best H @ epoch 123\n",
      "Epoch 124 | Learning Rate: 0.000575\n",
      "---\n",
      "alpha: 0.083\n",
      "Epoch 124, loss_H: 0.001909, loss_Pcv: 0.019404\n",
      "Epoch 124 | Train Loss: 0.003572 | Val Loss: 0.003356 | Time: 5.20s\n",
      "(Best Epoch 123 | best H: 0.001947| best Pcv: 0.019428| val_loss : 0.003356)\n",
      "âœ… Save best H @ epoch 124\n",
      "Epoch 125 | Learning Rate: 0.000569\n",
      "---\n",
      "alpha: 0.083\n",
      "Epoch 125, loss_H: 0.001886, loss_Pcv: 0.019187\n",
      "Epoch 125 | Train Loss: 0.003437 | Val Loss: 0.003328 | Time: 5.08s\n",
      "(Best Epoch 124 | best H: 0.001909| best Pcv: 0.019404| val_loss : 0.003328)\n",
      "âœ… Save best H @ epoch 125\n",
      "Epoch 126 | Learning Rate: 0.000564\n",
      "---\n",
      "alpha: 0.084\n",
      "Epoch 126, loss_H: 0.001858, loss_Pcv: 0.019074\n",
      "Epoch 126 | Train Loss: 0.003412 | Val Loss: 0.003305 | Time: 4.90s\n",
      "(Best Epoch 125 | best H: 0.001886| best Pcv: 0.019187| val_loss : 0.003305)\n",
      "âœ… Save best H @ epoch 126\n",
      "Epoch 127 | Learning Rate: 0.000558\n",
      "---\n",
      "alpha: 0.085\n",
      "Epoch 127, loss_H: 0.001830, loss_Pcv: 0.018989\n",
      "Epoch 127 | Train Loss: 0.003386 | Val Loss: 0.003283 | Time: 5.01s\n",
      "(Best Epoch 126 | best H: 0.001858| best Pcv: 0.019074| val_loss : 0.003283)\n",
      "âœ… Save best H @ epoch 127\n",
      "Epoch 128 | Learning Rate: 0.000553\n",
      "---\n",
      "alpha: 0.085\n",
      "Epoch 128, loss_H: 0.001806, loss_Pcv: 0.018895\n",
      "Epoch 128 | Train Loss: 0.003374 | Val Loss: 0.003264 | Time: 5.09s\n",
      "(Best Epoch 127 | best H: 0.001830| best Pcv: 0.018989| val_loss : 0.003264)\n",
      "âœ… Save best H @ epoch 128\n",
      "Epoch 129 | Learning Rate: 0.000547\n",
      "---\n",
      "alpha: 0.086\n",
      "Epoch 129, loss_H: 0.001778, loss_Pcv: 0.018863\n",
      "Epoch 129 | Train Loss: 0.003461 | Val Loss: 0.003248 | Time: 5.19s\n",
      "(Best Epoch 128 | best H: 0.001806| best Pcv: 0.018895| val_loss : 0.003248)\n",
      "âœ… Save best H @ epoch 129\n",
      "Epoch 130 | Learning Rate: 0.000542\n",
      "---\n",
      "alpha: 0.087\n",
      "Epoch 130, loss_H: 0.001761, loss_Pcv: 0.018648\n",
      "Epoch 130 | Train Loss: 0.003325 | Val Loss: 0.003225 | Time: 5.44s\n",
      "(Best Epoch 129 | best H: 0.001778| best Pcv: 0.018863| val_loss : 0.003225)\n",
      "âœ… Save best H @ epoch 130\n",
      "Epoch 131 | Learning Rate: 0.000536\n",
      "---\n",
      "alpha: 0.087\n",
      "Epoch 131, loss_H: 0.001750, loss_Pcv: 0.018384\n",
      "Epoch 131 | Train Loss: 0.003316 | Val Loss: 0.003203 | Time: 5.18s\n",
      "(Best Epoch 130 | best H: 0.001761| best Pcv: 0.018648| val_loss : 0.003203)\n",
      "âœ… Save best H @ epoch 131\n",
      "Epoch 132 | Learning Rate: 0.000531\n",
      "---\n",
      "alpha: 0.088\n",
      "Epoch 132, loss_H: 0.001704, loss_Pcv: 0.018725\n",
      "Epoch 132 | Train Loss: 0.003276 | Val Loss: 0.003202 | Time: 5.04s\n",
      "(Best Epoch 131 | best H: 0.001750| best Pcv: 0.018384| val_loss : 0.003202)\n",
      "âœ… Save best H @ epoch 132\n",
      "Epoch 133 | Learning Rate: 0.000525\n",
      "---\n",
      "alpha: 0.089\n",
      "Epoch 133, loss_H: 0.001712, loss_Pcv: 0.018164\n",
      "Epoch 133 | Train Loss: 0.003403 | Val Loss: 0.003171 | Time: 5.21s\n",
      "(Best Epoch 132 | best H: 0.001704| best Pcv: 0.018725| val_loss : 0.003171)\n",
      "  H ç„¡æ”¹å–„ wait_H=1/150\n",
      "Epoch 134 | Learning Rate: 0.000520\n",
      "---\n",
      "alpha: 0.089\n",
      "Epoch 134, loss_H: 0.001682, loss_Pcv: 0.018225\n",
      "Epoch 134 | Train Loss: 0.003244 | Val Loss: 0.003160 | Time: 4.95s\n",
      "(Best Epoch 132 | best H: 0.001704| best Pcv: 0.018725| val_loss : 0.003160)\n",
      "âœ… Save best H @ epoch 134\n",
      "Epoch 135 | Learning Rate: 0.000515\n",
      "---\n",
      "alpha: 0.090\n",
      "Epoch 135, loss_H: 0.001654, loss_Pcv: 0.018335\n",
      "Epoch 135 | Train Loss: 0.003232 | Val Loss: 0.003156 | Time: 5.25s\n",
      "(Best Epoch 134 | best H: 0.001682| best Pcv: 0.018225| val_loss : 0.003156)\n",
      "âœ… Save best H @ epoch 135\n",
      "Epoch 136 | Learning Rate: 0.000510\n",
      "---\n",
      "alpha: 0.091\n",
      "Epoch 136, loss_H: 0.001652, loss_Pcv: 0.017950\n",
      "Epoch 136 | Train Loss: 0.003224 | Val Loss: 0.003129 | Time: 5.30s\n",
      "(Best Epoch 135 | best H: 0.001654| best Pcv: 0.018335| val_loss : 0.003129)\n",
      "âœ… Save best H @ epoch 136\n",
      "Epoch 137 | Learning Rate: 0.000505\n",
      "---\n",
      "alpha: 0.091\n",
      "Epoch 137, loss_H: 0.001646, loss_Pcv: 0.017717\n",
      "Epoch 137 | Train Loss: 0.003239 | Val Loss: 0.003113 | Time: 4.88s\n",
      "(Best Epoch 136 | best H: 0.001652| best Pcv: 0.017950| val_loss : 0.003113)\n",
      "âœ… Save best H @ epoch 137\n",
      "Epoch 138 | Learning Rate: 0.000500\n",
      "---\n",
      "alpha: 0.092\n",
      "Epoch 138, loss_H: 0.001602, loss_Pcv: 0.018089\n",
      "Epoch 138 | Train Loss: 0.003213 | Val Loss: 0.003119 | Time: 4.99s\n",
      "(Best Epoch 137 | best H: 0.001646| best Pcv: 0.017717| val_loss : 0.003119)\n",
      "âœ… Save best H @ epoch 138\n",
      "Epoch 139 | Learning Rate: 0.000495\n",
      "---\n",
      "alpha: 0.093\n",
      "Epoch 139, loss_H: 0.001595, loss_Pcv: 0.017808\n",
      "Epoch 139 | Train Loss: 0.003212 | Val Loss: 0.003097 | Time: 4.99s\n",
      "(Best Epoch 138 | best H: 0.001602| best Pcv: 0.018089| val_loss : 0.003097)\n",
      "âœ… Save best H @ epoch 139\n",
      "Epoch 140 | Learning Rate: 0.000490\n",
      "---\n",
      "alpha: 0.093\n",
      "Epoch 140, loss_H: 0.001584, loss_Pcv: 0.017653\n",
      "Epoch 140 | Train Loss: 0.003169 | Val Loss: 0.003083 | Time: 5.12s\n",
      "(Best Epoch 139 | best H: 0.001595| best Pcv: 0.017808| val_loss : 0.003083)\n",
      "âœ… Save best H @ epoch 140\n",
      "Epoch 141 | Learning Rate: 0.000485\n",
      "---\n",
      "alpha: 0.094\n",
      "Epoch 141, loss_H: 0.001578, loss_Pcv: 0.017429\n",
      "Epoch 141 | Train Loss: 0.003192 | Val Loss: 0.003068 | Time: 5.48s\n",
      "(Best Epoch 140 | best H: 0.001584| best Pcv: 0.017653| val_loss : 0.003068)\n",
      "âœ… Save best H @ epoch 141\n",
      "Epoch 142 | Learning Rate: 0.000480\n",
      "---\n",
      "alpha: 0.095\n",
      "Epoch 142, loss_H: 0.001562, loss_Pcv: 0.017353\n",
      "Epoch 142 | Train Loss: 0.003184 | Val Loss: 0.003057 | Time: 5.17s\n",
      "(Best Epoch 141 | best H: 0.001578| best Pcv: 0.017429| val_loss : 0.003057)\n",
      "âœ… Save best H @ epoch 142\n",
      "Epoch 143 | Learning Rate: 0.000475\n",
      "---\n",
      "alpha: 0.095\n",
      "Epoch 143, loss_H: 0.001549, loss_Pcv: 0.017292\n",
      "Epoch 143 | Train Loss: 0.003186 | Val Loss: 0.003050 | Time: 4.97s\n",
      "(Best Epoch 142 | best H: 0.001562| best Pcv: 0.017353| val_loss : 0.003050)\n",
      "âœ… Save best H @ epoch 143\n",
      "Epoch 144 | Learning Rate: 0.000470\n",
      "---\n",
      "alpha: 0.096\n",
      "Epoch 144, loss_H: 0.001531, loss_Pcv: 0.017314\n",
      "Epoch 144 | Train Loss: 0.003281 | Val Loss: 0.003046 | Time: 4.98s\n",
      "(Best Epoch 143 | best H: 0.001549| best Pcv: 0.017292| val_loss : 0.003046)\n",
      "âœ… Save best H @ epoch 144\n",
      "Epoch 145 | Learning Rate: 0.000466\n",
      "---\n",
      "alpha: 0.097\n",
      "Epoch 145, loss_H: 0.001535, loss_Pcv: 0.016972\n",
      "Epoch 145 | Train Loss: 0.003155 | Val Loss: 0.003027 | Time: 4.97s\n",
      "(Best Epoch 144 | best H: 0.001531| best Pcv: 0.017314| val_loss : 0.003027)\n",
      "  H ç„¡æ”¹å–„ wait_H=1/150\n",
      "Epoch 146 | Learning Rate: 0.000461\n",
      "---\n",
      "alpha: 0.097\n",
      "Epoch 146, loss_H: 0.001502, loss_Pcv: 0.017207\n",
      "Epoch 146 | Train Loss: 0.003144 | Val Loss: 0.003031 | Time: 5.12s\n",
      "(Best Epoch 144 | best H: 0.001531| best Pcv: 0.017314| val_loss : 0.003031)\n",
      "âœ… Save best H @ epoch 146\n",
      "Epoch 147 | Learning Rate: 0.000456\n",
      "---\n",
      "alpha: 0.098\n",
      "Epoch 147, loss_H: 0.001515, loss_Pcv: 0.016773\n",
      "Epoch 147 | Train Loss: 0.003266 | Val Loss: 0.003010 | Time: 5.22s\n",
      "(Best Epoch 146 | best H: 0.001502| best Pcv: 0.017207| val_loss : 0.003010)\n",
      "  H ç„¡æ”¹å–„ wait_H=1/150\n",
      "Epoch 148 | Learning Rate: 0.000452\n",
      "---\n",
      "alpha: 0.099\n",
      "Epoch 148, loss_H: 0.001492, loss_Pcv: 0.016808\n",
      "Epoch 148 | Train Loss: 0.003171 | Val Loss: 0.003003 | Time: 5.03s\n",
      "(Best Epoch 146 | best H: 0.001502| best Pcv: 0.017207| val_loss : 0.003003)\n",
      "âœ… Save best H @ epoch 148\n",
      "Epoch 149 | Learning Rate: 0.000447\n",
      "---\n",
      "alpha: 0.099\n",
      "Epoch 149, loss_H: 0.001473, loss_Pcv: 0.016878\n",
      "Epoch 149 | Train Loss: 0.003266 | Val Loss: 0.003003 | Time: 4.93s\n",
      "(Best Epoch 148 | best H: 0.001492| best Pcv: 0.016808| val_loss : 0.003003)\n",
      "âœ… Save best H @ epoch 149\n",
      "Epoch 150 | Learning Rate: 0.000443\n",
      "---\n",
      "alpha: 0.100\n",
      "Epoch 150, loss_H: 0.001490, loss_Pcv: 0.016492\n",
      "Epoch 150 | Train Loss: 0.003113 | Val Loss: 0.002990 | Time: 5.05s\n",
      "(Best Epoch 149 | best H: 0.001473| best Pcv: 0.016878| val_loss : 0.002990)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  H ç„¡æ”¹å–„ wait_H=1/150\n",
      "Epoch 151 | Learning Rate: 0.000438\n",
      "---\n",
      "alpha: 0.101\n",
      "Epoch 151, loss_H: 0.001443, loss_Pcv: 0.016852\n",
      "Epoch 151 | Train Loss: 0.003140 | Val Loss: 0.002994 | Time: 5.30s\n",
      "(Best Epoch 149 | best H: 0.001473| best Pcv: 0.016878| val_loss : 0.002994)\n",
      "âœ… Save best H @ epoch 151\n",
      "Epoch 152 | Learning Rate: 0.000434\n",
      "---\n",
      "alpha: 0.101\n",
      "Epoch 152, loss_H: 0.001467, loss_Pcv: 0.016309\n",
      "Epoch 152 | Train Loss: 0.003094 | Val Loss: 0.002971 | Time: 5.39s\n",
      "(Best Epoch 151 | best H: 0.001443| best Pcv: 0.016852| val_loss : 0.002971)\n",
      "  H ç„¡æ”¹å–„ wait_H=1/150\n",
      "Epoch 153 | Learning Rate: 0.000430\n",
      "---\n",
      "alpha: 0.102\n",
      "Epoch 153, loss_H: 0.001444, loss_Pcv: 0.016334\n",
      "Epoch 153 | Train Loss: 0.003098 | Val Loss: 0.002963 | Time: 5.15s\n",
      "(Best Epoch 151 | best H: 0.001443| best Pcv: 0.016852| val_loss : 0.002963)\n",
      "  H ç„¡æ”¹å–„ wait_H=2/150\n",
      "Epoch 154 | Learning Rate: 0.000425\n",
      "---\n",
      "alpha: 0.103\n",
      "Epoch 154, loss_H: 0.001425, loss_Pcv: 0.016420\n",
      "Epoch 154 | Train Loss: 0.003242 | Val Loss: 0.002964 | Time: 5.15s\n",
      "(Best Epoch 151 | best H: 0.001443| best Pcv: 0.016852| val_loss : 0.002964)\n",
      "âœ… Save best H @ epoch 154\n",
      "Epoch 155 | Learning Rate: 0.000421\n",
      "---\n",
      "alpha: 0.103\n",
      "Epoch 155, loss_H: 0.001430, loss_Pcv: 0.016111\n",
      "Epoch 155 | Train Loss: 0.003248 | Val Loss: 0.002947 | Time: 5.07s\n",
      "(Best Epoch 154 | best H: 0.001425| best Pcv: 0.016420| val_loss : 0.002947)\n",
      "  H ç„¡æ”¹å–„ wait_H=1/150\n",
      "Epoch 156 | Learning Rate: 0.000417\n",
      "---\n",
      "alpha: 0.104\n",
      "Epoch 156, loss_H: 0.001413, loss_Pcv: 0.016128\n",
      "Epoch 156 | Train Loss: 0.003100 | Val Loss: 0.002943 | Time: 5.12s\n",
      "(Best Epoch 154 | best H: 0.001425| best Pcv: 0.016420| val_loss : 0.002943)\n",
      "âœ… Save best H @ epoch 156\n",
      "Epoch 157 | Learning Rate: 0.000413\n",
      "---\n",
      "alpha: 0.105\n",
      "Epoch 157, loss_H: 0.001397, loss_Pcv: 0.016162\n",
      "Epoch 157 | Train Loss: 0.003097 | Val Loss: 0.002942 | Time: 5.18s\n",
      "(Best Epoch 156 | best H: 0.001413| best Pcv: 0.016128| val_loss : 0.002942)\n",
      "âœ… Save best H @ epoch 157\n",
      "Epoch 158 | Learning Rate: 0.000409\n",
      "---\n",
      "alpha: 0.105\n",
      "Epoch 158, loss_H: 0.001395, loss_Pcv: 0.015990\n",
      "Epoch 158 | Train Loss: 0.003078 | Val Loss: 0.002932 | Time: 5.43s\n",
      "(Best Epoch 157 | best H: 0.001397| best Pcv: 0.016162| val_loss : 0.002932)\n",
      "âœ… Save best H @ epoch 158\n",
      "Epoch 159 | Learning Rate: 0.000405\n",
      "---\n",
      "alpha: 0.106\n",
      "Epoch 159, loss_H: 0.001394, loss_Pcv: 0.015848\n",
      "Epoch 159 | Train Loss: 0.003212 | Val Loss: 0.002926 | Time: 5.08s\n",
      "(Best Epoch 158 | best H: 0.001395| best Pcv: 0.015990| val_loss : 0.002926)\n",
      "âœ… Save best H @ epoch 159\n",
      "Epoch 160 | Learning Rate: 0.000401\n",
      "---\n",
      "alpha: 0.107\n",
      "Epoch 160, loss_H: 0.001377, loss_Pcv: 0.015873\n",
      "Epoch 160 | Train Loss: 0.003077 | Val Loss: 0.002924 | Time: 5.05s\n",
      "(Best Epoch 159 | best H: 0.001394| best Pcv: 0.015848| val_loss : 0.002924)\n",
      "âœ… Save best H @ epoch 160\n",
      "Epoch 161 | Learning Rate: 0.000397\n",
      "---\n",
      "alpha: 0.107\n",
      "Epoch 161, loss_H: 0.001369, loss_Pcv: 0.015821\n",
      "Epoch 161 | Train Loss: 0.003073 | Val Loss: 0.002920 | Time: 5.08s\n",
      "(Best Epoch 160 | best H: 0.001377| best Pcv: 0.015873| val_loss : 0.002920)\n",
      "âœ… Save best H @ epoch 161\n",
      "Epoch 162 | Learning Rate: 0.000393\n",
      "---\n",
      "alpha: 0.108\n",
      "Epoch 162, loss_H: 0.001367, loss_Pcv: 0.015687\n",
      "Epoch 162 | Train Loss: 0.003201 | Val Loss: 0.002913 | Time: 5.21s\n",
      "(Best Epoch 161 | best H: 0.001369| best Pcv: 0.015821| val_loss : 0.002913)\n",
      "âœ… Save best H @ epoch 162\n",
      "Epoch 163 | Learning Rate: 0.000389\n",
      "---\n",
      "alpha: 0.109\n",
      "Epoch 163, loss_H: 0.001362, loss_Pcv: 0.015596\n",
      "Epoch 163 | Train Loss: 0.003066 | Val Loss: 0.002909 | Time: 5.35s\n",
      "(Best Epoch 162 | best H: 0.001367| best Pcv: 0.015687| val_loss : 0.002909)\n",
      "âœ… Save best H @ epoch 163\n",
      "Epoch 164 | Learning Rate: 0.000385\n",
      "---\n",
      "alpha: 0.109\n",
      "Epoch 164, loss_H: 0.001345, loss_Pcv: 0.015695\n",
      "Epoch 164 | Train Loss: 0.003044 | Val Loss: 0.002914 | Time: 5.29s\n",
      "(Best Epoch 163 | best H: 0.001362| best Pcv: 0.015596| val_loss : 0.002914)\n",
      "âœ… Save best H @ epoch 164\n",
      "Epoch 165 | Learning Rate: 0.000381\n",
      "---\n",
      "alpha: 0.110\n",
      "Epoch 165, loss_H: 0.001333, loss_Pcv: 0.015743\n",
      "Epoch 165 | Train Loss: 0.003094 | Val Loss: 0.002918 | Time: 4.94s\n",
      "(Best Epoch 164 | best H: 0.001345| best Pcv: 0.015695| val_loss : 0.002918)\n",
      "âœ… Save best H @ epoch 165\n",
      "Epoch 166 | Learning Rate: 0.000377\n",
      "---\n",
      "alpha: 0.111\n",
      "Epoch 166, loss_H: 0.001369, loss_Pcv: 0.015284\n",
      "Epoch 166 | Train Loss: 0.003056 | Val Loss: 0.002909 | Time: 5.05s\n",
      "(Best Epoch 165 | best H: 0.001333| best Pcv: 0.015743| val_loss : 0.002909)\n",
      "  H ç„¡æ”¹å–„ wait_H=1/150\n",
      "Epoch 167 | Learning Rate: 0.000373\n",
      "---\n",
      "alpha: 0.111\n",
      "Epoch 167, loss_H: 0.001318, loss_Pcv: 0.015630\n",
      "Epoch 167 | Train Loss: 0.003052 | Val Loss: 0.002912 | Time: 5.02s\n",
      "(Best Epoch 165 | best H: 0.001333| best Pcv: 0.015743| val_loss : 0.002912)\n",
      "âœ… Save best H @ epoch 167\n",
      "Epoch 168 | Learning Rate: 0.000370\n",
      "---\n",
      "alpha: 0.112\n",
      "Epoch 168, loss_H: 0.001335, loss_Pcv: 0.015188\n",
      "Epoch 168 | Train Loss: 0.003028 | Val Loss: 0.002887 | Time: 4.98s\n",
      "(Best Epoch 167 | best H: 0.001318| best Pcv: 0.015630| val_loss : 0.002887)\n",
      "  H ç„¡æ”¹å–„ wait_H=1/150\n",
      "Epoch 169 | Learning Rate: 0.000366\n",
      "---\n",
      "alpha: 0.113\n",
      "Epoch 169, loss_H: 0.001317, loss_Pcv: 0.015256\n",
      "Epoch 169 | Train Loss: 0.003050 | Val Loss: 0.002887 | Time: 5.49s\n",
      "(Best Epoch 167 | best H: 0.001318| best Pcv: 0.015630| val_loss : 0.002887)\n",
      "âœ… Save best H @ epoch 169\n",
      "Epoch 170 | Learning Rate: 0.000362\n",
      "---\n",
      "alpha: 0.113\n",
      "Epoch 170, loss_H: 0.001311, loss_Pcv: 0.015196\n",
      "Epoch 170 | Train Loss: 0.003037 | Val Loss: 0.002885 | Time: 4.99s\n",
      "(Best Epoch 169 | best H: 0.001317| best Pcv: 0.015256| val_loss : 0.002885)\n",
      "âœ… Save best H @ epoch 170\n",
      "Epoch 171 | Learning Rate: 0.000359\n",
      "---\n",
      "alpha: 0.114\n",
      "Epoch 171, loss_H: 0.001315, loss_Pcv: 0.015022\n",
      "Epoch 171 | Train Loss: 0.003194 | Val Loss: 0.002877 | Time: 4.90s\n",
      "(Best Epoch 170 | best H: 0.001311| best Pcv: 0.015196| val_loss : 0.002877)\n",
      "  H ç„¡æ”¹å–„ wait_H=1/150\n",
      "Epoch 172 | Learning Rate: 0.000355\n",
      "---\n",
      "alpha: 0.115\n",
      "Epoch 172, loss_H: 0.001303, loss_Pcv: 0.015010\n",
      "Epoch 172 | Train Loss: 0.003029 | Val Loss: 0.002875 | Time: 5.02s\n",
      "(Best Epoch 170 | best H: 0.001311| best Pcv: 0.015196| val_loss : 0.002875)\n",
      "âœ… Save best H @ epoch 172\n",
      "Epoch 173 | Learning Rate: 0.000351\n",
      "---\n",
      "alpha: 0.115\n",
      "Epoch 173, loss_H: 0.001297, loss_Pcv: 0.014982\n",
      "Epoch 173 | Train Loss: 0.003036 | Val Loss: 0.002876 | Time: 5.16s\n",
      "(Best Epoch 172 | best H: 0.001303| best Pcv: 0.015010| val_loss : 0.002876)\n",
      "âœ… Save best H @ epoch 173\n",
      "Epoch 174 | Learning Rate: 0.000348\n",
      "---\n",
      "alpha: 0.116\n",
      "Epoch 174, loss_H: 0.001299, loss_Pcv: 0.014849\n",
      "Epoch 174 | Train Loss: 0.003206 | Val Loss: 0.002871 | Time: 5.14s\n",
      "(Best Epoch 173 | best H: 0.001297| best Pcv: 0.014982| val_loss : 0.002871)\n",
      "  H ç„¡æ”¹å–„ wait_H=1/150\n",
      "Epoch 175 | Learning Rate: 0.000344\n",
      "---\n",
      "alpha: 0.117\n",
      "Epoch 175, loss_H: 0.001279, loss_Pcv: 0.014952\n",
      "Epoch 175 | Train Loss: 0.003049 | Val Loss: 0.002874 | Time: 5.32s\n",
      "(Best Epoch 173 | best H: 0.001297| best Pcv: 0.014982| val_loss : 0.002874)\n",
      "âœ… Save best H @ epoch 175\n",
      "Epoch 176 | Learning Rate: 0.000341\n",
      "---\n",
      "alpha: 0.117\n",
      "Epoch 176, loss_H: 0.001278, loss_Pcv: 0.014853\n",
      "Epoch 176 | Train Loss: 0.003021 | Val Loss: 0.002871 | Time: 5.03s\n",
      "(Best Epoch 175 | best H: 0.001279| best Pcv: 0.014952| val_loss : 0.002871)\n",
      "âœ… Save best H @ epoch 176\n",
      "Epoch 177 | Learning Rate: 0.000338\n",
      "---\n",
      "alpha: 0.118\n",
      "Epoch 177, loss_H: 0.001277, loss_Pcv: 0.014724\n",
      "Epoch 177 | Train Loss: 0.003014 | Val Loss: 0.002864 | Time: 5.00s\n",
      "(Best Epoch 176 | best H: 0.001278| best Pcv: 0.014853| val_loss : 0.002864)\n",
      "  H ç„¡æ”¹å–„ wait_H=1/150\n",
      "Epoch 178 | Learning Rate: 0.000334\n",
      "---\n",
      "alpha: 0.119\n",
      "Epoch 178, loss_H: 0.001266, loss_Pcv: 0.014794\n",
      "Epoch 178 | Train Loss: 0.003024 | Val Loss: 0.002872 | Time: 5.06s\n",
      "(Best Epoch 176 | best H: 0.001278| best Pcv: 0.014853| val_loss : 0.002872)\n",
      "âœ… Save best H @ epoch 178\n",
      "Epoch 179 | Learning Rate: 0.000331\n",
      "---\n",
      "alpha: 0.119\n",
      "Epoch 179, loss_H: 0.001263, loss_Pcv: 0.014720\n",
      "Epoch 179 | Train Loss: 0.003194 | Val Loss: 0.002869 | Time: 4.88s\n",
      "(Best Epoch 178 | best H: 0.001266| best Pcv: 0.014794| val_loss : 0.002869)\n",
      "âœ… Save best H @ epoch 179\n",
      "Epoch 180 | Learning Rate: 0.000328\n",
      "---\n",
      "alpha: 0.120\n",
      "Epoch 180, loss_H: 0.001261, loss_Pcv: 0.014593\n",
      "Epoch 180 | Train Loss: 0.003351 | Val Loss: 0.002861 | Time: 4.96s\n",
      "(Best Epoch 179 | best H: 0.001263| best Pcv: 0.014720| val_loss : 0.002861)\n",
      "âœ… Save best H @ epoch 180\n",
      "Epoch 181 | Learning Rate: 0.000324\n",
      "---\n",
      "alpha: 0.121\n",
      "Epoch 181, loss_H: 0.001254, loss_Pcv: 0.014544\n",
      "Epoch 181 | Train Loss: 0.003190 | Val Loss: 0.002858 | Time: 5.18s\n",
      "(Best Epoch 180 | best H: 0.001261| best Pcv: 0.014593| val_loss : 0.002858)\n",
      "âœ… Save best H @ epoch 181\n",
      "Epoch 182 | Learning Rate: 0.000321\n",
      "---\n",
      "alpha: 0.121\n",
      "Epoch 182, loss_H: 0.001235, loss_Pcv: 0.014757\n",
      "Epoch 182 | Train Loss: 0.003035 | Val Loss: 0.002876 | Time: 4.92s\n",
      "(Best Epoch 181 | best H: 0.001254| best Pcv: 0.014544| val_loss : 0.002876)\n",
      "âœ… Save best H @ epoch 182\n",
      "Epoch 183 | Learning Rate: 0.000318\n",
      "---\n",
      "alpha: 0.122\n",
      "Epoch 183, loss_H: 0.001259, loss_Pcv: 0.014338\n",
      "Epoch 183 | Train Loss: 0.003027 | Val Loss: 0.002855 | Time: 4.92s\n",
      "(Best Epoch 182 | best H: 0.001235| best Pcv: 0.014757| val_loss : 0.002855)\n",
      "  H ç„¡æ”¹å–„ wait_H=1/150\n",
      "Epoch 184 | Learning Rate: 0.000315\n",
      "---\n",
      "alpha: 0.123\n",
      "Epoch 184, loss_H: 0.001227, loss_Pcv: 0.014656\n",
      "Epoch 184 | Train Loss: 0.003046 | Val Loss: 0.002875 | Time: 5.21s\n",
      "(Best Epoch 182 | best H: 0.001235| best Pcv: 0.014757| val_loss : 0.002875)\n",
      "âœ… Save best H @ epoch 184\n",
      "Epoch 185 | Learning Rate: 0.000312\n",
      "---\n",
      "alpha: 0.123\n",
      "Epoch 185, loss_H: 0.001244, loss_Pcv: 0.014268\n",
      "Epoch 185 | Train Loss: 0.003184 | Val Loss: 0.002851 | Time: 5.14s\n",
      "(Best Epoch 184 | best H: 0.001227| best Pcv: 0.014656| val_loss : 0.002851)\n",
      "  H ç„¡æ”¹å–„ wait_H=1/150\n",
      "Epoch 186 | Learning Rate: 0.000308\n",
      "---\n",
      "alpha: 0.124\n",
      "Epoch 186, loss_H: 0.001225, loss_Pcv: 0.014412\n",
      "Epoch 186 | Train Loss: 0.003016 | Val Loss: 0.002860 | Time: 5.35s\n",
      "(Best Epoch 184 | best H: 0.001227| best Pcv: 0.014656| val_loss : 0.002860)\n",
      "âœ… Save best H @ epoch 186\n",
      "Epoch 187 | Learning Rate: 0.000305\n",
      "---\n",
      "alpha: 0.125\n",
      "Epoch 187, loss_H: 0.001223, loss_Pcv: 0.014301\n",
      "Epoch 187 | Train Loss: 0.003039 | Val Loss: 0.002853 | Time: 5.05s\n",
      "(Best Epoch 186 | best H: 0.001225| best Pcv: 0.014412| val_loss : 0.002853)\n",
      "âœ… Save best H @ epoch 187\n",
      "Epoch 188 | Learning Rate: 0.000302\n",
      "---\n",
      "alpha: 0.125\n",
      "Epoch 188, loss_H: 0.001234, loss_Pcv: 0.014096\n",
      "Epoch 188 | Train Loss: 0.003025 | Val Loss: 0.002846 | Time: 5.03s\n",
      "(Best Epoch 187 | best H: 0.001223| best Pcv: 0.014301| val_loss : 0.002846)\n",
      "  H ç„¡æ”¹å–„ wait_H=1/150\n",
      "Epoch 189 | Learning Rate: 0.000299\n",
      "---\n",
      "alpha: 0.126\n",
      "Epoch 189, loss_H: 0.001202, loss_Pcv: 0.014469\n",
      "Epoch 189 | Train Loss: 0.003036 | Val Loss: 0.002873 | Time: 5.12s\n",
      "(Best Epoch 187 | best H: 0.001223| best Pcv: 0.014301| val_loss : 0.002873)\n",
      "âœ… Save best H @ epoch 189\n",
      "Epoch 190 | Learning Rate: 0.000296\n",
      "---\n",
      "alpha: 0.127\n",
      "Epoch 190, loss_H: 0.001231, loss_Pcv: 0.013985\n",
      "Epoch 190 | Train Loss: 0.003030 | Val Loss: 0.002847 | Time: 4.99s\n",
      "(Best Epoch 189 | best H: 0.001202| best Pcv: 0.014469| val_loss : 0.002847)\n",
      "  H ç„¡æ”¹å–„ wait_H=1/150\n",
      "Epoch 191 | Learning Rate: 0.000293\n",
      "---\n",
      "alpha: 0.127\n",
      "Epoch 191, loss_H: 0.001201, loss_Pcv: 0.014218\n",
      "Epoch 191 | Train Loss: 0.003212 | Val Loss: 0.002858 | Time: 5.15s\n",
      "(Best Epoch 189 | best H: 0.001202| best Pcv: 0.014469| val_loss : 0.002858)\n",
      "  H ç„¡æ”¹å–„ wait_H=2/150\n",
      "Epoch 192 | Learning Rate: 0.000290\n",
      "---\n",
      "alpha: 0.128\n",
      "Epoch 192, loss_H: 0.001217, loss_Pcv: 0.013906\n",
      "Epoch 192 | Train Loss: 0.003204 | Val Loss: 0.002841 | Time: 5.14s\n",
      "(Best Epoch 189 | best H: 0.001202| best Pcv: 0.014469| val_loss : 0.002841)\n",
      "  H ç„¡æ”¹å–„ wait_H=3/150\n",
      "Epoch 193 | Learning Rate: 0.000287\n",
      "---\n",
      "alpha: 0.129\n",
      "Epoch 193, loss_H: 0.001194, loss_Pcv: 0.014128\n",
      "Epoch 193 | Train Loss: 0.003188 | Val Loss: 0.002858 | Time: 5.02s\n",
      "(Best Epoch 189 | best H: 0.001202| best Pcv: 0.014469| val_loss : 0.002858)\n",
      "âœ… Save best H @ epoch 193\n",
      "Epoch 194 | Learning Rate: 0.000285\n",
      "---\n",
      "alpha: 0.129\n",
      "Epoch 194, loss_H: 0.001210, loss_Pcv: 0.013827\n",
      "Epoch 194 | Train Loss: 0.003017 | Val Loss: 0.002842 | Time: 5.01s\n",
      "(Best Epoch 193 | best H: 0.001194| best Pcv: 0.014128| val_loss : 0.002842)\n",
      "  H ç„¡æ”¹å–„ wait_H=1/150\n",
      "Epoch 195 | Learning Rate: 0.000282\n",
      "---\n",
      "alpha: 0.130\n",
      "Epoch 195, loss_H: 0.001198, loss_Pcv: 0.013857\n",
      "Epoch 195 | Train Loss: 0.003206 | Val Loss: 0.002844 | Time: 5.29s\n",
      "(Best Epoch 193 | best H: 0.001194| best Pcv: 0.014128| val_loss : 0.002844)\n",
      "  H ç„¡æ”¹å–„ wait_H=2/150\n",
      "Epoch 196 | Learning Rate: 0.000279\n",
      "---\n",
      "alpha: 0.131\n",
      "Epoch 196, loss_H: 0.001197, loss_Pcv: 0.013784\n",
      "Epoch 196 | Train Loss: 0.003024 | Val Loss: 0.002842 | Time: 4.94s\n",
      "(Best Epoch 193 | best H: 0.001194| best Pcv: 0.014128| val_loss : 0.002842)\n",
      "  H ç„¡æ”¹å–„ wait_H=3/150\n",
      "Epoch 197 | Learning Rate: 0.000276\n",
      "---\n",
      "alpha: 0.131\n",
      "Epoch 197, loss_H: 0.001171, loss_Pcv: 0.014092\n",
      "Epoch 197 | Train Loss: 0.003020 | Val Loss: 0.002868 | Time: 5.01s\n",
      "(Best Epoch 193 | best H: 0.001194| best Pcv: 0.014128| val_loss : 0.002868)\n",
      "âœ… Save best H @ epoch 197\n",
      "Epoch 198 | Learning Rate: 0.000273\n",
      "---\n",
      "alpha: 0.132\n",
      "Epoch 198, loss_H: 0.001194, loss_Pcv: 0.013677\n",
      "Epoch 198 | Train Loss: 0.003010 | Val Loss: 0.002842 | Time: 5.24s\n",
      "(Best Epoch 197 | best H: 0.001171| best Pcv: 0.014092| val_loss : 0.002842)\n",
      "  H ç„¡æ”¹å–„ wait_H=1/150\n",
      "Epoch 199 | Learning Rate: 0.000271\n",
      "---\n",
      "alpha: 0.133\n",
      "Epoch 199, loss_H: 0.001168, loss_Pcv: 0.013910\n",
      "Epoch 199 | Train Loss: 0.003015 | Val Loss: 0.002858 | Time: 4.96s\n",
      "(Best Epoch 197 | best H: 0.001171| best Pcv: 0.014092| val_loss : 0.002858)\n",
      "âœ… Save best H @ epoch 199\n",
      "Epoch 200 | Learning Rate: 0.000268\n",
      "---\n",
      "alpha: 0.133\n",
      "Epoch 200, loss_H: 0.001184, loss_Pcv: 0.013604\n",
      "Epoch 200 | Train Loss: 0.003025 | Val Loss: 0.002840 | Time: 5.12s\n",
      "(Best Epoch 199 | best H: 0.001168| best Pcv: 0.013910| val_loss : 0.002840)\n",
      "  H ç„¡æ”¹å–„ wait_H=1/150\n",
      "Epoch 201 | Learning Rate: 0.000265\n",
      "---\n",
      "alpha: 0.134\n",
      "Epoch 201, loss_H: 0.001168, loss_Pcv: 0.013707\n",
      "Epoch 201 | Train Loss: 0.003026 | Val Loss: 0.002849 | Time: 5.20s\n",
      "(Best Epoch 199 | best H: 0.001168| best Pcv: 0.013910| val_loss : 0.002849)\n",
      "  H ç„¡æ”¹å–„ wait_H=2/150\n",
      "Epoch 202 | Learning Rate: 0.000263\n",
      "---\n",
      "alpha: 0.135\n",
      "Epoch 202, loss_H: 0.001170, loss_Pcv: 0.013600\n",
      "Epoch 202 | Train Loss: 0.003207 | Val Loss: 0.002844 | Time: 5.08s\n",
      "(Best Epoch 199 | best H: 0.001168| best Pcv: 0.013910| val_loss : 0.002844)\n",
      "  H ç„¡æ”¹å–„ wait_H=3/150\n",
      "Epoch 203 | Learning Rate: 0.000260\n",
      "---\n",
      "alpha: 0.135\n",
      "Epoch 203, loss_H: 0.001169, loss_Pcv: 0.013560\n",
      "Epoch 203 | Train Loss: 0.003051 | Val Loss: 0.002846 | Time: 5.20s\n",
      "(Best Epoch 199 | best H: 0.001168| best Pcv: 0.013910| val_loss : 0.002846)\n",
      "  H ç„¡æ”¹å–„ wait_H=4/150\n",
      "Epoch 204 | Learning Rate: 0.000257\n",
      "---\n",
      "alpha: 0.136\n",
      "Epoch 204, loss_H: 0.001158, loss_Pcv: 0.013611\n",
      "Epoch 204 | Train Loss: 0.003206 | Val Loss: 0.002851 | Time: 5.09s\n",
      "(Best Epoch 199 | best H: 0.001168| best Pcv: 0.013910| val_loss : 0.002851)\n",
      "âœ… Save best H @ epoch 204\n",
      "Epoch 205 | Learning Rate: 0.000255\n",
      "---\n",
      "alpha: 0.137\n",
      "Epoch 205, loss_H: 0.001175, loss_Pcv: 0.013384\n",
      "Epoch 205 | Train Loss: 0.003042 | Val Loss: 0.002844 | Time: 4.96s\n",
      "(Best Epoch 204 | best H: 0.001158| best Pcv: 0.013611| val_loss : 0.002844)\n",
      "  H ç„¡æ”¹å–„ wait_H=1/150\n",
      "Epoch 206 | Learning Rate: 0.000252\n",
      "---\n",
      "alpha: 0.137\n",
      "Epoch 206, loss_H: 0.001144, loss_Pcv: 0.013669\n",
      "Epoch 206 | Train Loss: 0.003030 | Val Loss: 0.002864 | Time: 5.23s\n",
      "(Best Epoch 204 | best H: 0.001158| best Pcv: 0.013611| val_loss : 0.002864)\n",
      "âœ… Save best H @ epoch 206\n",
      "Epoch 207 | Learning Rate: 0.000250\n",
      "---\n",
      "alpha: 0.138\n",
      "Epoch 207, loss_H: 0.001167, loss_Pcv: 0.013326\n",
      "Epoch 207 | Train Loss: 0.003030 | Val Loss: 0.002845 | Time: 5.37s\n",
      "(Best Epoch 206 | best H: 0.001144| best Pcv: 0.013669| val_loss : 0.002845)\n",
      "  H ç„¡æ”¹å–„ wait_H=1/150\n",
      "Epoch 208 | Learning Rate: 0.000247\n",
      "---\n",
      "alpha: 0.139\n",
      "Epoch 208, loss_H: 0.001137, loss_Pcv: 0.013587\n",
      "Epoch 208 | Train Loss: 0.003427 | Val Loss: 0.002864 | Time: 5.30s\n",
      "(Best Epoch 206 | best H: 0.001144| best Pcv: 0.013669| val_loss : 0.002864)\n",
      "âœ… Save best H @ epoch 208\n",
      "Epoch 209 | Learning Rate: 0.000245\n",
      "---\n",
      "alpha: 0.139\n",
      "Epoch 209, loss_H: 0.001158, loss_Pcv: 0.013262\n",
      "Epoch 209 | Train Loss: 0.003029 | Val Loss: 0.002844 | Time: 5.09s\n",
      "(Best Epoch 208 | best H: 0.001137| best Pcv: 0.013587| val_loss : 0.002844)\n",
      "  H ç„¡æ”¹å–„ wait_H=1/150\n",
      "Epoch 210 | Learning Rate: 0.000242\n",
      "---\n",
      "alpha: 0.140\n",
      "Epoch 210, loss_H: 0.001129, loss_Pcv: 0.013496\n",
      "Epoch 210 | Train Loss: 0.003251 | Val Loss: 0.002860 | Time: 4.94s\n",
      "(Best Epoch 208 | best H: 0.001137| best Pcv: 0.013587| val_loss : 0.002860)\n",
      "âœ… Save best H @ epoch 210\n",
      "Epoch 211 | Learning Rate: 0.000240\n",
      "---\n",
      "alpha: 0.141\n",
      "Epoch 211, loss_H: 0.001146, loss_Pcv: 0.013220\n",
      "Epoch 211 | Train Loss: 0.003046 | Val Loss: 0.002844 | Time: 5.05s\n",
      "(Best Epoch 210 | best H: 0.001129| best Pcv: 0.013496| val_loss : 0.002844)\n",
      "  H ç„¡æ”¹å–„ wait_H=1/150\n",
      "Epoch 212 | Learning Rate: 0.000238\n",
      "---\n",
      "alpha: 0.141\n",
      "Epoch 212, loss_H: 0.001129, loss_Pcv: 0.013345\n",
      "Epoch 212 | Train Loss: 0.003411 | Val Loss: 0.002855 | Time: 5.08s\n",
      "(Best Epoch 210 | best H: 0.001129| best Pcv: 0.013496| val_loss : 0.002855)\n",
      "  H ç„¡æ”¹å–„ wait_H=2/150\n",
      "Epoch 213 | Learning Rate: 0.000235\n",
      "---\n",
      "alpha: 0.142\n",
      "Epoch 213, loss_H: 0.001143, loss_Pcv: 0.013143\n",
      "Epoch 213 | Train Loss: 0.003238 | Val Loss: 0.002847 | Time: 5.24s\n",
      "(Best Epoch 210 | best H: 0.001129| best Pcv: 0.013496| val_loss : 0.002847)\n",
      "  H ç„¡æ”¹å–„ wait_H=3/150\n",
      "Epoch 214 | Learning Rate: 0.000233\n",
      "---\n",
      "alpha: 0.143\n",
      "Epoch 214, loss_H: 0.001136, loss_Pcv: 0.013135\n",
      "Epoch 214 | Train Loss: 0.003045 | Val Loss: 0.002847 | Time: 5.18s\n",
      "(Best Epoch 210 | best H: 0.001129| best Pcv: 0.013496| val_loss : 0.002847)\n",
      "  H ç„¡æ”¹å–„ wait_H=4/150\n",
      "Epoch 215 | Learning Rate: 0.000230\n",
      "---\n",
      "alpha: 0.143\n",
      "Epoch 215, loss_H: 0.001121, loss_Pcv: 0.013238\n",
      "Epoch 215 | Train Loss: 0.003041 | Val Loss: 0.002858 | Time: 5.13s\n",
      "(Best Epoch 210 | best H: 0.001129| best Pcv: 0.013496| val_loss : 0.002858)\n",
      "âœ… Save best H @ epoch 215\n",
      "Epoch 216 | Learning Rate: 0.000228\n",
      "---\n",
      "alpha: 0.144\n",
      "Epoch 216, loss_H: 0.001127, loss_Pcv: 0.013114\n",
      "Epoch 216 | Train Loss: 0.003037 | Val Loss: 0.002853 | Time: 4.97s\n",
      "(Best Epoch 215 | best H: 0.001121| best Pcv: 0.013238| val_loss : 0.002853)\n",
      "  H ç„¡æ”¹å–„ wait_H=1/150\n",
      "Epoch 217 | Learning Rate: 0.000226\n",
      "---\n",
      "alpha: 0.145\n",
      "Epoch 217, loss_H: 0.001118, loss_Pcv: 0.013162\n",
      "Epoch 217 | Train Loss: 0.003059 | Val Loss: 0.002861 | Time: 5.27s\n",
      "(Best Epoch 215 | best H: 0.001121| best Pcv: 0.013238| val_loss : 0.002861)\n",
      "âœ… Save best H @ epoch 217\n",
      "Epoch 218 | Learning Rate: 0.000224\n",
      "---\n",
      "alpha: 0.145\n",
      "Epoch 218, loss_H: 0.001122, loss_Pcv: 0.013049\n",
      "Epoch 218 | Train Loss: 0.003046 | Val Loss: 0.002856 | Time: 5.18s\n",
      "(Best Epoch 217 | best H: 0.001118| best Pcv: 0.013162| val_loss : 0.002856)\n",
      "  H ç„¡æ”¹å–„ wait_H=1/150\n",
      "Epoch 219 | Learning Rate: 0.000221\n",
      "---\n",
      "alpha: 0.146\n",
      "Epoch 219, loss_H: 0.001118, loss_Pcv: 0.013015\n",
      "Epoch 219 | Train Loss: 0.003248 | Val Loss: 0.002855 | Time: 5.18s\n",
      "(Best Epoch 217 | best H: 0.001118| best Pcv: 0.013162| val_loss : 0.002855)\n",
      "  H ç„¡æ”¹å–„ wait_H=2/150\n",
      "Epoch 220 | Learning Rate: 0.000219\n",
      "---\n",
      "alpha: 0.147\n",
      "Epoch 220, loss_H: 0.001112, loss_Pcv: 0.013016\n",
      "Epoch 220 | Train Loss: 0.003042 | Val Loss: 0.002858 | Time: 5.18s\n",
      "(Best Epoch 217 | best H: 0.001118| best Pcv: 0.013162| val_loss : 0.002858)\n",
      "âœ… Save best H @ epoch 220\n",
      "Epoch 221 | Learning Rate: 0.000217\n",
      "---\n",
      "alpha: 0.147\n",
      "Epoch 221, loss_H: 0.001109, loss_Pcv: 0.013027\n",
      "Epoch 221 | Train Loss: 0.003062 | Val Loss: 0.002865 | Time: 4.98s\n",
      "(Best Epoch 220 | best H: 0.001112| best Pcv: 0.013016| val_loss : 0.002865)\n",
      "âœ… Save best H @ epoch 221\n",
      "Epoch 222 | Learning Rate: 0.000215\n",
      "---\n",
      "alpha: 0.148\n",
      "Epoch 222, loss_H: 0.001107, loss_Pcv: 0.012949\n",
      "Epoch 222 | Train Loss: 0.003275 | Val Loss: 0.002860 | Time: 5.01s\n",
      "(Best Epoch 221 | best H: 0.001109| best Pcv: 0.013027| val_loss : 0.002860)\n",
      "âœ… Save best H @ epoch 222\n",
      "Epoch 223 | Learning Rate: 0.000213\n",
      "---\n",
      "alpha: 0.149\n",
      "Epoch 223, loss_H: 0.001107, loss_Pcv: 0.012891\n",
      "Epoch 223 | Train Loss: 0.003060 | Val Loss: 0.002859 | Time: 5.06s\n",
      "(Best Epoch 222 | best H: 0.001107| best Pcv: 0.012949| val_loss : 0.002859)\n",
      "  H ç„¡æ”¹å–„ wait_H=1/150\n",
      "Epoch 224 | Learning Rate: 0.000211\n",
      "---\n",
      "alpha: 0.149\n",
      "Epoch 224, loss_H: 0.001098, loss_Pcv: 0.012930\n",
      "Epoch 224 | Train Loss: 0.003060 | Val Loss: 0.002865 | Time: 5.01s\n",
      "(Best Epoch 222 | best H: 0.001107| best Pcv: 0.012949| val_loss : 0.002865)\n",
      "âœ… Save best H @ epoch 224\n",
      "Epoch 225 | Learning Rate: 0.000208\n",
      "---\n",
      "alpha: 0.150\n",
      "Epoch 225, loss_H: 0.001104, loss_Pcv: 0.012810\n",
      "Epoch 225 | Train Loss: 0.003068 | Val Loss: 0.002860 | Time: 5.11s\n",
      "(Best Epoch 224 | best H: 0.001098| best Pcv: 0.012930| val_loss : 0.002860)\n",
      "  H ç„¡æ”¹å–„ wait_H=1/150\n",
      "Epoch 226 | Learning Rate: 0.000206\n",
      "---\n",
      "alpha: 0.151\n",
      "Epoch 226, loss_H: 0.001104, loss_Pcv: 0.012784\n",
      "Epoch 226 | Train Loss: 0.003059 | Val Loss: 0.002864 | Time: 5.13s\n",
      "(Best Epoch 224 | best H: 0.001098| best Pcv: 0.012930| val_loss : 0.002864)\n",
      "  H ç„¡æ”¹å–„ wait_H=2/150\n",
      "Epoch 227 | Learning Rate: 0.000204\n",
      "---\n",
      "alpha: 0.151\n",
      "Epoch 227, loss_H: 0.001090, loss_Pcv: 0.012884\n",
      "Epoch 227 | Train Loss: 0.003076 | Val Loss: 0.002875 | Time: 4.91s\n",
      "(Best Epoch 224 | best H: 0.001098| best Pcv: 0.012930| val_loss : 0.002875)\n",
      "âœ… Save best H @ epoch 227\n",
      "Epoch 228 | Learning Rate: 0.000202\n",
      "---\n",
      "alpha: 0.152\n",
      "Epoch 228, loss_H: 0.001101, loss_Pcv: 0.012707\n",
      "Epoch 228 | Train Loss: 0.003074 | Val Loss: 0.002865 | Time: 5.29s\n",
      "(Best Epoch 227 | best H: 0.001090| best Pcv: 0.012884| val_loss : 0.002865)\n",
      "  H ç„¡æ”¹å–„ wait_H=1/150\n",
      "Epoch 229 | Learning Rate: 0.000200\n",
      "---\n",
      "alpha: 0.153\n",
      "Epoch 229, loss_H: 0.001087, loss_Pcv: 0.012820\n",
      "Epoch 229 | Train Loss: 0.003068 | Val Loss: 0.002879 | Time: 5.28s\n",
      "(Best Epoch 227 | best H: 0.001090| best Pcv: 0.012884| val_loss : 0.002879)\n",
      "âœ… Save best H @ epoch 229\n",
      "Epoch 230 | Learning Rate: 0.000198\n",
      "---\n",
      "alpha: 0.153\n",
      "Epoch 230, loss_H: 0.001087, loss_Pcv: 0.012784\n",
      "Epoch 230 | Train Loss: 0.003077 | Val Loss: 0.002880 | Time: 5.06s\n",
      "(Best Epoch 229 | best H: 0.001087| best Pcv: 0.012820| val_loss : 0.002880)\n",
      "  H ç„¡æ”¹å–„ wait_H=1/150\n",
      "Epoch 231 | Learning Rate: 0.000196\n",
      "---\n",
      "alpha: 0.154\n",
      "Epoch 231, loss_H: 0.001089, loss_Pcv: 0.012703\n",
      "Epoch 231 | Train Loss: 0.003118 | Val Loss: 0.002877 | Time: 5.24s\n",
      "(Best Epoch 229 | best H: 0.001087| best Pcv: 0.012820| val_loss : 0.002877)\n",
      "  H ç„¡æ”¹å–„ wait_H=2/150\n",
      "Epoch 232 | Learning Rate: 0.000194\n",
      "---\n",
      "alpha: 0.155\n",
      "Epoch 232, loss_H: 0.001095, loss_Pcv: 0.012599\n",
      "Epoch 232 | Train Loss: 0.003087 | Val Loss: 0.002874 | Time: 5.03s\n",
      "(Best Epoch 229 | best H: 0.001087| best Pcv: 0.012820| val_loss : 0.002874)\n",
      "  H ç„¡æ”¹å–„ wait_H=3/150\n",
      "Epoch 233 | Learning Rate: 0.000192\n",
      "---\n",
      "alpha: 0.155\n",
      "Epoch 233, loss_H: 0.001072, loss_Pcv: 0.012887\n",
      "Epoch 233 | Train Loss: 0.003102 | Val Loss: 0.002908 | Time: 5.06s\n",
      "(Best Epoch 229 | best H: 0.001087| best Pcv: 0.012820| val_loss : 0.002908)\n",
      "âœ… Save best H @ epoch 233\n",
      "Epoch 234 | Learning Rate: 0.000190\n",
      "---\n",
      "alpha: 0.156\n",
      "Epoch 234, loss_H: 0.001102, loss_Pcv: 0.012517\n",
      "Epoch 234 | Train Loss: 0.003315 | Val Loss: 0.002882 | Time: 5.22s\n",
      "(Best Epoch 233 | best H: 0.001072| best Pcv: 0.012887| val_loss : 0.002882)\n",
      "  H ç„¡æ”¹å–„ wait_H=1/150\n",
      "Epoch 235 | Learning Rate: 0.000188\n",
      "---\n",
      "alpha: 0.157\n",
      "Epoch 235, loss_H: 0.001070, loss_Pcv: 0.012795\n",
      "Epoch 235 | Train Loss: 0.003290 | Val Loss: 0.002907 | Time: 5.17s\n",
      "(Best Epoch 233 | best H: 0.001072| best Pcv: 0.012887| val_loss : 0.002907)\n",
      "âœ… Save best H @ epoch 235\n",
      "Epoch 236 | Learning Rate: 0.000187\n",
      "---\n",
      "alpha: 0.157\n",
      "Epoch 236, loss_H: 0.001102, loss_Pcv: 0.012502\n",
      "Epoch 236 | Train Loss: 0.003336 | Val Loss: 0.002895 | Time: 5.03s\n",
      "(Best Epoch 235 | best H: 0.001070| best Pcv: 0.012795| val_loss : 0.002895)\n",
      "  H ç„¡æ”¹å–„ wait_H=1/150\n",
      "Epoch 237 | Learning Rate: 0.000185\n",
      "---\n",
      "alpha: 0.158\n",
      "Epoch 237, loss_H: 0.001060, loss_Pcv: 0.012840\n",
      "Epoch 237 | Train Loss: 0.003105 | Val Loss: 0.002921 | Time: 5.19s\n",
      "(Best Epoch 235 | best H: 0.001070| best Pcv: 0.012795| val_loss : 0.002921)\n",
      "âœ… Save best H @ epoch 237\n",
      "Epoch 238 | Learning Rate: 0.000183\n",
      "---\n",
      "alpha: 0.159\n",
      "Epoch 238, loss_H: 0.001085, loss_Pcv: 0.012439\n",
      "Epoch 238 | Train Loss: 0.003096 | Val Loss: 0.002887 | Time: 5.01s\n",
      "(Best Epoch 237 | best H: 0.001060| best Pcv: 0.012840| val_loss : 0.002887)\n",
      "  H ç„¡æ”¹å–„ wait_H=1/150\n",
      "Epoch 239 | Learning Rate: 0.000181\n",
      "---\n",
      "alpha: 0.159\n",
      "Epoch 239, loss_H: 0.001063, loss_Pcv: 0.012626\n",
      "Epoch 239 | Train Loss: 0.003110 | Val Loss: 0.002905 | Time: 5.12s\n",
      "(Best Epoch 237 | best H: 0.001060| best Pcv: 0.012840| val_loss : 0.002905)\n",
      "  H ç„¡æ”¹å–„ wait_H=2/150\n",
      "Epoch 240 | Learning Rate: 0.000179\n",
      "---\n",
      "alpha: 0.160\n",
      "Epoch 240, loss_H: 0.001074, loss_Pcv: 0.012429\n",
      "Epoch 240 | Train Loss: 0.003094 | Val Loss: 0.002891 | Time: 5.37s\n",
      "(Best Epoch 237 | best H: 0.001060| best Pcv: 0.012840| val_loss : 0.002891)\n",
      "  H ç„¡æ”¹å–„ wait_H=3/150\n",
      "Epoch 241 | Learning Rate: 0.000177\n",
      "---\n",
      "alpha: 0.161\n",
      "Epoch 241, loss_H: 0.001068, loss_Pcv: 0.012439\n",
      "Epoch 241 | Train Loss: 0.003100 | Val Loss: 0.002895 | Time: 5.24s\n",
      "(Best Epoch 237 | best H: 0.001060| best Pcv: 0.012840| val_loss : 0.002895)\n",
      "  H ç„¡æ”¹å–„ wait_H=4/150\n",
      "Epoch 242 | Learning Rate: 0.000176\n",
      "---\n",
      "alpha: 0.161\n",
      "Epoch 242, loss_H: 0.001062, loss_Pcv: 0.012466\n",
      "Epoch 242 | Train Loss: 0.003095 | Val Loss: 0.002902 | Time: 5.08s\n",
      "(Best Epoch 237 | best H: 0.001060| best Pcv: 0.012840| val_loss : 0.002902)\n",
      "  H ç„¡æ”¹å–„ wait_H=5/150\n",
      "Epoch 243 | Learning Rate: 0.000174\n",
      "---\n",
      "alpha: 0.162\n",
      "Epoch 243, loss_H: 0.001081, loss_Pcv: 0.012332\n",
      "Epoch 243 | Train Loss: 0.003118 | Val Loss: 0.002903 | Time: 5.10s\n",
      "(Best Epoch 237 | best H: 0.001060| best Pcv: 0.012840| val_loss : 0.002903)\n",
      "  H ç„¡æ”¹å–„ wait_H=6/150\n",
      "Epoch 244 | Learning Rate: 0.000172\n",
      "---\n",
      "alpha: 0.163\n",
      "Epoch 244, loss_H: 0.001048, loss_Pcv: 0.012598\n",
      "Epoch 244 | Train Loss: 0.003132 | Val Loss: 0.002927 | Time: 5.08s\n",
      "(Best Epoch 237 | best H: 0.001060| best Pcv: 0.012840| val_loss : 0.002927)\n",
      "âœ… Save best H @ epoch 244\n",
      "Epoch 245 | Learning Rate: 0.000170\n",
      "---\n",
      "alpha: 0.163\n",
      "Epoch 245, loss_H: 0.001067, loss_Pcv: 0.012291\n",
      "Epoch 245 | Train Loss: 0.003111 | Val Loss: 0.002900 | Time: 5.13s\n",
      "(Best Epoch 244 | best H: 0.001048| best Pcv: 0.012598| val_loss : 0.002900)\n",
      "  H ç„¡æ”¹å–„ wait_H=1/150\n",
      "Epoch 246 | Learning Rate: 0.000169\n",
      "---\n",
      "alpha: 0.164\n",
      "Epoch 246, loss_H: 0.001054, loss_Pcv: 0.012375\n",
      "Epoch 246 | Train Loss: 0.003111 | Val Loss: 0.002911 | Time: 5.15s\n",
      "(Best Epoch 244 | best H: 0.001048| best Pcv: 0.012598| val_loss : 0.002911)\n",
      "  H ç„¡æ”¹å–„ wait_H=2/150\n",
      "Epoch 247 | Learning Rate: 0.000167\n",
      "---\n",
      "alpha: 0.165\n",
      "Epoch 247, loss_H: 0.001056, loss_Pcv: 0.012303\n",
      "Epoch 247 | Train Loss: 0.003116 | Val Loss: 0.002908 | Time: 5.13s\n",
      "(Best Epoch 244 | best H: 0.001048| best Pcv: 0.012598| val_loss : 0.002908)\n",
      "  H ç„¡æ”¹å–„ wait_H=3/150\n",
      "Epoch 248 | Learning Rate: 0.000165\n",
      "---\n",
      "alpha: 0.165\n",
      "Epoch 248, loss_H: 0.001050, loss_Pcv: 0.012342\n",
      "Epoch 248 | Train Loss: 0.003337 | Val Loss: 0.002917 | Time: 5.08s\n",
      "(Best Epoch 244 | best H: 0.001048| best Pcv: 0.012598| val_loss : 0.002917)\n",
      "  H ç„¡æ”¹å–„ wait_H=4/150\n",
      "Epoch 249 | Learning Rate: 0.000164\n",
      "---\n",
      "alpha: 0.166\n",
      "Epoch 249, loss_H: 0.001056, loss_Pcv: 0.012239\n",
      "Epoch 249 | Train Loss: 0.003113 | Val Loss: 0.002912 | Time: 4.97s\n",
      "(Best Epoch 244 | best H: 0.001048| best Pcv: 0.012598| val_loss : 0.002912)\n",
      "  H ç„¡æ”¹å–„ wait_H=5/150\n",
      "Epoch 250 | Learning Rate: 0.000162\n",
      "---\n",
      "alpha: 0.167\n",
      "Epoch 250, loss_H: 0.001048, loss_Pcv: 0.012273\n",
      "Epoch 250 | Train Loss: 0.003367 | Val Loss: 0.002919 | Time: 5.08s\n",
      "(Best Epoch 244 | best H: 0.001048| best Pcv: 0.012598| val_loss : 0.002919)\n",
      "  H ç„¡æ”¹å–„ wait_H=6/150\n",
      "Epoch 251 | Learning Rate: 0.000160\n",
      "---\n",
      "alpha: 0.167\n",
      "Epoch 251, loss_H: 0.001058, loss_Pcv: 0.012163\n",
      "Epoch 251 | Train Loss: 0.003136 | Val Loss: 0.002916 | Time: 5.14s\n",
      "(Best Epoch 244 | best H: 0.001048| best Pcv: 0.012598| val_loss : 0.002916)\n",
      "  H ç„¡æ”¹å–„ wait_H=7/150\n",
      "Epoch 252 | Learning Rate: 0.000159\n",
      "---\n",
      "alpha: 0.168\n",
      "Epoch 252, loss_H: 0.001035, loss_Pcv: 0.012364\n",
      "Epoch 252 | Train Loss: 0.003131 | Val Loss: 0.002938 | Time: 5.12s\n",
      "(Best Epoch 244 | best H: 0.001048| best Pcv: 0.012598| val_loss : 0.002938)\n",
      "âœ… Save best H @ epoch 252\n",
      "Epoch 253 | Learning Rate: 0.000157\n",
      "---\n",
      "alpha: 0.169\n",
      "Epoch 253, loss_H: 0.001051, loss_Pcv: 0.012136\n",
      "Epoch 253 | Train Loss: 0.003150 | Val Loss: 0.002921 | Time: 5.20s\n",
      "(Best Epoch 252 | best H: 0.001035| best Pcv: 0.012364| val_loss : 0.002921)\n",
      "  H ç„¡æ”¹å–„ wait_H=1/150\n",
      "Epoch 254 | Learning Rate: 0.000156\n",
      "---\n",
      "alpha: 0.169\n",
      "Epoch 254, loss_H: 0.001041, loss_Pcv: 0.012169\n",
      "Epoch 254 | Train Loss: 0.003134 | Val Loss: 0.002925 | Time: 5.23s\n",
      "(Best Epoch 252 | best H: 0.001035| best Pcv: 0.012364| val_loss : 0.002925)\n",
      "  H ç„¡æ”¹å–„ wait_H=2/150\n",
      "Epoch 255 | Learning Rate: 0.000154\n",
      "---\n",
      "alpha: 0.170\n",
      "Epoch 255, loss_H: 0.001040, loss_Pcv: 0.012145\n",
      "Epoch 255 | Train Loss: 0.003359 | Val Loss: 0.002928 | Time: 5.03s\n",
      "(Best Epoch 252 | best H: 0.001035| best Pcv: 0.012364| val_loss : 0.002928)\n",
      "  H ç„¡æ”¹å–„ wait_H=3/150\n",
      "Epoch 256 | Learning Rate: 0.000153\n",
      "---\n",
      "alpha: 0.171\n",
      "Epoch 256, loss_H: 0.001040, loss_Pcv: 0.012110\n",
      "Epoch 256 | Train Loss: 0.003132 | Val Loss: 0.002930 | Time: 5.12s\n",
      "(Best Epoch 252 | best H: 0.001035| best Pcv: 0.012364| val_loss : 0.002930)\n",
      "  H ç„¡æ”¹å–„ wait_H=4/150\n",
      "Epoch 257 | Learning Rate: 0.000151\n",
      "---\n",
      "alpha: 0.171\n",
      "Epoch 257, loss_H: 0.001041, loss_Pcv: 0.012067\n",
      "Epoch 257 | Train Loss: 0.003152 | Val Loss: 0.002930 | Time: 5.23s\n",
      "(Best Epoch 252 | best H: 0.001035| best Pcv: 0.012364| val_loss : 0.002930)\n",
      "  H ç„¡æ”¹å–„ wait_H=5/150\n",
      "Epoch 258 | Learning Rate: 0.000150\n",
      "---\n",
      "alpha: 0.172\n",
      "Epoch 258, loss_H: 0.001043, loss_Pcv: 0.012033\n",
      "Epoch 258 | Train Loss: 0.003157 | Val Loss: 0.002933 | Time: 4.92s\n",
      "(Best Epoch 252 | best H: 0.001035| best Pcv: 0.012364| val_loss : 0.002933)\n",
      "  H ç„¡æ”¹å–„ wait_H=6/150\n",
      "Epoch 259 | Learning Rate: 0.000148\n",
      "---\n",
      "alpha: 0.173\n",
      "Epoch 259, loss_H: 0.001025, loss_Pcv: 0.012164\n",
      "Epoch 259 | Train Loss: 0.003372 | Val Loss: 0.002949 | Time: 5.18s\n",
      "(Best Epoch 252 | best H: 0.001035| best Pcv: 0.012364| val_loss : 0.002949)\n",
      "âœ… Save best H @ epoch 259\n",
      "Epoch 260 | Learning Rate: 0.000147\n",
      "---\n",
      "alpha: 0.173\n",
      "Epoch 260, loss_H: 0.001041, loss_Pcv: 0.011981\n",
      "Epoch 260 | Train Loss: 0.003413 | Val Loss: 0.002937 | Time: 5.29s\n",
      "(Best Epoch 259 | best H: 0.001025| best Pcv: 0.012164| val_loss : 0.002937)\n",
      "  H ç„¡æ”¹å–„ wait_H=1/150\n",
      "Epoch 261 | Learning Rate: 0.000145\n",
      "---\n",
      "alpha: 0.174\n",
      "Epoch 261, loss_H: 0.001023, loss_Pcv: 0.012083\n",
      "Epoch 261 | Train Loss: 0.003153 | Val Loss: 0.002947 | Time: 5.05s\n",
      "(Best Epoch 259 | best H: 0.001025| best Pcv: 0.012164| val_loss : 0.002947)\n",
      "âœ… Save best H @ epoch 261\n",
      "Epoch 262 | Learning Rate: 0.000144\n",
      "---\n",
      "alpha: 0.175\n",
      "Epoch 262, loss_H: 0.001034, loss_Pcv: 0.011970\n",
      "Epoch 262 | Train Loss: 0.003398 | Val Loss: 0.002944 | Time: 5.01s\n",
      "(Best Epoch 261 | best H: 0.001023| best Pcv: 0.012083| val_loss : 0.002944)\n",
      "  H ç„¡æ”¹å–„ wait_H=1/150\n",
      "Epoch 263 | Learning Rate: 0.000142\n",
      "---\n",
      "alpha: 0.175\n",
      "Epoch 263, loss_H: 0.001019, loss_Pcv: 0.012114\n",
      "Epoch 263 | Train Loss: 0.003156 | Val Loss: 0.002964 | Time: 4.89s\n",
      "(Best Epoch 261 | best H: 0.001023| best Pcv: 0.012083| val_loss : 0.002964)\n",
      "âœ… Save best H @ epoch 263\n",
      "Epoch 264 | Learning Rate: 0.000141\n",
      "---\n",
      "alpha: 0.176\n",
      "Epoch 264, loss_H: 0.001030, loss_Pcv: 0.011944\n",
      "Epoch 264 | Train Loss: 0.003409 | Val Loss: 0.002951 | Time: 5.04s\n",
      "(Best Epoch 263 | best H: 0.001019| best Pcv: 0.012114| val_loss : 0.002951)\n",
      "  H ç„¡æ”¹å–„ wait_H=1/150\n",
      "Epoch 265 | Learning Rate: 0.000139\n",
      "---\n",
      "alpha: 0.177\n",
      "Epoch 265, loss_H: 0.001028, loss_Pcv: 0.011941\n",
      "Epoch 265 | Train Loss: 0.003182 | Val Loss: 0.002956 | Time: 5.17s\n",
      "(Best Epoch 263 | best H: 0.001019| best Pcv: 0.012114| val_loss : 0.002956)\n",
      "  H ç„¡æ”¹å–„ wait_H=2/150\n",
      "Epoch 266 | Learning Rate: 0.000138\n",
      "---\n",
      "alpha: 0.177\n",
      "Epoch 266, loss_H: 0.001014, loss_Pcv: 0.012053\n",
      "Epoch 266 | Train Loss: 0.003197 | Val Loss: 0.002972 | Time: 4.96s\n",
      "(Best Epoch 263 | best H: 0.001019| best Pcv: 0.012114| val_loss : 0.002972)\n",
      "âœ… Save best H @ epoch 266\n",
      "Epoch 267 | Learning Rate: 0.000137\n",
      "---\n",
      "alpha: 0.178\n",
      "Epoch 267, loss_H: 0.001042, loss_Pcv: 0.011877\n",
      "Epoch 267 | Train Loss: 0.003208 | Val Loss: 0.002971 | Time: 4.93s\n",
      "(Best Epoch 266 | best H: 0.001014| best Pcv: 0.012053| val_loss : 0.002971)\n",
      "  H ç„¡æ”¹å–„ wait_H=1/150\n",
      "Epoch 268 | Learning Rate: 0.000135\n",
      "---\n",
      "alpha: 0.179\n",
      "Epoch 268, loss_H: 0.001004, loss_Pcv: 0.012186\n",
      "Epoch 268 | Train Loss: 0.003220 | Val Loss: 0.003002 | Time: 4.89s\n",
      "(Best Epoch 266 | best H: 0.001014| best Pcv: 0.012053| val_loss : 0.003002)\n",
      "âœ… Save best H @ epoch 268\n",
      "Epoch 269 | Learning Rate: 0.000134\n",
      "---\n",
      "alpha: 0.179\n",
      "Epoch 269, loss_H: 0.001025, loss_Pcv: 0.011832\n",
      "Epoch 269 | Train Loss: 0.003454 | Val Loss: 0.002963 | Time: 4.90s\n",
      "(Best Epoch 268 | best H: 0.001004| best Pcv: 0.012186| val_loss : 0.002963)\n",
      "  H ç„¡æ”¹å–„ wait_H=1/150\n",
      "Epoch 270 | Learning Rate: 0.000133\n",
      "---\n",
      "alpha: 0.180\n",
      "Epoch 270, loss_H: 0.001015, loss_Pcv: 0.011900\n",
      "Epoch 270 | Train Loss: 0.003679 | Val Loss: 0.002974 | Time: 5.01s\n",
      "(Best Epoch 268 | best H: 0.001004| best Pcv: 0.012186| val_loss : 0.002974)\n",
      "  H ç„¡æ”¹å–„ wait_H=2/150\n",
      "Epoch 271 | Learning Rate: 0.000131\n",
      "---\n",
      "alpha: 0.181\n",
      "Epoch 271, loss_H: 0.001025, loss_Pcv: 0.011792\n",
      "Epoch 271 | Train Loss: 0.003194 | Val Loss: 0.002970 | Time: 4.90s\n",
      "(Best Epoch 268 | best H: 0.001004| best Pcv: 0.012186| val_loss : 0.002970)\n",
      "  H ç„¡æ”¹å–„ wait_H=3/150\n",
      "Epoch 272 | Learning Rate: 0.000130\n",
      "---\n",
      "alpha: 0.181\n",
      "Epoch 272, loss_H: 0.001012, loss_Pcv: 0.011849\n",
      "Epoch 272 | Train Loss: 0.003200 | Val Loss: 0.002977 | Time: 5.34s\n",
      "(Best Epoch 268 | best H: 0.001004| best Pcv: 0.012186| val_loss : 0.002977)\n",
      "  H ç„¡æ”¹å–„ wait_H=4/150\n",
      "Epoch 273 | Learning Rate: 0.000129\n",
      "---\n",
      "alpha: 0.182\n",
      "Epoch 273, loss_H: 0.001008, loss_Pcv: 0.011854\n",
      "Epoch 273 | Train Loss: 0.003198 | Val Loss: 0.002982 | Time: 5.10s\n",
      "(Best Epoch 268 | best H: 0.001004| best Pcv: 0.012186| val_loss : 0.002982)\n",
      "  H ç„¡æ”¹å–„ wait_H=5/150\n",
      "Epoch 274 | Learning Rate: 0.000127\n",
      "---\n",
      "alpha: 0.183\n",
      "Epoch 274, loss_H: 0.001015, loss_Pcv: 0.011768\n",
      "Epoch 274 | Train Loss: 0.003462 | Val Loss: 0.002979 | Time: 4.88s\n",
      "(Best Epoch 268 | best H: 0.001004| best Pcv: 0.012186| val_loss : 0.002979)\n",
      "  H ç„¡æ”¹å–„ wait_H=6/150\n",
      "Epoch 275 | Learning Rate: 0.000126\n",
      "---\n",
      "alpha: 0.183\n",
      "Epoch 275, loss_H: 0.001002, loss_Pcv: 0.011868\n",
      "Epoch 275 | Train Loss: 0.003213 | Val Loss: 0.002994 | Time: 5.03s\n",
      "(Best Epoch 268 | best H: 0.001004| best Pcv: 0.012186| val_loss : 0.002994)\n",
      "âœ… Save best H @ epoch 275\n",
      "Epoch 276 | Learning Rate: 0.000125\n",
      "---\n",
      "alpha: 0.184\n",
      "Epoch 276, loss_H: 0.001009, loss_Pcv: 0.011754\n",
      "Epoch 276 | Train Loss: 0.003192 | Val Loss: 0.002986 | Time: 5.28s\n",
      "(Best Epoch 275 | best H: 0.001002| best Pcv: 0.011868| val_loss : 0.002986)\n",
      "  H ç„¡æ”¹å–„ wait_H=1/150\n",
      "Epoch 277 | Learning Rate: 0.000124\n",
      "---\n",
      "alpha: 0.185\n",
      "Epoch 277, loss_H: 0.001013, loss_Pcv: 0.011702\n",
      "Epoch 277 | Train Loss: 0.003215 | Val Loss: 0.002987 | Time: 4.92s\n",
      "(Best Epoch 275 | best H: 0.001002| best Pcv: 0.011868| val_loss : 0.002987)\n",
      "  H ç„¡æ”¹å–„ wait_H=2/150\n",
      "Epoch 278 | Learning Rate: 0.000122\n",
      "---\n",
      "alpha: 0.185\n",
      "Epoch 278, loss_H: 0.001003, loss_Pcv: 0.011783\n",
      "Epoch 278 | Train Loss: 0.003726 | Val Loss: 0.003000 | Time: 5.01s\n",
      "(Best Epoch 275 | best H: 0.001002| best Pcv: 0.011868| val_loss : 0.003000)\n",
      "  H ç„¡æ”¹å–„ wait_H=3/150\n",
      "Epoch 279 | Learning Rate: 0.000121\n",
      "---\n",
      "alpha: 0.186\n",
      "Epoch 279, loss_H: 0.001005, loss_Pcv: 0.011703\n",
      "Epoch 279 | Train Loss: 0.003486 | Val Loss: 0.002995 | Time: 4.93s\n",
      "(Best Epoch 275 | best H: 0.001002| best Pcv: 0.011868| val_loss : 0.002995)\n",
      "  H ç„¡æ”¹å–„ wait_H=4/150\n",
      "Epoch 280 | Learning Rate: 0.000120\n",
      "---\n",
      "alpha: 0.187\n",
      "Epoch 280, loss_H: 0.001000, loss_Pcv: 0.011703\n",
      "Epoch 280 | Train Loss: 0.003753 | Val Loss: 0.002998 | Time: 4.99s\n",
      "(Best Epoch 275 | best H: 0.001002| best Pcv: 0.011868| val_loss : 0.002998)\n",
      "âœ… Save best H @ epoch 280\n",
      "Epoch 281 | Learning Rate: 0.000119\n",
      "---\n",
      "alpha: 0.187\n",
      "Epoch 281, loss_H: 0.001007, loss_Pcv: 0.011650\n",
      "Epoch 281 | Train Loss: 0.003243 | Val Loss: 0.003000 | Time: 5.39s\n",
      "(Best Epoch 280 | best H: 0.001000| best Pcv: 0.011703| val_loss : 0.003000)\n",
      "  H ç„¡æ”¹å–„ wait_H=1/150\n",
      "Epoch 282 | Learning Rate: 0.000118\n",
      "---\n",
      "alpha: 0.188\n",
      "Epoch 282, loss_H: 0.000992, loss_Pcv: 0.011774\n",
      "Epoch 282 | Train Loss: 0.003258 | Val Loss: 0.003019 | Time: 5.64s\n",
      "(Best Epoch 280 | best H: 0.001000| best Pcv: 0.011703| val_loss : 0.003019)\n",
      "âœ… Save best H @ epoch 282\n",
      "Epoch 283 | Learning Rate: 0.000116\n",
      "---\n",
      "alpha: 0.189\n",
      "Epoch 283, loss_H: 0.001004, loss_Pcv: 0.011623\n",
      "Epoch 283 | Train Loss: 0.003254 | Val Loss: 0.003008 | Time: 5.54s\n",
      "(Best Epoch 282 | best H: 0.000992| best Pcv: 0.011774| val_loss : 0.003008)\n",
      "  H ç„¡æ”¹å–„ wait_H=1/150\n",
      "Epoch 284 | Learning Rate: 0.000115\n",
      "---\n",
      "alpha: 0.189\n",
      "Epoch 284, loss_H: 0.000993, loss_Pcv: 0.011694\n",
      "Epoch 284 | Train Loss: 0.003246 | Val Loss: 0.003019 | Time: 5.53s\n",
      "(Best Epoch 282 | best H: 0.000992| best Pcv: 0.011774| val_loss : 0.003019)\n",
      "  H ç„¡æ”¹å–„ wait_H=2/150\n",
      "Epoch 285 | Learning Rate: 0.000114\n",
      "---\n",
      "alpha: 0.190\n",
      "Epoch 285, loss_H: 0.000998, loss_Pcv: 0.011619\n",
      "Epoch 285 | Train Loss: 0.003508 | Val Loss: 0.003016 | Time: 5.07s\n",
      "(Best Epoch 282 | best H: 0.000992| best Pcv: 0.011774| val_loss : 0.003016)\n",
      "  H ç„¡æ”¹å–„ wait_H=3/150\n",
      "Epoch 286 | Learning Rate: 0.000113\n",
      "---\n",
      "alpha: 0.191\n",
      "Epoch 286, loss_H: 0.000997, loss_Pcv: 0.011609\n",
      "Epoch 286 | Train Loss: 0.003518 | Val Loss: 0.003020 | Time: 4.99s\n",
      "(Best Epoch 282 | best H: 0.000992| best Pcv: 0.011774| val_loss : 0.003020)\n",
      "  H ç„¡æ”¹å–„ wait_H=4/150\n",
      "Epoch 287 | Learning Rate: 0.000112\n",
      "---\n",
      "alpha: 0.191\n",
      "Epoch 287, loss_H: 0.000995, loss_Pcv: 0.011601\n",
      "Epoch 287 | Train Loss: 0.003257 | Val Loss: 0.003024 | Time: 5.21s\n",
      "(Best Epoch 282 | best H: 0.000992| best Pcv: 0.011774| val_loss : 0.003024)\n",
      "  H ç„¡æ”¹å–„ wait_H=5/150\n",
      "Epoch 288 | Learning Rate: 0.000111\n",
      "---\n",
      "alpha: 0.192\n",
      "Epoch 288, loss_H: 0.000996, loss_Pcv: 0.011578\n",
      "Epoch 288 | Train Loss: 0.003250 | Val Loss: 0.003028 | Time: 4.96s\n",
      "(Best Epoch 282 | best H: 0.000992| best Pcv: 0.011774| val_loss : 0.003028)\n",
      "  H ç„¡æ”¹å–„ wait_H=6/150\n",
      "Epoch 289 | Learning Rate: 0.000110\n",
      "---\n",
      "alpha: 0.193\n",
      "Epoch 289, loss_H: 0.000990, loss_Pcv: 0.011616\n",
      "Epoch 289 | Train Loss: 0.003256 | Val Loss: 0.003037 | Time: 4.95s\n",
      "(Best Epoch 282 | best H: 0.000992| best Pcv: 0.011774| val_loss : 0.003037)\n",
      "âœ… Save best H @ epoch 289\n",
      "Epoch 290 | Learning Rate: 0.000108\n",
      "---\n",
      "alpha: 0.193\n",
      "Epoch 290, loss_H: 0.000994, loss_Pcv: 0.011554\n",
      "Epoch 290 | Train Loss: 0.003254 | Val Loss: 0.003036 | Time: 4.93s\n",
      "(Best Epoch 289 | best H: 0.000990| best Pcv: 0.011616| val_loss : 0.003036)\n",
      "  H ç„¡æ”¹å–„ wait_H=1/150\n",
      "Epoch 291 | Learning Rate: 0.000107\n",
      "---\n",
      "alpha: 0.194\n",
      "Epoch 291, loss_H: 0.000998, loss_Pcv: 0.011503\n",
      "Epoch 291 | Train Loss: 0.003258 | Val Loss: 0.003036 | Time: 4.85s\n",
      "(Best Epoch 289 | best H: 0.000990| best Pcv: 0.011616| val_loss : 0.003036)\n",
      "  H ç„¡æ”¹å–„ wait_H=2/150\n",
      "Epoch 292 | Learning Rate: 0.000106\n",
      "---\n",
      "alpha: 0.195\n",
      "Epoch 292, loss_H: 0.000989, loss_Pcv: 0.011531\n",
      "Epoch 292 | Train Loss: 0.003809 | Val Loss: 0.003041 | Time: 4.93s\n",
      "(Best Epoch 289 | best H: 0.000990| best Pcv: 0.011616| val_loss : 0.003041)\n",
      "âœ… Save best H @ epoch 292\n",
      "Epoch 293 | Learning Rate: 0.000105\n",
      "---\n",
      "alpha: 0.195\n",
      "Epoch 293, loss_H: 0.000985, loss_Pcv: 0.011562\n",
      "Epoch 293 | Train Loss: 0.003542 | Val Loss: 0.003051 | Time: 5.05s\n",
      "(Best Epoch 292 | best H: 0.000989| best Pcv: 0.011531| val_loss : 0.003051)\n",
      "âœ… Save best H @ epoch 293\n",
      "Epoch 294 | Learning Rate: 0.000104\n",
      "---\n",
      "alpha: 0.196\n",
      "Epoch 294, loss_H: 0.000991, loss_Pcv: 0.011484\n",
      "Epoch 294 | Train Loss: 0.003552 | Val Loss: 0.003048 | Time: 4.98s\n",
      "(Best Epoch 293 | best H: 0.000985| best Pcv: 0.011562| val_loss : 0.003048)\n",
      "  H ç„¡æ”¹å–„ wait_H=1/150\n",
      "Epoch 295 | Learning Rate: 0.000103\n",
      "---\n",
      "alpha: 0.197\n",
      "Epoch 295, loss_H: 0.000981, loss_Pcv: 0.011538\n",
      "Epoch 295 | Train Loss: 0.003278 | Val Loss: 0.003057 | Time: 5.12s\n",
      "(Best Epoch 293 | best H: 0.000985| best Pcv: 0.011562| val_loss : 0.003057)\n",
      "âœ… Save best H @ epoch 295\n",
      "Epoch 296 | Learning Rate: 0.000102\n",
      "---\n",
      "alpha: 0.197\n",
      "Epoch 296, loss_H: 0.000989, loss_Pcv: 0.011440\n",
      "Epoch 296 | Train Loss: 0.003285 | Val Loss: 0.003051 | Time: 5.00s\n",
      "(Best Epoch 295 | best H: 0.000981| best Pcv: 0.011538| val_loss : 0.003051)\n",
      "  H ç„¡æ”¹å–„ wait_H=1/150\n",
      "Epoch 297 | Learning Rate: 0.000101\n",
      "---\n",
      "alpha: 0.198\n",
      "Epoch 297, loss_H: 0.000979, loss_Pcv: 0.011511\n",
      "Epoch 297 | Train Loss: 0.003561 | Val Loss: 0.003064 | Time: 5.02s\n",
      "(Best Epoch 295 | best H: 0.000981| best Pcv: 0.011538| val_loss : 0.003064)\n",
      "âœ… Save best H @ epoch 297\n",
      "Epoch 298 | Learning Rate: 0.000100\n",
      "---\n",
      "alpha: 0.199\n",
      "Epoch 298, loss_H: 0.000986, loss_Pcv: 0.011421\n",
      "Epoch 298 | Train Loss: 0.003562 | Val Loss: 0.003059 | Time: 5.26s\n",
      "(Best Epoch 297 | best H: 0.000979| best Pcv: 0.011511| val_loss : 0.003059)\n",
      "  H ç„¡æ”¹å–„ wait_H=1/150\n",
      "Epoch 299 | Learning Rate: 0.000099\n",
      "---\n",
      "alpha: 0.199\n",
      "Epoch 299, loss_H: 0.000981, loss_Pcv: 0.011430\n",
      "Epoch 299 | Train Loss: 0.003295 | Val Loss: 0.003064 | Time: 5.04s\n",
      "(Best Epoch 297 | best H: 0.000979| best Pcv: 0.011511| val_loss : 0.003064)\n",
      "  H ç„¡æ”¹å–„ wait_H=2/150\n",
      "Epoch 300 | Learning Rate: 0.000098\n",
      "---\n",
      "alpha: 0.200\n",
      "Epoch 300, loss_H: 0.000978, loss_Pcv: 0.011426\n",
      "Epoch 300 | Train Loss: 0.003316 | Val Loss: 0.003068 | Time: 5.05s\n",
      "(Best Epoch 297 | best H: 0.000979| best Pcv: 0.011511| val_loss : 0.003068)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  H ç„¡æ”¹å–„ wait_H=3/150\n",
      "Epoch 301 | Learning Rate: 0.000097\n",
      "---\n",
      "alpha: 0.201\n",
      "Epoch 301, loss_H: 0.000978, loss_Pcv: 0.011406\n",
      "Epoch 301 | Train Loss: 0.003307 | Val Loss: 0.003071 | Time: 4.96s\n",
      "(Best Epoch 297 | best H: 0.000979| best Pcv: 0.011511| val_loss : 0.003071)\n",
      "  H ç„¡æ”¹å–„ wait_H=4/150\n",
      "Epoch 302 | Learning Rate: 0.000096\n",
      "---\n",
      "alpha: 0.201\n",
      "Epoch 302, loss_H: 0.000974, loss_Pcv: 0.011444\n",
      "Epoch 302 | Train Loss: 0.003329 | Val Loss: 0.003082 | Time: 5.01s\n",
      "(Best Epoch 297 | best H: 0.000979| best Pcv: 0.011511| val_loss : 0.003082)\n",
      "âœ… Save best H @ epoch 302\n",
      "Epoch 303 | Learning Rate: 0.000095\n",
      "---\n",
      "alpha: 0.202\n",
      "Epoch 303, loss_H: 0.000979, loss_Pcv: 0.011376\n",
      "Epoch 303 | Train Loss: 0.003319 | Val Loss: 0.003080 | Time: 5.12s\n",
      "(Best Epoch 302 | best H: 0.000974| best Pcv: 0.011444| val_loss : 0.003080)\n",
      "  H ç„¡æ”¹å–„ wait_H=1/150\n",
      "Epoch 304 | Learning Rate: 0.000094\n",
      "---\n",
      "alpha: 0.203\n",
      "Epoch 304, loss_H: 0.000986, loss_Pcv: 0.011323\n",
      "Epoch 304 | Train Loss: 0.003595 | Val Loss: 0.003081 | Time: 4.95s\n",
      "(Best Epoch 302 | best H: 0.000974| best Pcv: 0.011444| val_loss : 0.003081)\n",
      "  H ç„¡æ”¹å–„ wait_H=2/150\n",
      "Epoch 305 | Learning Rate: 0.000093\n",
      "---\n",
      "alpha: 0.203\n",
      "Epoch 305, loss_H: 0.000969, loss_Pcv: 0.011440\n",
      "Epoch 305 | Train Loss: 0.003320 | Val Loss: 0.003098 | Time: 5.08s\n",
      "(Best Epoch 302 | best H: 0.000974| best Pcv: 0.011444| val_loss : 0.003098)\n",
      "âœ… Save best H @ epoch 305\n",
      "Epoch 306 | Learning Rate: 0.000092\n",
      "---\n",
      "alpha: 0.204\n",
      "Epoch 306, loss_H: 0.000986, loss_Pcv: 0.011292\n",
      "Epoch 306 | Train Loss: 0.003323 | Val Loss: 0.003089 | Time: 5.25s\n",
      "(Best Epoch 305 | best H: 0.000969| best Pcv: 0.011440| val_loss : 0.003089)\n",
      "  H ç„¡æ”¹å–„ wait_H=1/150\n",
      "Epoch 307 | Learning Rate: 0.000091\n",
      "---\n",
      "alpha: 0.205\n",
      "Epoch 307, loss_H: 0.000966, loss_Pcv: 0.011465\n",
      "Epoch 307 | Train Loss: 0.003323 | Val Loss: 0.003114 | Time: 5.26s\n",
      "(Best Epoch 305 | best H: 0.000969| best Pcv: 0.011440| val_loss : 0.003114)\n",
      "âœ… Save best H @ epoch 307\n",
      "Epoch 308 | Learning Rate: 0.000091\n",
      "---\n",
      "alpha: 0.205\n",
      "Epoch 308, loss_H: 0.000984, loss_Pcv: 0.011273\n",
      "Epoch 308 | Train Loss: 0.003333 | Val Loss: 0.003097 | Time: 4.99s\n",
      "(Best Epoch 307 | best H: 0.000966| best Pcv: 0.011465| val_loss : 0.003097)\n",
      "  H ç„¡æ”¹å–„ wait_H=1/150\n",
      "Epoch 309 | Learning Rate: 0.000090\n",
      "---\n",
      "alpha: 0.206\n",
      "Epoch 309, loss_H: 0.000974, loss_Pcv: 0.011302\n",
      "Epoch 309 | Train Loss: 0.003350 | Val Loss: 0.003102 | Time: 5.12s\n",
      "(Best Epoch 307 | best H: 0.000966| best Pcv: 0.011465| val_loss : 0.003102)\n",
      "  H ç„¡æ”¹å–„ wait_H=2/150\n",
      "Epoch 310 | Learning Rate: 0.000089\n",
      "---\n",
      "alpha: 0.207\n",
      "Epoch 310, loss_H: 0.000975, loss_Pcv: 0.011268\n",
      "Epoch 310 | Train Loss: 0.003357 | Val Loss: 0.003102 | Time: 5.06s\n",
      "(Best Epoch 307 | best H: 0.000966| best Pcv: 0.011465| val_loss : 0.003102)\n",
      "  H ç„¡æ”¹å–„ wait_H=3/150\n",
      "Epoch 311 | Learning Rate: 0.000088\n",
      "---\n",
      "alpha: 0.207\n",
      "Epoch 311, loss_H: 0.000972, loss_Pcv: 0.011278\n",
      "Epoch 311 | Train Loss: 0.003359 | Val Loss: 0.003108 | Time: 5.00s\n",
      "(Best Epoch 307 | best H: 0.000966| best Pcv: 0.011465| val_loss : 0.003108)\n",
      "  H ç„¡æ”¹å–„ wait_H=4/150\n",
      "Epoch 312 | Learning Rate: 0.000087\n",
      "---\n",
      "alpha: 0.208\n",
      "Epoch 312, loss_H: 0.000973, loss_Pcv: 0.011253\n",
      "Epoch 312 | Train Loss: 0.003654 | Val Loss: 0.003112 | Time: 5.24s\n",
      "(Best Epoch 307 | best H: 0.000966| best Pcv: 0.011465| val_loss : 0.003112)\n",
      "  H ç„¡æ”¹å–„ wait_H=5/150\n",
      "Epoch 313 | Learning Rate: 0.000086\n",
      "---\n",
      "alpha: 0.209\n",
      "Epoch 313, loss_H: 0.000969, loss_Pcv: 0.011284\n",
      "Epoch 313 | Train Loss: 0.003342 | Val Loss: 0.003121 | Time: 5.00s\n",
      "(Best Epoch 307 | best H: 0.000966| best Pcv: 0.011465| val_loss : 0.003121)\n",
      "  H ç„¡æ”¹å–„ wait_H=6/150\n",
      "Epoch 314 | Learning Rate: 0.000085\n",
      "---\n",
      "alpha: 0.209\n",
      "Epoch 314, loss_H: 0.000971, loss_Pcv: 0.011241\n",
      "Epoch 314 | Train Loss: 0.003648 | Val Loss: 0.003121 | Time: 4.82s\n",
      "(Best Epoch 307 | best H: 0.000966| best Pcv: 0.011465| val_loss : 0.003121)\n",
      "  H ç„¡æ”¹å–„ wait_H=7/150\n",
      "Epoch 315 | Learning Rate: 0.000084\n",
      "---\n",
      "alpha: 0.210\n",
      "Epoch 315, loss_H: 0.000970, loss_Pcv: 0.011227\n",
      "Epoch 315 | Train Loss: 0.003361 | Val Loss: 0.003124 | Time: 4.93s\n",
      "(Best Epoch 307 | best H: 0.000966| best Pcv: 0.011465| val_loss : 0.003124)\n",
      "  H ç„¡æ”¹å–„ wait_H=8/150\n",
      "Epoch 316 | Learning Rate: 0.000084\n",
      "---\n",
      "alpha: 0.211\n",
      "Epoch 316, loss_H: 0.000965, loss_Pcv: 0.011251\n",
      "Epoch 316 | Train Loss: 0.003362 | Val Loss: 0.003132 | Time: 5.07s\n",
      "(Best Epoch 307 | best H: 0.000966| best Pcv: 0.011465| val_loss : 0.003132)\n",
      "âœ… Save best H @ epoch 316\n",
      "Epoch 317 | Learning Rate: 0.000083\n",
      "---\n",
      "alpha: 0.211\n",
      "Epoch 317, loss_H: 0.000968, loss_Pcv: 0.011200\n",
      "Epoch 317 | Train Loss: 0.003371 | Val Loss: 0.003131 | Time: 5.17s\n",
      "(Best Epoch 316 | best H: 0.000965| best Pcv: 0.011251| val_loss : 0.003131)\n",
      "  H ç„¡æ”¹å–„ wait_H=1/150\n",
      "Epoch 318 | Learning Rate: 0.000082\n",
      "---\n",
      "alpha: 0.212\n",
      "Epoch 318, loss_H: 0.000964, loss_Pcv: 0.011214\n",
      "Epoch 318 | Train Loss: 0.003376 | Val Loss: 0.003137 | Time: 5.19s\n",
      "(Best Epoch 316 | best H: 0.000965| best Pcv: 0.011251| val_loss : 0.003137)\n",
      "  H ç„¡æ”¹å–„ wait_H=2/150\n",
      "Epoch 319 | Learning Rate: 0.000081\n",
      "---\n",
      "alpha: 0.213\n",
      "Epoch 319, loss_H: 0.000971, loss_Pcv: 0.011150\n",
      "Epoch 319 | Train Loss: 0.003668 | Val Loss: 0.003136 | Time: 4.99s\n",
      "(Best Epoch 316 | best H: 0.000965| best Pcv: 0.011251| val_loss : 0.003136)\n",
      "  H ç„¡æ”¹å–„ wait_H=3/150\n",
      "Epoch 320 | Learning Rate: 0.000080\n",
      "---\n",
      "alpha: 0.213\n",
      "Epoch 320, loss_H: 0.000964, loss_Pcv: 0.011180\n",
      "Epoch 320 | Train Loss: 0.003681 | Val Loss: 0.003143 | Time: 5.03s\n",
      "(Best Epoch 316 | best H: 0.000965| best Pcv: 0.011251| val_loss : 0.003143)\n",
      "  H ç„¡æ”¹å–„ wait_H=4/150\n",
      "Epoch 321 | Learning Rate: 0.000079\n",
      "---\n",
      "alpha: 0.214\n",
      "Epoch 321, loss_H: 0.000967, loss_Pcv: 0.011141\n",
      "Epoch 321 | Train Loss: 0.003692 | Val Loss: 0.003144 | Time: 5.10s\n",
      "(Best Epoch 316 | best H: 0.000965| best Pcv: 0.011251| val_loss : 0.003144)\n",
      "  H ç„¡æ”¹å–„ wait_H=5/150\n",
      "Epoch 322 | Learning Rate: 0.000079\n",
      "---\n",
      "alpha: 0.215\n",
      "Epoch 322, loss_H: 0.000956, loss_Pcv: 0.011228\n",
      "Epoch 322 | Train Loss: 0.003395 | Val Loss: 0.003161 | Time: 5.11s\n",
      "(Best Epoch 316 | best H: 0.000965| best Pcv: 0.011251| val_loss : 0.003161)\n",
      "âœ… Save best H @ epoch 322\n",
      "Epoch 323 | Learning Rate: 0.000078\n",
      "---\n",
      "alpha: 0.215\n",
      "Epoch 323, loss_H: 0.000966, loss_Pcv: 0.011124\n",
      "Epoch 323 | Train Loss: 0.003705 | Val Loss: 0.003153 | Time: 5.04s\n",
      "(Best Epoch 322 | best H: 0.000956| best Pcv: 0.011228| val_loss : 0.003153)\n",
      "  H ç„¡æ”¹å–„ wait_H=1/150\n",
      "Epoch 324 | Learning Rate: 0.000077\n",
      "---\n",
      "alpha: 0.216\n",
      "Epoch 324, loss_H: 0.000957, loss_Pcv: 0.011192\n",
      "Epoch 324 | Train Loss: 0.003699 | Val Loss: 0.003167 | Time: 5.00s\n",
      "(Best Epoch 322 | best H: 0.000956| best Pcv: 0.011228| val_loss : 0.003167)\n",
      "  H ç„¡æ”¹å–„ wait_H=2/150\n",
      "Epoch 325 | Learning Rate: 0.000076\n",
      "---\n",
      "alpha: 0.217\n",
      "Epoch 325, loss_H: 0.000963, loss_Pcv: 0.011118\n",
      "Epoch 325 | Train Loss: 0.003395 | Val Loss: 0.003163 | Time: 4.93s\n",
      "(Best Epoch 322 | best H: 0.000956| best Pcv: 0.011228| val_loss : 0.003163)\n",
      "  H ç„¡æ”¹å–„ wait_H=3/150\n",
      "Epoch 326 | Learning Rate: 0.000076\n",
      "---\n",
      "alpha: 0.217\n",
      "Epoch 326, loss_H: 0.000962, loss_Pcv: 0.011104\n",
      "Epoch 326 | Train Loss: 0.003419 | Val Loss: 0.003166 | Time: 5.04s\n",
      "(Best Epoch 322 | best H: 0.000956| best Pcv: 0.011228| val_loss : 0.003166)\n",
      "  H ç„¡æ”¹å–„ wait_H=4/150\n",
      "Epoch 327 | Learning Rate: 0.000075\n",
      "---\n",
      "alpha: 0.218\n",
      "Epoch 327, loss_H: 0.000958, loss_Pcv: 0.011120\n",
      "Epoch 327 | Train Loss: 0.003416 | Val Loss: 0.003173 | Time: 5.04s\n",
      "(Best Epoch 322 | best H: 0.000956| best Pcv: 0.011228| val_loss : 0.003173)\n",
      "  H ç„¡æ”¹å–„ wait_H=5/150\n",
      "Epoch 328 | Learning Rate: 0.000074\n",
      "---\n",
      "alpha: 0.219\n",
      "Epoch 328, loss_H: 0.000958, loss_Pcv: 0.011098\n",
      "Epoch 328 | Train Loss: 0.004026 | Val Loss: 0.003175 | Time: 5.22s\n",
      "(Best Epoch 322 | best H: 0.000956| best Pcv: 0.011228| val_loss : 0.003175)\n",
      "  H ç„¡æ”¹å–„ wait_H=6/150\n",
      "Epoch 329 | Learning Rate: 0.000073\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_cu128_pre",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
