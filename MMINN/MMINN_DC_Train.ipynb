{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Network Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step0: Import Package & Hyperparameter Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 清空所有變數\n",
    "%reset -f\n",
    "# # 強制 Python 回收記憶體\n",
    "# import gc\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Package\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook 環境，跳過切換目錄\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from datetime import datetime\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    os.chdir(os.path.dirname(os.path.abspath(__file__)))\n",
    "except NameError:\n",
    "    print(\"Notebook 環境，跳過切換目錄\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Unified Hyperparameter Configuration\n",
    "class Config:\n",
    "    SEED = 1\n",
    "    NUM_EPOCHS = 1500\n",
    "    BATCH_SIZE = 64\n",
    "    LEARNING_RATE = 0.01  #論文提供\n",
    "    LR_SCHEDULER_GAMMA = 0.9545861499908413  #論文提供\n",
    "    DECAY_EPOCH = 455\n",
    "    # DECAY_RATIO = 0.5\n",
    "    EARLY_STOPPING_PATIENCE = 150\n",
    "    HIDDEN_SIZE = 40\n",
    "    OPERATOR_SIZE = 30\n",
    "    MAXOUT_H = 1\n",
    "\n",
    "\n",
    "# Reproducibility\n",
    "random.seed(Config.SEED)\n",
    "np.random.seed(Config.SEED)\n",
    "torch.manual_seed(Config.SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Material & Number of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "material = \"CH467160\"\n",
    "fix_way = \"uesed_for_PFC_test4\"\n",
    "note = \"optuna_parm_test1\"\n",
    "note_detail = \"使用optuna的參數\"\n",
    "downsample = 1024\n",
    "save_figure = True\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d\")\n",
    "\n",
    "# 訓練情況況\n",
    "plot_interval = 150\n",
    "train_show_sample = 1\n",
    "\n",
    "result_dir = os.path.join(\"results\",\n",
    "                          f\"{timestamp}_{fix_way}_{material}_{note}\")\n",
    "os.makedirs(result_dir, exist_ok=True)\n",
    "\n",
    "# 定義保存模型的路徑\n",
    "model_save_dir = result_dir\n",
    "model_save_path = os.path.join(\n",
    "    model_save_dir, f\"{material}_{fix_way}_{note}_{timestamp}.pt\")  # 定義模型保存檔名\n",
    "\n",
    "figure_save_base_path = result_dir\n",
    "\n",
    "# Select device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1: Data processing and data loader generate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Preprocess data into a data loader\n",
    "def get_dataloader(data_B,\n",
    "                   data_F,\n",
    "                   data_T,\n",
    "                   data_H,\n",
    "                   data_N,\n",
    "                   data_Hdc,\n",
    "                   data_Duty_P,\n",
    "                   data_Duty_N,\n",
    "                   data_Pcv,\n",
    "                   global_B_max,\n",
    "                   global_H_max,\n",
    "                   n_init=16):\n",
    "\n",
    "    # Data pre-process\n",
    "\n",
    "    # ── 0. 全域設定/降階設定 ──────────────────────────────\n",
    "    eps = 1e-8  # 防止除以 0\n",
    "    if downsample == 1024:\n",
    "        seq_length = 1024  # 單筆波形點數 (不再 down-sample)\n",
    "    else:\n",
    "        seq_length = downsample\n",
    "        cols = np.linspace(0, 1023, seq_length, dtype=int)\n",
    "        data_B = data_B[:, cols]\n",
    "        data_H = data_H[:, cols]\n",
    "\n",
    "    # ── 1. 波形拼接 (補 n_init 點作初始磁化) ────\n",
    "    data_length = seq_length + n_init\n",
    "    data_B = np.hstack((data_B[:, -n_init:], data_B))  # (batch, data_length)\n",
    "    data_H = np.hstack((data_H[:, -n_init:], data_H))\n",
    "\n",
    "    print(\"B shape:\", data_B.shape)\n",
    "    print(\"H shape:\", data_H.shape)\n",
    "    print(\"F shape:\", data_F.shape)\n",
    "    print(\"T shape:\", data_T.shape)\n",
    "    print(\"Hdc shape:\", data_Hdc.shape)\n",
    "    print(\"N shape:\", data_N.shape)\n",
    "    print(\"Duty Pos shape:\", data_Duty_P.shape)\n",
    "    print(\"Duty Neg shape:\", data_Duty_N.shape)\n",
    "    print(\"Pcv shape:\", data_Pcv.shape)\n",
    "\n",
    "    # ── 2. 轉成 Tensor ───────────────────────────\n",
    "    B = torch.from_numpy(data_B).view(-1, data_length, 1).float()  # (B,N,1)\n",
    "    H = torch.from_numpy(data_H).view(-1, data_length, 1).float()\n",
    "    F = torch.log10(torch.from_numpy(data_F).view(-1, 1).float())  # 純量\n",
    "    T = torch.from_numpy(data_T).view(-1, 1).float()\n",
    "    Hdc = torch.from_numpy(data_Hdc).view(-1, 1).float()\n",
    "    N = torch.from_numpy(data_N).view(-1, 1).float()\n",
    "    Duty_P = torch.from_numpy(data_Duty_P).view(-1, 1).float()\n",
    "    Duty_N = torch.from_numpy(data_Duty_N).view(-1, 1).float()\n",
    "    Pcv = torch.log10(torch.from_numpy(data_Pcv).view(-1, 1).float())\n",
    "\n",
    "    # ── 3. 每筆樣本各自找最大幅值 (per-profile scale) ─\n",
    "    # scale_B = torch.max(torch.abs(B), dim=1,\n",
    "    #                     keepdim=True).values + eps  # (B,1,1)\n",
    "    # scale_H = torch.max(torch.abs(H), dim=1, keepdim=True).values + eps\n",
    "\n",
    "    # ── 4. 先計算導數，再除以 scale_B ─────────────\n",
    "    dB = torch.diff(B, dim=1, prepend=B[:, :1])\n",
    "    dB_dt = dB * (seq_length * F.view(-1, 1, 1))  # 真實斜率\n",
    "    # d2B = torch.diff(dB, dim=1, prepend=dB[:, :1])\n",
    "    # d2B_dt = d2B * (seq_length * F.view(-1, 1, 1))\n",
    "\n",
    "    # ── 5. 形成模型輸入 (已經縮放到 [-1,1]) ────────\n",
    "    # in_B = B / scale_B\n",
    "    # out_H = H / scale_H  # 預測目標\n",
    "    # in_dB_dt = dB_dt / scale_B\n",
    "    # 後續發現d2B無改善準確度(可能要多波形種類才有效幫助)，先以輸入0代入\n",
    "    # in_d2B_dt = d2B_dt / scale_B\n",
    "\n",
    "    # *修正成使用全域最大幅值 (ver.250806)\n",
    "    in_B = B / global_B_max\n",
    "    out_H = H / global_H_max\n",
    "    in_dB_dt = dB_dt / global_B_max\n",
    "    in_d2B_dt = torch.zeros_like(in_dB_dt)\n",
    "\n",
    "    # ── 6. 純量特徵：計算 z-score 參數 ─────────────\n",
    "    def safe_mean_std(tensor, eps=1e-8):\n",
    "        m = torch.mean(tensor).item()\n",
    "        s = torch.std(tensor).item()\n",
    "        return [m, 1.0 if s < eps else s]\n",
    "\n",
    "    #  Compute normalization parameters (均值 & 標準差)**\n",
    "    norm = [\n",
    "        safe_mean_std(F),\n",
    "        safe_mean_std(T),\n",
    "        safe_mean_std(Hdc),\n",
    "        safe_mean_std(N),\n",
    "        safe_mean_std(Pcv)\n",
    "    ]\n",
    "\n",
    "    # 用來做test固定標準化參數的\n",
    "    print(\"0.F, 1.T, 2.Hdc, 3.N, 4.Pcv\")\n",
    "    material_name = f\"{material}\"\n",
    "    print(f'\"{material_name}\": [')\n",
    "    for param in norm:\n",
    "        print(f\"    {param},\")\n",
    "    print(\"]\")\n",
    "\n",
    "    # Data Normalization\n",
    "    in_F = (F - norm[0][0]) / norm[0][1]  # F\n",
    "    in_T = (T - norm[1][0]) / norm[1][1]  # T\n",
    "    in_Hdc = (Hdc - norm[2][0]) / norm[2][1]  # Hdc\n",
    "    in_N = (N - norm[3][0]) / norm[3][1]  # N\n",
    "    in_Pcv = (Pcv - norm[4][0]) / norm[4][1]  # Pcv\n",
    "    in_Duty_P = Duty_P  # Duty Pos\n",
    "    in_Duty_N = Duty_N  # Duty Neg\n",
    "\n",
    "    # #   → 方便推論復原，保留 scale_B, scale_H 當作額外純量\n",
    "    # aux_features = torch.cat(\n",
    "    #     (in_F, in_T, in_Hdc, in_N, in_Duty_P, in_Duty_N, in_Pcv,\n",
    "    #      scale_B.squeeze(-1), scale_H.squeeze(-1)),\n",
    "    #     dim=1)\n",
    "\n",
    "    # ── 7. 產生初始 Preisach operator 狀態 s0 ──────\n",
    "    max_B, _ = torch.max(in_B, dim=1)\n",
    "    min_B, _ = torch.min(in_B, dim=1)\n",
    "    # s0 = get_operator_init(in_B[:, 0] - dB[:, 0] / scale_B.squeeze(-1),\n",
    "    #                        dB / scale_B, max_B, min_B)\n",
    "\n",
    "    s0 = get_operator_init(in_B[:, 0] - dB[:, 0] / global_B_max,\n",
    "                           dB / global_B_max, max_B, min_B)\n",
    "\n",
    "    # ── 8. 組合 Dataset ───────────────────────────\n",
    "    # wave_inputs = torch.cat(\n",
    "    #     (\n",
    "    #         in_B,  # ① B\n",
    "    #         dB / scale_B,  # ② ΔB\n",
    "    #         in_dB_dt,  # ③ dB/dt\n",
    "    #         in_d2B_dt),\n",
    "    #     dim=2)  # ④ d²B/dt²   → (B,L,4)\n",
    "\n",
    "    # amps = torch.cat((scale_B.squeeze(-1), scale_H.squeeze(-1)),\n",
    "    #                 dim=1)  # (B,2)\n",
    "\n",
    "    wave_inputs = torch.cat(\n",
    "        (\n",
    "            in_B,  # ① B\n",
    "            dB / global_B_max,  # ② ΔB\n",
    "            in_dB_dt,  # ③ dB/dt\n",
    "            in_d2B_dt),\n",
    "        dim=2)  # ④ d²B/dt²   → (B,L,4)\n",
    "\n",
    "    aux_features = torch.cat((in_F, in_T, in_Hdc, in_N, in_Duty_P, in_Duty_N),\n",
    "                             dim=1)  # (B,4)\n",
    "\n",
    "    amp_B = torch.full((len(B), 1), global_B_max, dtype=torch.float32)\n",
    "    amp_H = torch.full((len(B), 1), global_H_max, dtype=torch.float32)\n",
    "    amps = torch.cat((amp_B, amp_H), dim=1)  # 仍給 RNN2 用\n",
    "\n",
    "    # 這裡把 Pcv（已 z-score）單獨拿出來當另一個 label\n",
    "    target_Pcv = in_Pcv  # (B,1)\n",
    "\n",
    "    full_dataset = torch.utils.data.TensorDataset(\n",
    "        wave_inputs,  # 0  → 模型序列輸入\n",
    "        aux_features,  # 1  → 4 個純量\n",
    "        amps,  # 2  → 幅值係數\n",
    "        s0,  # 3  → Preisach 初始狀態\n",
    "        out_H,  # 4  → 目標 H  (已 scale_H)\n",
    "        target_Pcv)  # 5  → 目標 Pcv (已 z-score)\n",
    "\n",
    "    # ── 9. Train / Valid split & DataLoader ───────\n",
    "    train_size = int(0.8 * len(full_dataset))\n",
    "    valid_size = len(full_dataset) - train_size\n",
    "    train_set, valid_set = torch.utils.data.random_split(\n",
    "        full_dataset, [train_size, valid_size],\n",
    "        generator=torch.Generator().manual_seed(Config.SEED))\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_set,\n",
    "                                               batch_size=Config.BATCH_SIZE,\n",
    "                                               shuffle=True,\n",
    "                                               num_workers=0,\n",
    "                                               pin_memory=True,\n",
    "                                               collate_fn=filter_input)\n",
    "\n",
    "    valid_loader = torch.utils.data.DataLoader(valid_set,\n",
    "                                               batch_size=Config.BATCH_SIZE,\n",
    "                                               shuffle=False,\n",
    "                                               num_workers=0,\n",
    "                                               pin_memory=True,\n",
    "                                               collate_fn=filter_input)\n",
    "\n",
    "    return train_loader, valid_loader, norm\n",
    "\n",
    "\n",
    "# %% Predict the operator state at t0\n",
    "def get_operator_init(B1,\n",
    "                      dB,\n",
    "                      Bmax,\n",
    "                      Bmin,\n",
    "                      max_out_H=Config.MAXOUT_H,\n",
    "                      operator_size=Config.OPERATOR_SIZE):\n",
    "    \"\"\"Compute the initial state of hysteresis operators\"\"\"\n",
    "    s0 = torch.zeros((dB.shape[0], operator_size))\n",
    "    operator_thre = torch.from_numpy(\n",
    "        np.linspace(max_out_H / operator_size, max_out_H,\n",
    "                    operator_size)).view(1, -1)\n",
    "\n",
    "    for i in range(dB.shape[0]):\n",
    "        for j in range(operator_size):\n",
    "            r = operator_thre[0, j]\n",
    "            if (Bmax[i] >= r) or (Bmin[i] <= -r):\n",
    "                if dB[i, 0] >= 0:\n",
    "                    if B1[i] > Bmin[i] + 2 * r:\n",
    "                        s0[i, j] = r\n",
    "                    else:\n",
    "                        s0[i, j] = B1[i] - (r + Bmin[i])\n",
    "                else:\n",
    "                    if B1[i] < Bmax[i] - 2 * r:\n",
    "                        s0[i, j] = -r\n",
    "                    else:\n",
    "                        s0[i, j] = B1[i] + (r - Bmax[i])\n",
    "    return s0\n",
    "\n",
    "\n",
    "def filter_input(batch):\n",
    "    inputs, features, amps, s0, target_H, target_Pcv = zip(*batch)\n",
    "\n",
    "    inputs = torch.stack(inputs)\n",
    "    features = torch.stack(features)\n",
    "    amps = torch.stack(amps)\n",
    "    s0 = torch.stack(s0)\n",
    "    target_H = torch.stack(target_H)[:, -downsample:, :]  # 保留全長\n",
    "    target_Pcv = torch.stack(target_Pcv)  # (B,1)\n",
    "\n",
    "    return inputs, features, amps, s0, target_H, target_Pcv\n",
    "\n",
    "\n",
    "# 溫度頻率不變加入微小的 epsilon\n",
    "def safe_mean_std(tensor, eps=1e-8):\n",
    "    m_tensor = torch.mean(tensor)  # 還是 Tensor\n",
    "    s_tensor = torch.std(tensor)  # 還是 Tensor\n",
    "\n",
    "    m_val = m_tensor.item()  # 第一次轉成 float\n",
    "    s_val = s_tensor.item()\n",
    "    if s_val < eps:\n",
    "        s_val = 1.0\n",
    "    return [m_val, s_val]  # 直接回傳 float\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2: Define Network Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Magnetization mechansim-determined neural network\n",
    "\"\"\"\n",
    "    Parameters:\n",
    "    - hidden_size: number of eddy current slices (RNN neuron)\n",
    "    - operator_size: number of operators\n",
    "    - input_size: number of inputs (1.B 2.dB 3.dB/dt 4.d2B/dt)\n",
    "    - var_size: number of supplenmentary variables (1.F 2.T 3.Hdc 4.N 5.Duty_P 6.Duty_N)        \n",
    "    - output_size: number of outputs (1.H)\n",
    "    \n",
    "    只先把d2B/dt考量在EddyCell裡面\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class MMINet(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            norm,\n",
    "            hidden_size=Config.HIDDEN_SIZE,\n",
    "            operator_size=Config.OPERATOR_SIZE,\n",
    "            input_size=4,  # Add d2B(250203)\n",
    "            var_size=6,\n",
    "            output_size=1):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.var_size = var_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.operator_size = operator_size\n",
    "        self.norm = norm  #*這裡改成從外部傳入 norm(250203)\n",
    "\n",
    "        self.rnn1 = StopOperatorCell(self.operator_size)\n",
    "        self.dnn1 = nn.Linear(self.operator_size + self.var_size, 1)\n",
    "        # var_size (F T Hdc N Duty_P Duty_N ) + 3 (B, dB/dt, d2B/dt)\n",
    "        self.rnn2 = EddyCell(var_size + 3, self.hidden_size, output_size)\n",
    "        self.dnn2 = nn.Linear(self.hidden_size, 1)\n",
    "        self.rnn2_hx = None\n",
    "        # var_size=6: 1.F 2.T 3.Hdc 4.N 5.Duty_P 6.Duty_N + 1 for P_prelim\n",
    "        self.loss_mlp = nn.Sequential(nn.Linear(self.var_size + 1, 128),\n",
    "                                      nn.ReLU(), nn.Linear(128, 64), nn.ReLU(),\n",
    "                                      nn.Linear(64, 32), nn.ReLU(),\n",
    "                                      nn.Linear(32, 1))\n",
    "\n",
    "    def forward(self, x, var, amps, s0, n_init=16):\n",
    "        \"\"\"\n",
    "        Parameters: \n",
    "        - x(batch,seq,input_size): Input features (1.B, 2.dB, 3.dB/dt)  \n",
    "        - var(batch,var_size): Supplementary inputs (1.F 2.T 3.Hdc 4.N 5.Duty_P 6.Duty_N) \n",
    "        - s0(batch,1): Operator inital states\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)  # Batch size\n",
    "        seq_size = x.size(1)  # Ser\n",
    "        self.rnn1_hx = s0\n",
    "\n",
    "        # !Initialize DNN2 input (1.B 2.dB/dt 3.d2B)\n",
    "        # x2 = torch.cat((x[:, :, 0:1], x[:, :, 2:3]), dim=2)\n",
    "        # !選取 B, dB/dt, d2B/dt\n",
    "        x2 = torch.cat((x[:, :, 0:1], x[:, :, 2:4]), dim=2)\n",
    "\n",
    "        for t in range(seq_size):\n",
    "            # RNN1 input (dB,state)\n",
    "            self.rnn1_hx = self.rnn1(x[:, t, 1:2], self.rnn1_hx)\n",
    "\n",
    "            # DNN1 input (rnn1_hx,F,T,Hdc,N)\n",
    "            dnn1_in = torch.cat((self.rnn1_hx, var), dim=1)\n",
    "\n",
    "            # H hysteresis prediction\n",
    "            H_hyst_pred = self.dnn1(dnn1_in)\n",
    "\n",
    "            # DNN2 input (B,dB/dt,T,F)\n",
    "            rnn2_in = torch.cat((x2[:, t, :], var), dim=1)\n",
    "\n",
    "            # Initialize second rnn state\n",
    "            if t == 0:\n",
    "                H_eddy_init = x[:, t, 0:1] - H_hyst_pred\n",
    "                buffer = x.new_ones(x.size(0), self.hidden_size)\n",
    "                self.rnn2_hx = Variable(\n",
    "                    (buffer / torch.sum(self.dnn2.weight, dim=1)) *\n",
    "                    H_eddy_init)\n",
    "\n",
    "            #rnn2_in = torch.cat((rnn2_in,H_hyst_pred),dim=1)\n",
    "            self.rnn2_hx = self.rnn2(rnn2_in, self.rnn2_hx)\n",
    "\n",
    "            # H eddy prediction\n",
    "            H_eddy = self.dnn2(self.rnn2_hx)\n",
    "\n",
    "            # H total\n",
    "            H_total = (H_hyst_pred + H_eddy).view(batch_size, 1,\n",
    "                                                  self.output_size)\n",
    "            if t == 0:\n",
    "                output = H_total\n",
    "            else:\n",
    "                output = torch.cat((output, H_total), dim=1)\n",
    "\n",
    "        H = (output[:, n_init:, :])\n",
    "\n",
    "        amp_B = amps[:, 0:1]  # (batch,1)\n",
    "        amp_H = amps[:, 1:2]  # (batch,1)\n",
    "        B_amp = x[:, n_init:, 0:1] * amp_B.unsqueeze(1)\n",
    "        H_amp = output[:, n_init:, :] * amp_H.unsqueeze(1)\n",
    "        P_prelim = torch.trapz(H_amp, B_amp, axis=1) * (10**(\n",
    "            var[:, 0:1] * self.norm[0][1] + self.norm[0][0]))\n",
    "        Pcv_log = torch.log10(P_prelim.clamp(min=1e-12))\n",
    "        Pcv = (Pcv_log - self.norm[4][0]) / self.norm[4][1]\n",
    "        mlp_input = torch.cat((var, Pcv), dim=1)  # (batch, 5)\n",
    "        s = self.loss_mlp(mlp_input)\n",
    "        Pcv_mlp = Pcv + s\n",
    "\n",
    "        return H, Pcv_mlp\n",
    "\n",
    "\n",
    "class StopOperatorCell():\n",
    "\n",
    "    def __init__(self, operator_size):\n",
    "        self.operator_thre = torch.from_numpy(\n",
    "            np.linspace(Config.MAXOUT_H / operator_size, Config.MAXOUT_H,\n",
    "                        operator_size)).view(1, -1)\n",
    "\n",
    "    def sslu(self, X):\n",
    "        a = torch.ones_like(X)\n",
    "        return torch.max(-a, torch.min(a, X))\n",
    "\n",
    "    def __call__(self, dB, state):\n",
    "        r = self.operator_thre.to(dB.device)\n",
    "        output = self.sslu((dB + state) / r) * r\n",
    "        return output.float()\n",
    "\n",
    "\n",
    "class EddyCell(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size=1):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.x2h = nn.Linear(input_size, hidden_size, bias=False)\n",
    "        self.h2h = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        hidden = self.x2h(x) + self.h2h(hidden)\n",
    "        hidden = torch.sigmoid(hidden)\n",
    "        return hidden\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3: Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def load_dataset(material, base_path=\"./Data/\"):\n",
    "\n",
    "    in_file1 = f\"{base_path}{material}/train/B_Field.csv\"\n",
    "    in_file2 = f\"{base_path}{material}/train/Frequency.csv\"\n",
    "    in_file3 = f\"{base_path}{material}/train/Temperature.csv\"\n",
    "    in_file4 = f\"{base_path}{material}/train/H_Field.csv\"\n",
    "    in_file5 = f\"{base_path}{material}/train/Volumetric_Loss.csv\"\n",
    "    in_file6 = f\"{base_path}{material}/train/Hdc.csv\"\n",
    "    in_file7 = f\"{base_path}{material}/train/Turns.csv\"\n",
    "    in_file8 = f\"{base_path}{material}/train/Duty_P.csv\"\n",
    "    in_file9 = f\"{base_path}{material}/train/Duty_N.csv\"\n",
    "\n",
    "    data_B = np.genfromtxt(in_file1, delimiter=',')  # N x 1024\n",
    "    data_F = np.genfromtxt(in_file2, delimiter=',')  # N x 1\n",
    "    data_T = np.genfromtxt(in_file3, delimiter=',')  # N x 1\n",
    "    data_H = np.genfromtxt(in_file4, delimiter=',')  # N x 1024\n",
    "    data_Pcv = np.genfromtxt(in_file5, delimiter=',')  # N x 1\n",
    "    data_Hdc = np.genfromtxt(in_file6, delimiter=',')  # N x 1\n",
    "    data_N = np.genfromtxt(in_file7, delimiter=',')  # N x 1\n",
    "    data_Duty_P = np.genfromtxt(in_file8, delimiter=',')  # N x 1\n",
    "    data_Duty_N = np.genfromtxt(in_file9, delimiter=',')  # N x 1\n",
    "\n",
    "    return data_B, data_F, data_T, data_H, data_Pcv, data_Hdc, data_N, data_Duty_P, data_Duty_N\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainLogger:\n",
    "\n",
    "    def __init__(self, exp_name, config_dict, result_dir):\n",
    "        self.exp_name = exp_name\n",
    "        self.result_dir = result_dir\n",
    "        self.config = config_dict\n",
    "        os.makedirs(self.result_dir, exist_ok=True)\n",
    "\n",
    "        self._save_config()\n",
    "        self._write_metadata()\n",
    "\n",
    "    def _save_config(self):\n",
    "        with open(os.path.join(self.result_dir, \"config.json\"), \"w\") as f:\n",
    "            json.dump(self.config, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    def _write_metadata(self):\n",
    "        metadata = {\n",
    "            \"experiment_name\": self.exp_name,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "        with open(os.path.join(self.result_dir, \"meta.json\"), \"w\") as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "\n",
    "    def save_norm_params(self, norm, feature_names=[\"F\", \"T\", \"Hdc\", \"Pcv\"]):\n",
    "        \"\"\"\n",
    "        將標準化參數存成：\n",
    "        {\n",
    "          \"CH467160\": [\n",
    "             [mean_F, std_F],\n",
    "             [mean_T, std_T],\n",
    "             [mean_Hdc, std_Hdc],\n",
    "             [mean_N, std_N],\n",
    "             [mean_Pcv, std_Pcv],\n",
    "          ]\n",
    "        }\n",
    "        \"\"\"\n",
    "        # 從 exp_name 前半段取出 material\n",
    "        material_key = self.exp_name.split('_')[0]\n",
    "\n",
    "        # 直接把 norm (list of [mean, std]) 當成 value\n",
    "        output = {material_key: norm}\n",
    "\n",
    "        # 寫檔\n",
    "        with open(os.path.join(self.result_dir, \"norm_params.json\"), \"w\") as f:\n",
    "            json.dump(output, f, indent=4, ensure_ascii=False)\n",
    "        print(\n",
    "            f\"✅ Normalization parameters saved to {os.path.join(self.result_dir, 'norm_params.json')}\"\n",
    "        )\n",
    "\n",
    "    def save_summary(self, best_epoch, best_val_loss, best_loss_H,\n",
    "                     best_loss_Pcv, model_save_path, elapsed):\n",
    "        summary = {\n",
    "            \"exp_name\": self.exp_name,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"duration_sec\": elapsed,\n",
    "            \"config\": self.config,\n",
    "            \"best_model\": {\n",
    "                \"path\": model_save_path,\n",
    "                \"epoch\": best_epoch,\n",
    "                \"val_loss\": best_val_loss,\n",
    "                \"loss_H\": best_loss_H,\n",
    "                \"loss_Pcv\": best_loss_Pcv\n",
    "            },\n",
    "            \"note\": note,\n",
    "            \"note detail\": note_detail\n",
    "        }\n",
    "        with open(os.path.join(self.result_dir, \"summary.json\"), \"w\") as f:\n",
    "            json.dump(summary, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clamp_learning_rate(optimizer, min_lr=1e-5):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        if param_group['lr'] < min_lr:\n",
    "            param_group['lr'] = min_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(norm, train_loader, valid_loader, logger):\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "    model = MMINet(norm=norm).to(device)\n",
    "    print(\"=== Start Train  ===\")\n",
    "    print(r\"\"\"\n",
    "          \n",
    "          \n",
    "                                                    ⠀⠀⠀⠀⢀⡤⠖⠋⠉⠉⠉⠉⠙⠲⣦⣀⠀⠀⠀⠀⠀\n",
    "                                                    ⠀⠀⠀⡴⠋⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠻⣦⡀⠀⠀⠀\n",
    "                                                    ⠀⠀⡼⢁⡠⢼⠁⠀⢱⢄⣀⠀⠀⠀⠀⠀⠎⢿⡄⠀⠀\n",
    "                                                    ⠀⣸⠁⠀⣧⣼⠀⠀⣧⣼⠉⠀⠀⠀⠀⠀⠐⢬⣷⠀⠀\n",
    "                                                    ⡼⣿⢀⠀⣿⡟⠀⠀⣿⣿⠀⠀⠀⠀⠀⠀⠀⠀⢹⣧⠀\n",
    "                  我好想畢業                         ⣇⢹⠀⠁⠈⠀⠉⠃⠈⠃⠀⠀⠀⠀⠀⠀⠀⠀⡰⢸⡇\n",
    "                                                    ⠙⢿⣧⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⣏⣈⣉⣤⠿⠁\n",
    "                                                    ⠀⣠⣾⣿⠤⡀⠀⠀⠀⠀⠀⢀⣤⣶⣿⣿⣿⣿⣅⠀⠀\n",
    "                                                    ⢰⣧⣿⣿⣿⣦⣉⡐⠒⠒⢲⣿⣿⣿⣿⣿⣿⣶⣿⣧⠀\n",
    "                                                    ⠘⠿⢿⣿⣿⣿⡿⠿⠛⠿⠿⠿⣿⣿⣿⣿⣿⣿⡿⠟⠀\n",
    "                                                    ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠉⠉⠁⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⣾⣷⣄⠀⠀⠀⣀⣤⣤⣤⡀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣶⠏⠀⠀⣿⠀⢀⡾⠛⠋⠀⣾⣿⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡀⡏⠀⠀⠀⣿⢀⣾⠁⠀⣰⠆⢹⡿⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠃⣧⠀⠀⢠⡟⢸⡇⠀⣰⠟⠀⣼⠃⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣀⣹⣆⢀⣸⣇⣸⠃⢠⡏⠀⣸⠋⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⣀⣤⣴⣶⣶⣶⠾⠟⠛⠉⠉⠉⠈⠉⠉⠛⠁⢾⠁⣴⠇⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣀⣤⣤⣶⣶⠾⠟⠛⠛⣻⣿⣙⡁⠀⠀⢾⣶⣾⣷⣿⣶⣄⠀⠀⠀⠀⠰⢿⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⢀⣀⣀⣀⣠⣴⣶⣶⠾⠟⠛⠉⠉⠉⠀⠀⠀⠀⠀⣿⣻⣟⣻⣿⡦⠀⠘⣿⣿⣛⡿⢶⡇⠀⠀⠀⠀⠀⠀⢻⣆⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⣠⣶⣶⣶⣾⣿⣿⣿⣿⣿⣿⣿⣧⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⡟⠙⣿⣿⡗⠀⠀⠿⠉⣿⣿⣿⣶⠀⠀⠀⠀⠀⠀⠈⢿⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⠳⣄⣿⡿⠁⠀⠀⠘⢦⣿⣿⠇⠟⠁⠀⠀⠀⠀⠀⠀⣸⡇⠀⠀⠀⠀⠀⠀⠀\n",
    "⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠁⡇⠀⠀⠀⠀⠀⠀⠀\n",
    "⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢰⣇⠀⠀⠀⠀⠀⠀⠀\n",
    "⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⣿⠀⠀⠀⠀⠀⠀⠀\n",
    "⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡏⠀⠀⠀⠀⠀⠀⠀\n",
    "⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀\n",
    "⢻⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡟⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣇⡇⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠻⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢤⣤⡀⠀⠀⠀⠀⠀⠀⠀⣿⡇⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠈⠙⢿⣿⣿⣿⣿⣿⣿⠟⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣀⣤⡾⠟⠛⠆⠀⠀⠀⠀⠀⢀⢻⡇⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠈⠙⠿⣿⣭⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⣤⣴⣶⠾⠟⠋⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⣾⠇⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠈⠉⠙⠛⠷⠶⢶⣶⣦⣤⣴⡆⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣠⣌⣿⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⣿⡄⠀⠀⠀⠀⠀⠀⠀⠀⠙⠛⠛⠛⠃⠀⠀⠀⠀⠀⠀⠀⣤⣴⣾⣿⣿⣿⣓⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣼⣿⣷⣦⣄⣀⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⣀⣠⣤⣶⣾⣟⣯⣽⠟⠋⠀⠉⠳⣄⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣾⢇⠀⠉⠛⠷⣮⣍⣩⡍⢻⡟⠉⣉⢹⡏⠉⣿⣹⣷⣦⣿⠿⠟⠉⠀⠀⠀⠀⠀⠀⠙⣆⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣰⠏⢸⠇⠀⠀⠀⠀⠀⠉⠉⠛⠛⠛⠛⠛⠛⠛⠋⠉⠉⠀⠀⠀⠀⠀⢠⣠⡶⠀⠀⠀⠀⠘⣧⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢰⡿⠀⣸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠟⠁⠀⠀⠀⠀⠀⠘⣆⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣾⠃⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣾⣇⡀⠀⠀⠀⠀⠀⠀⢹⡆⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⣾⠀⣾⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⢿⣥⢠⣤⠼⠇⠀⠀⠘⣿⡄\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⣽⡄⠈⢿⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⣿⠿⠾⠷⠄⠀⠀⠀⢀⣿⠁\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⣧⠀⠸⣷⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣤⣾⠋⠀⠀⠀⠀⠀⠀⢰⣾⡿⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠙⣦⣠⣿⣿⣶⣶⣤⣤⣄⣀⣀⣀⣀⠀⠀⠀⠀⠀⠀⠀⣀⣀⣠⣴⣿⣇⠀⠀⠀⠀⠀⠀⠀⣸⡟⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⢻⣿⠀⠉⠛⢿⣿⣯⣿⡟⢿⠻⣿⢻⣿⢿⣿⣿⣿⣿⣿⠿⠟⠹⣟⢷⣄⠀⠀⠀⢀⣼⠟⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⣿⣄⠀⠀⠘⢷⣌⡻⠿⣿⣛⣿⣟⣛⣛⣋⣉⣉⣉⣀⡀⠀⠀⠈⠻⢿⣷⣶⣶⢛⣧⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⣏⠀⠀⠀⠀⠹⢯⣟⣛⢿⣿⣽⣅⣀⡀⠀⣀⡀⠀⠀⠀⠠⢦⣀⠰⡦⠀⢸⠀⣏⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢿⡀⠀⠀⠀⠀⠀⠀⠈⠉⢻⣿⡟⠛⠉⠉⠁⠀⠀⠀⠀⠀⠀⠈⠛⠷⠀⣸⠀⣿⡀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⣧⠀⠀⠀⠀⠀⠀⠀⠀⠘⣿⣇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⠀⣿⡇⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠸⢿⠀⠀⢦⡀⡀⠀⠀⠀⠀⢹⣿⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⡄⡏⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⡄⠀⠈⠳⣝⠦⢄⠀⠀⠀⣟⣷⠀⠀⠀⣷⣄⠀⠀⠀⠀⠀⠀⠀⠀⣿⡇⡇⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣄⣷⡀⠀⠀⠈⠙⠂⠀⠀⠀⢸⣿⡄⠀⠀⠘⢦⡙⢦⡀⠀⠀⠀⠀⢰⣷⣷⡇⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢻⡿⢧⣤⣀⡀⠀⠀⠀⠀⠀⠀⢿⣷⣄⠀⠀⠀⠁⠋⠀⠀⠀⠀⠀⢸⣿⣿⣇⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⣷⡀⠈⠉⠛⠛⠛⠛⠛⠛⠛⠛⢿⡍⠛⠳⠶⣶⣤⣤⣤⣤⣤⣤⠼⠟⡟⢿⡇⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⣷⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⣧⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠰⣾⡇⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣠⣤⣴⣿⣷⣶⣶⣶⣶⣶⣶⣦⣀⣀⣀⣻⡀⠀⠀⠀⣀⣀⠀⡀⠀⠀⠀⢀⣼⣿⠇⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣴⠟⠉⠁⠀⠀⠈⠻⣿⡆⢹⣯⣽⣿⣿⠟⠋⠙⣿⣶⣿⣿⣿⣿⣾⣿⣿⣿⣟⠋⠉⣇⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⡇⠀⠀⠀⡀⠀⠀⠀⠈⢻⣆⣿⠀⠀⠀⢁⣶⣿⠿⠟⠛⠷⣶⣽⣿⣿⣻⣏⠙⠃⣴⢻⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠸⣷⣀⠀⠀⠉⠀⠀⠀⠀⠀⢹⣿⠀⣀⣴⣿⠋⠀⠀⠀⠀⠀⠀⠉⠻⣿⣧⣿⢀⣰⣿⣿⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⢿⣶⣶⣤⣤⣤⣤⣤⣤⣾⣿⣟⣿⣿⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⣿⣅⣾⢿⣵⠇⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠉⠛⠛⠛⠛⠛⠛⠛⠛⠉⠉⠉⠁⢹⣜⠷⠦⠤⠤⠤⠤⠤⠴⠶⠛⣉⣱⠿⠁⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠛⠿⠷⣦⣤⣤⣄⣠⣤⣤⡶⠟⠁⠀⠀⠀⠀⠀⠀⠀\n",
    "                        \n",
    "                        \n",
    "                    我是Chill guy，這是我的畢業碩論，我做不完，但沒關西，大不了休學\n",
    "                        \n",
    "    \"\"\")\n",
    "    print(\"Number of parameters: \", count_parameters(model))\n",
    "\n",
    "    criterion_H = nn.MSELoss()\n",
    "    criterion_Pcv = nn.MSELoss()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=Config.LEARNING_RATE)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer,\n",
    "        step_size=Config.DECAY_EPOCH,\n",
    "        gamma=Config.LR_SCHEDULER_GAMMA)\n",
    "\n",
    "    # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "    #                                                        mode=\"min\",\n",
    "    #                                                        factor=0.5,\n",
    "    #                                                        patience=50,\n",
    "    #                                                        min_lr=1e-5)\n",
    "\n",
    "    # Loss 記錄\n",
    "    best_val_loss = float('inf')\n",
    "    best_loss_Pcv = float('inf')\n",
    "    best_loss_H = float('inf')\n",
    "\n",
    "    # Early stopping 紀錄\n",
    "    # patience_counter = 0 # 單純固定的早停計數器\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    fixed_idx = None\n",
    "\n",
    "    best_loss_H = float('inf')\n",
    "    best_loss_Pcv = float('inf')\n",
    "    wait_H = wait_Pcv = 0\n",
    "    MIN_DELTA = 1e-12  # 低進步門檻:驗證損失在後期常卡在小數點後幾位來回抖動；若不設門檻，模型可能因微小雜訊一直重置等待計數，永遠觸發不了早停\n",
    "    PATIENCE_H = Config.EARLY_STOPPING_PATIENCE\n",
    "    PATIENCE_PCV = Config.EARLY_STOPPING_PATIENCE\n",
    "    joint_phase = False\n",
    "\n",
    "    # 保存每個 epoch 的時間\n",
    "    epoch_times = []\n",
    "    # Logger 紀錄\n",
    "    best_epoch = 0\n",
    "    history = {\n",
    "        \"epoch\": [],\n",
    "        \"train_loss\": [],\n",
    "        \"val_loss\": [],\n",
    "        \"loss_H\": [],\n",
    "        \"loss_Pcv\": []\n",
    "    }\n",
    "\n",
    "    for epoch in range(Config.NUM_EPOCHS):\n",
    "\n",
    "        t0 = time.perf_counter()\n",
    "        alpha = (epoch + 1) / Config.NUM_EPOCHS\n",
    "\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "\n",
    "        for inputs, features, amps, s0, target_H, target_Pcv in train_loader:\n",
    "\n",
    "            inputs, features, amps, s0, target_H, target_Pcv = inputs.to(\n",
    "                device), features.to(device), amps.to(device), s0.to(\n",
    "                    device), target_H.to(device), target_Pcv.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.autocast(device_type=\"cuda\"):\n",
    "                outputs_H, outputs_Pcv = model(inputs, features, amps,\n",
    "                                               s0)  # 模型的輸出\n",
    "                loss_H = criterion_H(outputs_H, target_H)  # 使用真實的 H(t) 計算損失\n",
    "                loss_Pcv = criterion_Pcv(outputs_Pcv, target_Pcv)\n",
    "\n",
    "                loss = (1 - alpha) * loss_H + alpha * loss_Pcv\n",
    "                # alpha = 0.5\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)  # **記錄 Train Loss**\n",
    "\n",
    "        # ------------------------------vaildation------------------------------\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_loss_H = 0.0\n",
    "        val_loss_Pcv = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, features, amps, s0, target_H, target_Pcv in valid_loader:\n",
    "                inputs, features, amps, s0, target_H, target_Pcv = inputs.to(\n",
    "                    device), features.to(device), amps.to(device), s0.to(\n",
    "                        device), target_H.to(device), target_Pcv.to(device)\n",
    "\n",
    "                outputs_H, outputs_Pcv = model(inputs, features, amps,\n",
    "                                               s0)  # 模型的輸出\n",
    "                loss_H = criterion_H(outputs_H, target_H)  # 使用真實的 H(t) 計算損失\n",
    "                loss_Pcv = criterion_Pcv(outputs_Pcv, target_Pcv)\n",
    "\n",
    "                alpha = (epoch + 1) / Config.NUM_EPOCHS\n",
    "                # alpha = 0.5\n",
    "                loss = (1 - alpha) * loss_H + alpha * loss_Pcv\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                val_loss_H += loss_H.item()\n",
    "                val_loss_Pcv += loss_Pcv.item()\n",
    "\n",
    "        # 求驗證集平均\n",
    "        val_loss_H /= len(valid_loader)\n",
    "        val_loss_Pcv /= len(valid_loader)\n",
    "        val_loss /= len(valid_loader)\n",
    "        val_losses.append(val_loss)  # **記錄 Validation Loss**\n",
    "\n",
    "        # ──────────────判斷lr更新是依據哪個val_loss──────────────\n",
    "        if not joint_phase:\n",
    "            # H-phase: 監控 H loss\n",
    "            monitored_metric = val_loss_H\n",
    "            print(\n",
    "                f\"--- [Phase 1: H-focus] Monitoring val_loss_H: {monitored_metric:.6f} ---\"\n",
    "            )\n",
    "        else:\n",
    "            # Pcv-phase: 監控 Pcv loss\n",
    "            monitored_metric = val_loss_Pcv\n",
    "            print(\n",
    "                f\"--- [Phase 2: Pcv-focus] Monitoring val_loss_Pcv: {monitored_metric:.6f} ---\"\n",
    "            )\n",
    "\n",
    "        # scheduler.step(monitored_metric)  # 更新學習率\n",
    "        # current_lr = scheduler.get_last_lr()[0]\n",
    "        # print(f\"Epoch {epoch+1} | Learning Rate: {current_lr:.6f}\")\n",
    "\n",
    "        scheduler.step()  # scheduler 更新\n",
    "        clamp_learning_rate(optimizer)  # 避免learning rate掉到 0\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"Epoch {epoch+1} | Learning Rate: {current_lr:.6f}\")\n",
    "\n",
    "        # ─── 計算並輸出這個 epoch 的耗時 ───────────\n",
    "        te = time.perf_counter() - t0\n",
    "        epoch_times.append(te)\n",
    "        print(f\"---\")\n",
    "        print(f\"alpha: {alpha:.3f}\")\n",
    "        print(\n",
    "            f\"Epoch {epoch+1}, loss_H: {val_loss_H :.6f}, loss_Pcv: {val_loss_Pcv:.6f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Epoch {epoch+1} | Train Loss: {train_loss:.6f} | Val Loss: {val_loss:.6f} | Time: {te:.2f}s\"\n",
    "        )\n",
    "        print(\n",
    "            f\"(Best Epoch {best_epoch} | best H: {best_loss_H:.6f}| best Pcv: {best_loss_Pcv:.6f}| val_loss : {val_loss:.6f})\"\n",
    "        )\n",
    "\n",
    "        # ─── 記錄訓練歷史 ──────────────────────\n",
    "        history[\"epoch\"].append(epoch + 1)\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"loss_H\"].append(val_loss_H)\n",
    "        history[\"loss_Pcv\"].append(val_loss_Pcv)\n",
    "\n",
    "        # ======================================================繪製訓練情況======================================================\n",
    "\n",
    "        if (epoch + 1) % plot_interval == 0:\n",
    "\n",
    "            # 第一次產生固定的隨機索引\n",
    "            if fixed_idx is None:\n",
    "                batch_size_fix = 3\n",
    "                fixed_idx = torch.randperm(batch_size_fix)[:train_show_sample]\n",
    "\n",
    "            # # -------------------------設定圖表H(t)比較---------------------------------------\n",
    "\n",
    "            # outputs = [fixed_idx, :downsample,\n",
    "            #  0].detach().cpu().numpy()\n",
    "            # targets_np = target_H[fixed_idx, :downsample,\n",
    "            #                       0].detach().cpu().numpy()\n",
    "\n",
    "            # plt.figure(figsize=(12, 6))\n",
    "\n",
    "            # for i in range(outputs.shape[0]):  # 每一批數據繪製一個圖表\n",
    "            #     plt.plot(outputs[i, :, 0],\n",
    "            #              label=f\"Pred: Sample {i+1}\",\n",
    "            #              linestyle='--',\n",
    "            #              marker='o')\n",
    "            #     plt.plot(targets[i, :, 0],\n",
    "            #              label=f\"Target: Sample {i+1}\",\n",
    "            #              linestyle='-',\n",
    "            #              marker='x')\n",
    "\n",
    "            # # 添加標題和標籤\n",
    "            # plt.title(f\"Compare - Epoch {epoch + 1}\", fontsize=16)\n",
    "            # plt.xlabel(\"Index\", fontsize=14)\n",
    "            # plt.ylabel(\"Value\", fontsize=14)\n",
    "            # plt.legend(loc=\"upper right\", fontsize=12)\n",
    "            # plt.grid(alpha=0.5)\n",
    "\n",
    "            # # 顯示圖表\n",
    "            # plt.show()\n",
    "            # # -------------------------設定圖表H(t)比較 結束---------------------------------------\n",
    "\n",
    "            # # -------------------------設定圖表B-H比較---------------------------------------\n",
    "            # 取對應 sample\n",
    "            outputs_np = outputs_H[fixed_idx, -downsample:,\n",
    "                                   0].detach().cpu().numpy()\n",
    "            targets_np = target_H[fixed_idx, -downsample:,\n",
    "                                  0].detach().cpu().numpy()\n",
    "            B_seq_np = inputs[fixed_idx, -downsample:,\n",
    "                              0].detach().cpu().numpy()\n",
    "\n",
    "            # 設定圖表\n",
    "            plt.figure()\n",
    "\n",
    "            for i in range(train_show_sample):  # 每一批數據繪製一個圖表\n",
    "                plt.plot(outputs_np[i],\n",
    "                         B_seq_np[i],\n",
    "                         label=f\"Pred: Sample {i+1}\",\n",
    "                         markersize=1)\n",
    "\n",
    "                plt.plot(targets_np[i],\n",
    "                         B_seq_np[i],\n",
    "                         label=f\"Target: Sample {i+1}\",\n",
    "                         alpha=0.5)\n",
    "\n",
    "            # 添加標題和標籤\n",
    "            plt.title(f\"Compare - Epoch {epoch + 1}\")\n",
    "            plt.xlabel(\"Index\")\n",
    "            plt.ylabel(\"Value\")\n",
    "            plt.grid(alpha=0.5)\n",
    "            plt.legend()\n",
    "            if save_figure == True:\n",
    "                figure_save_path1 = os.path.join(\n",
    "                    figure_save_base_path,\n",
    "                    f\"Compare_Epoch {epoch + 1}.svg\")  # 定義模型保存檔名\n",
    "                plt.savefig(figure_save_path1)\n",
    "            plt.show()\n",
    "            # # -------------------------設定圖表B-H比較 END---------------------------------------\n",
    "        # ======================================================繪製訓練情況  END ======================================================\n",
    "\n",
    "        # ======================================================Early stop======================================================\n",
    "        # if val_loss < best_val_loss:\n",
    "        #     best_val_loss = val_loss\n",
    "        #     best_epoch = epoch + 1\n",
    "        #     best_loss_H = val_loss_H\n",
    "        #     best_loss_Pcv = val_loss_Pcv\n",
    "        #     torch.save(model.state_dict(), model_save_path)  # 保存最佳模型\n",
    "        #     print(\n",
    "        #         f\"→Saving model at epoch {epoch+1} with validation loss {val_loss:.6f}...\"\n",
    "        #     )\n",
    "        #     patience_counter = 0\n",
    "        # else:\n",
    "        #     patience_counter += 1\n",
    "        #     print(\n",
    "        #         f\"  無改善，patience_counter = {patience_counter}/{Config.EARLY_STOPPING_PATIENCE}\"\n",
    "        #     )\n",
    "\n",
    "        # if patience_counter >= Config.EARLY_STOPPING_PATIENCE:\n",
    "        #     print(\"Early stopping triggered.\")\n",
    "        #     break\n",
    "\n",
    "        # --- H跟Pcv一個結束就結束 ---\n",
    "        # joint_phase = (alpha >= SWITCH_ALPHA)  # 判斷現在在哪一段\n",
    "\n",
    "        # if not joint_phase:  # ① 只看 H\n",
    "        #     if val_loss_H < best_loss_H - MIN_DELTA:\n",
    "        #         best_loss_H = val_loss_H\n",
    "        #         best_epoch = epoch + 1\n",
    "        #         wait_H = 0\n",
    "        #         torch.save(model.state_dict(), model_save_path)\n",
    "        #         print(f\"✅ Save best H @ epoch {epoch+1}\")\n",
    "        #     else:\n",
    "        #         wait_H += 1\n",
    "        #         print(f\"  H 無改善，wait_H={wait_H}/{PATIENCE_H}\")\n",
    "        #     if wait_H >= PATIENCE_H:\n",
    "        #         print(\"🔸 Early-Stop (H) 觸發\")\n",
    "        #         break\n",
    "        # else:  # ② 只看 Pcv\n",
    "        #     if val_loss_Pcv < best_loss_Pcv - MIN_DELTA:\n",
    "        #         best_loss_Pcv = val_loss_Pcv\n",
    "        #         best_epoch = epoch + 1\n",
    "        #         wait_Pcv = 0\n",
    "        #         torch.save(model.state_dict(), model_save_path)\n",
    "        #         print(f\"✅ Save best Pcv @ epoch {epoch+1}\")\n",
    "        #     else:\n",
    "        #         wait_Pcv += 1\n",
    "        #         print(f\"  Pcv 無改善，wait_Pcv={wait_Pcv}/{PATIENCE_PCV}\")\n",
    "        #     if wait_Pcv >= PATIENCE_PCV:\n",
    "        #         print(\"🔸 Early-Stop (Pcv) 觸發\")\n",
    "        #         break\n",
    "\n",
    "        # --- H結束接著Pcv ---\n",
    "        if not joint_phase:  # H-phase\n",
    "            if val_loss_H < best_loss_H - MIN_DELTA:\n",
    "                best_loss_H = val_loss_H\n",
    "                best_loss_Pcv = val_loss_Pcv\n",
    "                best_epoch = epoch + 1\n",
    "                wait_H = 0\n",
    "                torch.save(model.state_dict(), model_save_path)\n",
    "                print(f\"✅ Save best H @ epoch {best_epoch}\")\n",
    "            else:\n",
    "                wait_H += 1\n",
    "                print(f\"  H 無改善 wait_H={wait_H}/{PATIENCE_H}\")\n",
    "\n",
    "            if wait_H >= PATIENCE_H:  # ← 不再 break！\n",
    "                print(\"🔸 H 早停 → 切到 Pcv-phase\")\n",
    "                joint_phase = True  # 切旗標\n",
    "                wait_Pcv = 0  # 重設計數\n",
    "                continue  # 直接下一個 epoch\n",
    "\n",
    "        else:  # Pcv-phase\n",
    "            if val_loss_Pcv < best_loss_Pcv - MIN_DELTA and val_loss_H < best_loss_H * 1.05 - MIN_DELTA:\n",
    "                best_loss_H = val_loss_H\n",
    "                best_loss_Pcv = val_loss_Pcv\n",
    "                best_epoch = epoch + 1\n",
    "                wait_Pcv = 0\n",
    "                torch.save(model.state_dict(), model_save_path)\n",
    "                print(f\"✅ Save best Pcv @ epoch {best_epoch}\")\n",
    "            else:\n",
    "                wait_Pcv += 1\n",
    "                print(f\"  Pcv 無改善 wait_Pcv={wait_Pcv}/{PATIENCE_PCV}\")\n",
    "\n",
    "            if wait_Pcv >= PATIENCE_PCV:  # 真正結束\n",
    "                print(\"🔸 Pcv 早停觸發，整體訓練結束\")\n",
    "                break\n",
    "\n",
    "        # ======================================================Early stop======================================================\n",
    "\n",
    "    print(f\"Training complete. Best model saved at {model_save_path}.\")\n",
    "    elapsed = time.perf_counter() - start_time  # ← 訓練結束，計算耗時\n",
    "    hrs = int(elapsed // 3600)\n",
    "    mins = int((elapsed % 3600) // 60)\n",
    "    secs = elapsed % 60\n",
    "    print(f\"訓練總耗時：{hrs} 小時 {mins} 分 {secs:.2f} 秒\")\n",
    "    logger.save_summary(best_epoch, best_val_loss, best_loss_H, best_loss_Pcv,\n",
    "                        model_save_path, elapsed)\n",
    "\n",
    "    hist_df = pd.DataFrame(history)\n",
    "\n",
    "    json_path = os.path.join(result_dir, \"training_history.json\")\n",
    "    hist_df.to_json(json_path, orient=\"records\", force_ascii=False, indent=2)\n",
    "    print(f\"✅ 已儲存訓練歷程到 {json_path}\")\n",
    "\n",
    "    # ==============================繪製 Train Loss 與 Validation Loss 圖==============================\n",
    "    # plt.figure(figsize=(10, 5))\n",
    "    # plt.plot(\n",
    "    #     range(1,\n",
    "    #           len(train_losses) + 1),\n",
    "    #     train_losses,\n",
    "    #     label=\"Train Loss\",\n",
    "    # )\n",
    "    # plt.plot(range(1,\n",
    "    #                len(val_losses) + 1),\n",
    "    #          val_losses,\n",
    "    #          label=\"Validation Loss\")\n",
    "\n",
    "    # plt.xlabel(\"Epochs\")\n",
    "    # plt.ylabel(\"Loss\")\n",
    "    # plt.title(\"Training & Validation Loss Curve\")\n",
    "    # plt.legend()\n",
    "    # plt.grid(alpha=0.5)\n",
    "    # if save_figure == True:\n",
    "    #     # 將圖表保存為 SVG 格式\n",
    "    #     figure_save_path2 = os.path.join(figure_save_base_path,\n",
    "    #                                      \"Training_Validation_Loss_Curve.svg\")\n",
    "    #     plt.savefig(figure_save_path2)\n",
    "    # plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    # --- ① Train / Val Total loss ---\n",
    "    plt.plot(range(1,\n",
    "                   len(train_losses) + 1),\n",
    "             train_losses,\n",
    "             label=\"Train Total\")\n",
    "    plt.plot(range(1, len(val_losses) + 1), val_losses, label=\"Val Total\")\n",
    "\n",
    "    # ➜ ② 另外畫 H-loss、Pcv-loss  (透明度低一點)\n",
    "    plt.plot(range(1,\n",
    "                   len(history[\"loss_H\"]) + 1),\n",
    "             history[\"loss_H\"],\n",
    "             label=\"Val H\",\n",
    "             alpha=0.4,\n",
    "             ls=\"--\")\n",
    "    plt.plot(range(1,\n",
    "                   len(history[\"loss_Pcv\"]) + 1),\n",
    "             history[\"loss_Pcv\"],\n",
    "             label=\"Val Pcv\",\n",
    "             alpha=0.4,\n",
    "             ls=\"--\")\n",
    "\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training & Validation Loss Curve\")\n",
    "    plt.grid(alpha=0.5)\n",
    "\n",
    "    # ➜ ③ 標出最佳模型 epoch\n",
    "    plt.axvline(best_epoch, ls=\":\", lw=1, c=\"k\", label=f\"best @ {best_epoch}\")\n",
    "\n",
    "    plt.legend()\n",
    "\n",
    "    # ➜ ④ 永久存圖\n",
    "    fig_loss_path = os.path.join(result_dir, \"loss_curve_final.svg\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fig_loss_path)\n",
    "    if not save_figure:\n",
    "        plt.close()  # 不顯示直接關\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "    # ==============================繪製 Train Loss 與 Validation Loss 圖 END==============================\n",
    "\n",
    "    # ===================================使用最佳模型來產生驗證結果=============================\n",
    "    model.load_state_dict(torch.load(model_save_path))  # 載入最佳模型\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, features, amps, s0, target_H, target_Pcv in valid_loader:\n",
    "            inputs, features, amps, s0, target_H, target_Pcv = inputs.to(\n",
    "                device), features.to(device), amps.to(device), s0.to(\n",
    "                    device), target_H.to(device), target_Pcv.to(device)\n",
    "\n",
    "            outputs_H, outputs_Pcv = model(inputs, features, amps, s0)\n",
    "            break  # 只使用一批驗證數據進行可視化\n",
    "\n",
    "    # 選取對應資料（index tensor 要先轉 list 才能 index numpy）\n",
    "    outputs_np = outputs_H[fixed_idx, -downsample:, 0].detach().cpu().numpy()\n",
    "    targets_np = target_H[fixed_idx, -downsample:, 0].detach().cpu().numpy()\n",
    "    B_seq_np = inputs[fixed_idx, -downsample:, 0].detach().cpu().numpy()\n",
    "\n",
    "    # 設定圖表\n",
    "    plt.figure()\n",
    "    for i in range(train_show_sample):  # 每一批數據繪製一個圖表\n",
    "        plt.plot(outputs_np[i], B_seq_np[i], label=f\"Pred: Sample {i+1}\")\n",
    "        plt.plot(targets_np[i],\n",
    "                 B_seq_np[i],\n",
    "                 label=f\"Target: Sample {i+1}\",\n",
    "                 alpha=0.7)\n",
    "\n",
    "        # 添加標題和標籤\n",
    "        plt.title(f\"Best Model - Predicted vs Target\")\n",
    "        plt.xlabel(\"H(A/m)\")\n",
    "        plt.ylabel(\"B(T)\")\n",
    "        plt.grid(alpha=0.5)\n",
    "        plt.legend()\n",
    "\n",
    "        if save_figure == True:\n",
    "            figure_save_path3 = os.path.join(\n",
    "                figure_save_base_path,\n",
    "                f\"Best Model Predicted vs Target Sample.svg\")  # 定義模型保存檔名\n",
    "            plt.savefig(figure_save_path3)\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # ===================================使用最佳模型來產生驗證結果 END============================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Train!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Python用\n",
    "    # BASE_DIR = Path(__file__).resolve().parent\n",
    "    # os.chdir(BASE_DIR)\n",
    "    # print(\"👉 Switch CWD to script folder:\", os.getcwd())\n",
    "\n",
    "    data_B, data_F, data_T, data_H, data_Pcv, data_Hdc, data_N, data_Duty_P, data_Duty_N = load_dataset(\n",
    "        material)\n",
    "\n",
    "    GLOBAL_B_MAX = np.abs(data_B).max()\n",
    "    GLOBAL_H_MAX = np.abs(data_H).max()\n",
    "\n",
    "    train_loader, valid_loader, norm = get_dataloader(data_B, data_F, data_T,\n",
    "                                                      data_H, data_N, data_Hdc,\n",
    "                                                      data_Duty_P, data_Duty_N,\n",
    "                                                      data_Pcv, GLOBAL_B_MAX,\n",
    "                                                      GLOBAL_H_MAX)\n",
    "\n",
    "    # ---- 印第一個 batch 檢查 ----\n",
    "    # # inputs, features, s0, target_H, target_Pcv = next(iter(train_loader))\n",
    "    # inputs, features, amps, s0, target_H, target_Pcv = next(iter(train_loader))\n",
    "\n",
    "    # print(\"=== Batch shape check ===\")\n",
    "    # print(f\"inputs      : {inputs.shape}\")  # (batch, seq_len, 4)\n",
    "    # print(f\"features    : {features.shape}\")  # (batch, 4)\n",
    "    # print(f\"s0          : {s0.shape}\")  # (batch, operator_size)\n",
    "    # print(f\"target_H    : {target_H.shape}\")  # (batch, seq_len, 1)\n",
    "    # # print(f\"target_Pcv  : {target_Pcv.shape}\")  # (batch, 1)\n",
    "    # print()\n",
    "\n",
    "    # # 選一筆樣本看看數值範圍\n",
    "    # idx = 0\n",
    "    # print(\"範例 inputs[0] (前 3 個時間點):\")\n",
    "    # print(inputs[idx, :3, :])  # B, ΔB, dB/dt, d²B/dt² (已歸一化到 ~[-1,1])\n",
    "    # print(\"範例 features[0]:\", features[idx])  # F, T, Hdc, N (已 z-score)\n",
    "    # print(\"範例 s0[0]:\", s0[idx, :5])  # 前 5 個 Preisach operator 狀態\n",
    "    # print(\"範例 target_H[0] (前 3 點):\", target_H[idx, :3, 0])\n",
    "    # # print(\"範例 target_Pcv[0]:\", target_Pcv[idx, 0])\n",
    "\n",
    "    # 產生 Logger（放在 train_model 前）\n",
    "\n",
    "    logger = TrainLogger(\n",
    "        exp_name=f\"{material}_{note}_{timestamp}\",\n",
    "        config_dict={\n",
    "            k: getattr(Config, k)\n",
    "            for k in dir(Config)\n",
    "            if not k.startswith('__') and not callable(getattr(Config, k))\n",
    "        },\n",
    "        result_dir=result_dir)\n",
    "    feature_names = [\"F\", \"T\", \"Hdc\", \"N\", \"Pcv\"]\n",
    "    logger.save_norm_params(norm, feature_names)\n",
    "\n",
    "    train_model(norm, train_loader, valid_loader, logger)  # logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B shape: (2418, 1040)\n",
      "H shape: (2418, 1040)\n",
      "F shape: (2418,)\n",
      "T shape: (2418,)\n",
      "Hdc shape: (2418,)\n",
      "N shape: (2418,)\n",
      "Duty Pos shape: (2418,)\n",
      "Duty Neg shape: (2418,)\n",
      "Pcv shape: (2418,)\n",
      "0.F, 1.T, 2.Hdc, 3.N, 4.Pcv\n",
      "\"CH467160\": [\n",
      "    [2.0, 1.0],\n",
      "    [25.0, 1.0],\n",
      "    [1245.2510986328125, 698.5667114257812],\n",
      "    [14.20181941986084, 4.4452033042907715],\n",
      "    [1.6664491891860962, 0.7449064254760742],\n",
      "]\n",
      "✅ Normalization parameters saved to results\\20250807_uesed_for_PFC_test4_CH467160_optuna_parm_test1\\norm_params.json\n",
      "=== Start Train  ===\n",
      "\n",
      "\n",
      "\n",
      "                                                    ⠀⠀⠀⠀⢀⡤⠖⠋⠉⠉⠉⠉⠙⠲⣦⣀⠀⠀⠀⠀⠀\n",
      "                                                    ⠀⠀⠀⡴⠋⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠻⣦⡀⠀⠀⠀\n",
      "                                                    ⠀⠀⡼⢁⡠⢼⠁⠀⢱⢄⣀⠀⠀⠀⠀⠀⠎⢿⡄⠀⠀\n",
      "                                                    ⠀⣸⠁⠀⣧⣼⠀⠀⣧⣼⠉⠀⠀⠀⠀⠀⠐⢬⣷⠀⠀\n",
      "                                                    ⡼⣿⢀⠀⣿⡟⠀⠀⣿⣿⠀⠀⠀⠀⠀⠀⠀⠀⢹⣧⠀\n",
      "                  我好想畢業                         ⣇⢹⠀⠁⠈⠀⠉⠃⠈⠃⠀⠀⠀⠀⠀⠀⠀⠀⡰⢸⡇\n",
      "                                                    ⠙⢿⣧⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⣏⣈⣉⣤⠿⠁\n",
      "                                                    ⠀⣠⣾⣿⠤⡀⠀⠀⠀⠀⠀⢀⣤⣶⣿⣿⣿⣿⣅⠀⠀\n",
      "                                                    ⢰⣧⣿⣿⣿⣦⣉⡐⠒⠒⢲⣿⣿⣿⣿⣿⣿⣶⣿⣧⠀\n",
      "                                                    ⠘⠿⢿⣿⣿⣿⡿⠿⠛⠿⠿⠿⣿⣿⣿⣿⣿⣿⡿⠟⠀\n",
      "                                                    ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠉⠉⠁⠀⠀⠀⠀\n",
      "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⣾⣷⣄⠀⠀⠀⣀⣤⣤⣤⡀⠀⠀⠀⠀⠀⠀⠀\n",
      "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣶⠏⠀⠀⣿⠀⢀⡾⠛⠋⠀⣾⣿⠀⠀⠀⠀⠀⠀⠀\n",
      "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡀⡏⠀⠀⠀⣿⢀⣾⠁⠀⣰⠆⢹⡿⠀⠀⠀⠀⠀⠀⠀\n",
      "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠃⣧⠀⠀⢠⡟⢸⡇⠀⣰⠟⠀⣼⠃⠀⠀⠀⠀⠀⠀⠀\n",
      "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣀⣹⣆⢀⣸⣇⣸⠃⢠⡏⠀⣸⠋⠀⠀⠀⠀⠀⠀⠀⠀\n",
      "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⣀⣤⣴⣶⣶⣶⠾⠟⠛⠉⠉⠉⠈⠉⠉⠛⠁⢾⠁⣴⠇⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
      "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣀⣤⣤⣶⣶⠾⠟⠛⠛⣻⣿⣙⡁⠀⠀⢾⣶⣾⣷⣿⣶⣄⠀⠀⠀⠀⠰⢿⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
      "⠀⠀⠀⠀⠀⠀⢀⣀⣀⣀⣠⣴⣶⣶⠾⠟⠛⠉⠉⠉⠀⠀⠀⠀⠀⣿⣻⣟⣻⣿⡦⠀⠘⣿⣿⣛⡿⢶⡇⠀⠀⠀⠀⠀⠀⢻⣆⠀⠀⠀⠀⠀⠀⠀⠀\n",
      "⣠⣶⣶⣶⣾⣿⣿⣿⣿⣿⣿⣿⣧⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⡟⠙⣿⣿⡗⠀⠀⠿⠉⣿⣿⣿⣶⠀⠀⠀⠀⠀⠀⠈⢿⠀⠀⠀⠀⠀⠀⠀⠀\n",
      "⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⠳⣄⣿⡿⠁⠀⠀⠘⢦⣿⣿⠇⠟⠁⠀⠀⠀⠀⠀⠀⣸⡇⠀⠀⠀⠀⠀⠀⠀\n",
      "⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠁⡇⠀⠀⠀⠀⠀⠀⠀\n",
      "⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢰⣇⠀⠀⠀⠀⠀⠀⠀\n",
      "⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⣿⠀⠀⠀⠀⠀⠀⠀\n",
      "⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡏⠀⠀⠀⠀⠀⠀⠀\n",
      "⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀\n",
      "⢻⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡟⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣇⡇⠀⠀⠀⠀⠀⠀⠀\n",
      "⠀⠻⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢤⣤⡀⠀⠀⠀⠀⠀⠀⠀⣿⡇⠀⠀⠀⠀⠀⠀⠀\n",
      "⠀⠀⠈⠙⢿⣿⣿⣿⣿⣿⣿⠟⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣀⣤⡾⠟⠛⠆⠀⠀⠀⠀⠀⢀⢻⡇⠀⠀⠀⠀⠀⠀⠀\n",
      "⠀⠀⠀⠀⠀⠈⠙⠿⣿⣭⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⣤⣴⣶⠾⠟⠋⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⣾⠇⠀⠀⠀⠀⠀⠀⠀\n",
      "⠀⠀⠀⠀⠀⠀⠀⠀⠈⠉⠙⠛⠷⠶⢶⣶⣦⣤⣴⡆⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣠⣌⣿⠀⠀⠀⠀⠀⠀⠀⠀\n",
      "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⣿⡄⠀⠀⠀⠀⠀⠀⠀⠀⠙⠛⠛⠛⠃⠀⠀⠀⠀⠀⠀⠀⣤⣴⣾⣿⣿⣿⣓⠀⠀⠀⠀⠀⠀⠀\n",
      "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣼⣿⣷⣦⣄⣀⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⣀⣠⣤⣶⣾⣟⣯⣽⠟⠋⠀⠉⠳⣄⠀⠀⠀⠀⠀\n",
      "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣾⢇⠀⠉⠛⠷⣮⣍⣩⡍⢻⡟⠉⣉⢹⡏⠉⣿⣹⣷⣦⣿⠿⠟⠉⠀⠀⠀⠀⠀⠀⠙⣆⠀⠀⠀⠀\n",
      "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣰⠏⢸⠇⠀⠀⠀⠀⠀⠉⠉⠛⠛⠛⠛⠛⠛⠛⠋⠉⠉⠀⠀⠀⠀⠀⢠⣠⡶⠀⠀⠀⠀⠘⣧⠀⠀⠀\n",
      "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢰⡿⠀⣸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠟⠁⠀⠀⠀⠀⠀⠘⣆⠀⠀\n",
      "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣾⠃⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣾⣇⡀⠀⠀⠀⠀⠀⠀⢹⡆⠀\n",
      "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⣾⠀⣾⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⢿⣥⢠⣤⠼⠇⠀⠀⠘⣿⡄\n",
      "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⣽⡄⠈⢿⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⣿⠿⠾⠷⠄⠀⠀⠀⢀⣿⠁\n",
      "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⣧⠀⠸⣷⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣤⣾⠋⠀⠀⠀⠀⠀⠀⢰⣾⡿⠀\n",
      "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠙⣦⣠⣿⣿⣶⣶⣤⣤⣄⣀⣀⣀⣀⠀⠀⠀⠀⠀⠀⠀⣀⣀⣠⣴⣿⣇⠀⠀⠀⠀⠀⠀⠀⣸⡟⠀⠀\n",
      "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⢻⣿⠀⠉⠛⢿⣿⣯⣿⡟⢿⠻⣿⢻⣿⢿⣿⣿⣿⣿⣿⠿⠟⠹⣟⢷⣄⠀⠀⠀⢀⣼⠟⠀⠀⠀\n",
      "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⣿⣄⠀⠀⠘⢷⣌⡻⠿⣿⣛⣿⣟⣛⣛⣋⣉⣉⣉⣀⡀⠀⠀⠈⠻⢿⣷⣶⣶⢛⣧⠀⠀⠀⠀\n",
      "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⣏⠀⠀⠀⠀⠹⢯⣟⣛⢿⣿⣽⣅⣀⡀⠀⣀⡀⠀⠀⠀⠠⢦⣀⠰⡦⠀⢸⠀⣏⠀⠀⠀⠀\n",
      "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢿⡀⠀⠀⠀⠀⠀⠀⠈⠉⢻⣿⡟⠛⠉⠉⠁⠀⠀⠀⠀⠀⠀⠈⠛⠷⠀⣸⠀⣿⡀⠀⠀⠀\n",
      "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⣧⠀⠀⠀⠀⠀⠀⠀⠀⠘⣿⣇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⠀⣿⡇⠀⠀⠀\n",
      "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠸⢿⠀⠀⢦⡀⡀⠀⠀⠀⠀⢹⣿⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⡄⡏⠀⠀⠀⠀\n",
      "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⡄⠀⠈⠳⣝⠦⢄⠀⠀⠀⣟⣷⠀⠀⠀⣷⣄⠀⠀⠀⠀⠀⠀⠀⠀⣿⡇⡇⠀⠀⠀⠀\n",
      "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣄⣷⡀⠀⠀⠈⠙⠂⠀⠀⠀⢸⣿⡄⠀⠀⠘⢦⡙⢦⡀⠀⠀⠀⠀⢰⣷⣷⡇⠀⠀⠀⠀\n",
      "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢻⡿⢧⣤⣀⡀⠀⠀⠀⠀⠀⠀⢿⣷⣄⠀⠀⠀⠁⠋⠀⠀⠀⠀⠀⢸⣿⣿⣇⠀⠀⠀⠀\n",
      "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⣷⡀⠈⠉⠛⠛⠛⠛⠛⠛⠛⠛⢿⡍⠛⠳⠶⣶⣤⣤⣤⣤⣤⣤⠼⠟⡟⢿⡇⠀⠀⠀\n",
      "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⣷⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⣧⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠰⣾⡇⠀⠀⠀\n",
      "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣠⣤⣴⣿⣷⣶⣶⣶⣶⣶⣶⣦⣀⣀⣀⣻⡀⠀⠀⠀⣀⣀⠀⡀⠀⠀⠀⢀⣼⣿⠇⠀⠀⠀\n",
      "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣴⠟⠉⠁⠀⠀⠈⠻⣿⡆⢹⣯⣽⣿⣿⠟⠋⠙⣿⣶⣿⣿⣿⣿⣾⣿⣿⣿⣟⠋⠉⣇⠀⠀⠀⠀\n",
      "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⡇⠀⠀⠀⡀⠀⠀⠀⠈⢻⣆⣿⠀⠀⠀⢁⣶⣿⠿⠟⠛⠷⣶⣽⣿⣿⣻⣏⠙⠃⣴⢻⠀⠀⠀⠀\n",
      "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠸⣷⣀⠀⠀⠉⠀⠀⠀⠀⠀⢹⣿⠀⣀⣴⣿⠋⠀⠀⠀⠀⠀⠀⠉⠻⣿⣧⣿⢀⣰⣿⣿⠀⠀⠀⠀\n",
      "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⢿⣶⣶⣤⣤⣤⣤⣤⣤⣾⣿⣟⣿⣿⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⣿⣅⣾⢿⣵⠇⠀⠀⠀⠀\n",
      "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠉⠛⠛⠛⠛⠛⠛⠛⠛⠉⠉⠉⠁⢹⣜⠷⠦⠤⠤⠤⠤⠤⠴⠶⠛⣉⣱⠿⠁⠀⠀⠀⠀⠀\n",
      "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠛⠿⠷⣦⣤⣤⣄⣠⣤⣤⡶⠟⠁⠀⠀⠀⠀⠀⠀⠀\n",
      "\n",
      "\n",
      "                    我是Chill guy，這是我的畢業碩論，我做不完，但沒關西，大不了休學\n",
      "\n",
      "    \n",
      "Number of parameters:  13431\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.009361 ---\n",
      "Epoch 1 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.001\n",
      "Epoch 1, loss_H: 0.009361, loss_Pcv: 37.554304\n",
      "Epoch 1 | Train Loss: 0.125011 | Val Loss: 0.034391 | Time: 24.53s\n",
      "(Best Epoch 0 | best H: inf| best Pcv: inf| val_loss : 0.034391)\n",
      "✅ Save best H @ epoch 1\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.006856 ---\n",
      "Epoch 2 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.001\n",
      "Epoch 2, loss_H: 0.006856, loss_Pcv: 0.515330\n",
      "Epoch 2 | Train Loss: 0.016382 | Val Loss: 0.007534 | Time: 24.48s\n",
      "(Best Epoch 1 | best H: 0.009361| best Pcv: 37.554304| val_loss : 0.007534)\n",
      "✅ Save best H @ epoch 2\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.005709 ---\n",
      "Epoch 3 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.002\n",
      "Epoch 3, loss_H: 0.005709, loss_Pcv: 0.633100\n",
      "Epoch 3 | Train Loss: 0.007720 | Val Loss: 0.006964 | Time: 24.37s\n",
      "(Best Epoch 2 | best H: 0.006856| best Pcv: 0.515330| val_loss : 0.006964)\n",
      "✅ Save best H @ epoch 3\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.004239 ---\n",
      "Epoch 4 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.003\n",
      "Epoch 4, loss_H: 0.004239, loss_Pcv: 0.479845\n",
      "Epoch 4 | Train Loss: 0.006350 | Val Loss: 0.005508 | Time: 24.38s\n",
      "(Best Epoch 3 | best H: 0.005709| best Pcv: 0.633100| val_loss : 0.005508)\n",
      "✅ Save best H @ epoch 4\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.003543 ---\n",
      "Epoch 5 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.003\n",
      "Epoch 5, loss_H: 0.003543, loss_Pcv: 0.306660\n",
      "Epoch 5 | Train Loss: 0.005558 | Val Loss: 0.004554 | Time: 24.29s\n",
      "(Best Epoch 4 | best H: 0.004239| best Pcv: 0.479845| val_loss : 0.004554)\n",
      "✅ Save best H @ epoch 5\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.003806 ---\n",
      "Epoch 6 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.004\n",
      "Epoch 6, loss_H: 0.003806, loss_Pcv: 0.205842\n",
      "Epoch 6 | Train Loss: 0.004376 | Val Loss: 0.004614 | Time: 24.09s\n",
      "(Best Epoch 5 | best H: 0.003543| best Pcv: 0.306660| val_loss : 0.004614)\n",
      "  H 無改善 wait_H=1/150\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.010644 ---\n",
      "Epoch 7 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.005\n",
      "Epoch 7, loss_H: 0.010644, loss_Pcv: 0.180694\n",
      "Epoch 7 | Train Loss: 0.007849 | Val Loss: 0.011438 | Time: 24.41s\n",
      "(Best Epoch 5 | best H: 0.003543| best Pcv: 0.306660| val_loss : 0.011438)\n",
      "  H 無改善 wait_H=2/150\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.001820 ---\n",
      "Epoch 8 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.005\n",
      "Epoch 8, loss_H: 0.001820, loss_Pcv: 0.142111\n",
      "Epoch 8 | Train Loss: 0.004806 | Val Loss: 0.002568 | Time: 23.43s\n",
      "(Best Epoch 5 | best H: 0.003543| best Pcv: 0.306660| val_loss : 0.002568)\n",
      "✅ Save best H @ epoch 8\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.002233 ---\n",
      "Epoch 9 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.006\n",
      "Epoch 9, loss_H: 0.002233, loss_Pcv: 0.472126\n",
      "Epoch 9 | Train Loss: 0.007494 | Val Loss: 0.005052 | Time: 23.86s\n",
      "(Best Epoch 8 | best H: 0.001820| best Pcv: 0.142111| val_loss : 0.005052)\n",
      "  H 無改善 wait_H=1/150\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.001542 ---\n",
      "Epoch 10 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.007\n",
      "Epoch 10, loss_H: 0.001542, loss_Pcv: 0.038830\n",
      "Epoch 10 | Train Loss: 0.004802 | Val Loss: 0.001790 | Time: 23.84s\n",
      "(Best Epoch 8 | best H: 0.001820| best Pcv: 0.142111| val_loss : 0.001790)\n",
      "✅ Save best H @ epoch 10\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.000451 ---\n",
      "Epoch 11 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.007\n",
      "Epoch 11, loss_H: 0.000451, loss_Pcv: 0.037805\n",
      "Epoch 11 | Train Loss: 0.001081 | Val Loss: 0.000725 | Time: 24.39s\n",
      "(Best Epoch 10 | best H: 0.001542| best Pcv: 0.038830| val_loss : 0.000725)\n",
      "✅ Save best H @ epoch 11\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.000363 ---\n",
      "Epoch 12 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.008\n",
      "Epoch 12, loss_H: 0.000363, loss_Pcv: 0.032105\n",
      "Epoch 12 | Train Loss: 0.000711 | Val Loss: 0.000617 | Time: 24.60s\n",
      "(Best Epoch 11 | best H: 0.000451| best Pcv: 0.037805| val_loss : 0.000617)\n",
      "✅ Save best H @ epoch 12\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.000361 ---\n",
      "Epoch 13 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.009\n",
      "Epoch 13, loss_H: 0.000361, loss_Pcv: 0.019846\n",
      "Epoch 13 | Train Loss: 0.000607 | Val Loss: 0.000530 | Time: 23.38s\n",
      "(Best Epoch 12 | best H: 0.000363| best Pcv: 0.032105| val_loss : 0.000530)\n",
      "✅ Save best H @ epoch 13\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.000295 ---\n",
      "Epoch 14 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.009\n",
      "Epoch 14, loss_H: 0.000295, loss_Pcv: 0.016917\n",
      "Epoch 14 | Train Loss: 0.000556 | Val Loss: 0.000450 | Time: 23.78s\n",
      "(Best Epoch 13 | best H: 0.000361| best Pcv: 0.019846| val_loss : 0.000450)\n",
      "✅ Save best H @ epoch 14\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.000289 ---\n",
      "Epoch 15 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.010\n",
      "Epoch 15, loss_H: 0.000289, loss_Pcv: 0.040490\n",
      "Epoch 15 | Train Loss: 0.000517 | Val Loss: 0.000691 | Time: 24.37s\n",
      "(Best Epoch 14 | best H: 0.000295| best Pcv: 0.016917| val_loss : 0.000691)\n",
      "✅ Save best H @ epoch 15\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.000310 ---\n",
      "Epoch 16 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.011\n",
      "Epoch 16, loss_H: 0.000310, loss_Pcv: 0.013007\n",
      "Epoch 16 | Train Loss: 0.000729 | Val Loss: 0.000445 | Time: 24.19s\n",
      "(Best Epoch 15 | best H: 0.000289| best Pcv: 0.040490| val_loss : 0.000445)\n",
      "  H 無改善 wait_H=1/150\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.000254 ---\n",
      "Epoch 17 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.011\n",
      "Epoch 17, loss_H: 0.000254, loss_Pcv: 0.011858\n",
      "Epoch 17 | Train Loss: 0.000515 | Val Loss: 0.000386 | Time: 23.90s\n",
      "(Best Epoch 15 | best H: 0.000289| best Pcv: 0.040490| val_loss : 0.000386)\n",
      "✅ Save best H @ epoch 17\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.000217 ---\n",
      "Epoch 18 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.012\n",
      "Epoch 18, loss_H: 0.000217, loss_Pcv: 0.012375\n",
      "Epoch 18 | Train Loss: 0.000457 | Val Loss: 0.000363 | Time: 24.02s\n",
      "(Best Epoch 17 | best H: 0.000254| best Pcv: 0.011858| val_loss : 0.000363)\n",
      "✅ Save best H @ epoch 18\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.001791 ---\n",
      "Epoch 19 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.013\n",
      "Epoch 19, loss_H: 0.001791, loss_Pcv: 0.059083\n",
      "Epoch 19 | Train Loss: 0.002232 | Val Loss: 0.002517 | Time: 24.31s\n",
      "(Best Epoch 18 | best H: 0.000217| best Pcv: 0.012375| val_loss : 0.002517)\n",
      "  H 無改善 wait_H=1/150\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.000258 ---\n",
      "Epoch 20 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.013\n",
      "Epoch 20, loss_H: 0.000258, loss_Pcv: 0.014107\n",
      "Epoch 20 | Train Loss: 0.001258 | Val Loss: 0.000442 | Time: 24.27s\n",
      "(Best Epoch 18 | best H: 0.000217| best Pcv: 0.012375| val_loss : 0.000442)\n",
      "  H 無改善 wait_H=2/150\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.000234 ---\n",
      "Epoch 21 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.014\n",
      "Epoch 21, loss_H: 0.000234, loss_Pcv: 0.017256\n",
      "Epoch 21 | Train Loss: 0.000449 | Val Loss: 0.000472 | Time: 24.01s\n",
      "(Best Epoch 18 | best H: 0.000217| best Pcv: 0.012375| val_loss : 0.000472)\n",
      "  H 無改善 wait_H=3/150\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.000349 ---\n",
      "Epoch 22 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.015\n",
      "Epoch 22, loss_H: 0.000349, loss_Pcv: 0.012723\n",
      "Epoch 22 | Train Loss: 0.001146 | Val Loss: 0.000531 | Time: 23.95s\n",
      "(Best Epoch 18 | best H: 0.000217| best Pcv: 0.012375| val_loss : 0.000531)\n",
      "  H 無改善 wait_H=4/150\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.000331 ---\n",
      "Epoch 23 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.015\n",
      "Epoch 23, loss_H: 0.000331, loss_Pcv: 0.016139\n",
      "Epoch 23 | Train Loss: 0.000629 | Val Loss: 0.000574 | Time: 25.10s\n",
      "(Best Epoch 18 | best H: 0.000217| best Pcv: 0.012375| val_loss : 0.000574)\n",
      "  H 無改善 wait_H=5/150\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.000368 ---\n",
      "Epoch 24 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.016\n",
      "Epoch 24, loss_H: 0.000368, loss_Pcv: 0.030519\n",
      "Epoch 24 | Train Loss: 0.000522 | Val Loss: 0.000850 | Time: 25.57s\n",
      "(Best Epoch 18 | best H: 0.000217| best Pcv: 0.012375| val_loss : 0.000850)\n",
      "  H 無改善 wait_H=6/150\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.000176 ---\n",
      "Epoch 25 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.017\n",
      "Epoch 25, loss_H: 0.000176, loss_Pcv: 0.009138\n",
      "Epoch 25 | Train Loss: 0.000456 | Val Loss: 0.000326 | Time: 26.06s\n",
      "(Best Epoch 18 | best H: 0.000217| best Pcv: 0.012375| val_loss : 0.000326)\n",
      "✅ Save best H @ epoch 25\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.000180 ---\n",
      "Epoch 26 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.017\n",
      "Epoch 26, loss_H: 0.000180, loss_Pcv: 0.013182\n",
      "Epoch 26 | Train Loss: 0.000387 | Val Loss: 0.000405 | Time: 25.72s\n",
      "(Best Epoch 25 | best H: 0.000176| best Pcv: 0.009138| val_loss : 0.000405)\n",
      "  H 無改善 wait_H=1/150\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.003952 ---\n",
      "Epoch 27 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.018\n",
      "Epoch 27, loss_H: 0.003952, loss_Pcv: 0.432701\n",
      "Epoch 27 | Train Loss: 0.001622 | Val Loss: 0.011669 | Time: 25.50s\n",
      "(Best Epoch 25 | best H: 0.000176| best Pcv: 0.009138| val_loss : 0.011669)\n",
      "  H 無改善 wait_H=2/150\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.000741 ---\n",
      "Epoch 28 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.019\n",
      "Epoch 28, loss_H: 0.000741, loss_Pcv: 0.028975\n",
      "Epoch 28 | Train Loss: 0.003619 | Val Loss: 0.001268 | Time: 25.96s\n",
      "(Best Epoch 25 | best H: 0.000176| best Pcv: 0.009138| val_loss : 0.001268)\n",
      "  H 無改善 wait_H=3/150\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.000288 ---\n",
      "Epoch 29 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.019\n",
      "Epoch 29, loss_H: 0.000288, loss_Pcv: 0.016178\n",
      "Epoch 29 | Train Loss: 0.000698 | Val Loss: 0.000595 | Time: 26.08s\n",
      "(Best Epoch 25 | best H: 0.000176| best Pcv: 0.009138| val_loss : 0.000595)\n",
      "  H 無改善 wait_H=4/150\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.000222 ---\n",
      "Epoch 30 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.020\n",
      "Epoch 30, loss_H: 0.000222, loss_Pcv: 0.009431\n",
      "Epoch 30 | Train Loss: 0.000533 | Val Loss: 0.000406 | Time: 24.31s\n",
      "(Best Epoch 25 | best H: 0.000176| best Pcv: 0.009138| val_loss : 0.000406)\n",
      "  H 無改善 wait_H=5/150\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.000145 ---\n",
      "Epoch 31 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.021\n",
      "Epoch 31, loss_H: 0.000145, loss_Pcv: 0.011385\n",
      "Epoch 31 | Train Loss: 0.000561 | Val Loss: 0.000377 | Time: 24.30s\n",
      "(Best Epoch 25 | best H: 0.000176| best Pcv: 0.009138| val_loss : 0.000377)\n",
      "✅ Save best H @ epoch 31\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.000169 ---\n",
      "Epoch 32 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.021\n",
      "Epoch 32, loss_H: 0.000169, loss_Pcv: 0.008878\n",
      "Epoch 32 | Train Loss: 0.000515 | Val Loss: 0.000355 | Time: 24.45s\n",
      "(Best Epoch 31 | best H: 0.000145| best Pcv: 0.011385| val_loss : 0.000355)\n",
      "  H 無改善 wait_H=1/150\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.000134 ---\n",
      "Epoch 33 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.022\n",
      "Epoch 33, loss_H: 0.000134, loss_Pcv: 0.009551\n",
      "Epoch 33 | Train Loss: 0.000564 | Val Loss: 0.000341 | Time: 24.05s\n",
      "(Best Epoch 31 | best H: 0.000145| best Pcv: 0.011385| val_loss : 0.000341)\n",
      "✅ Save best H @ epoch 33\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.000204 ---\n",
      "Epoch 34 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.023\n",
      "Epoch 34, loss_H: 0.000204, loss_Pcv: 0.019863\n",
      "Epoch 34 | Train Loss: 0.000448 | Val Loss: 0.000649 | Time: 24.00s\n",
      "(Best Epoch 33 | best H: 0.000134| best Pcv: 0.009551| val_loss : 0.000649)\n",
      "  H 無改善 wait_H=1/150\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.000773 ---\n",
      "Epoch 35 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.023\n",
      "Epoch 35, loss_H: 0.000773, loss_Pcv: 0.009750\n",
      "Epoch 35 | Train Loss: 0.000808 | Val Loss: 0.000982 | Time: 24.00s\n",
      "(Best Epoch 33 | best H: 0.000134| best Pcv: 0.009551| val_loss : 0.000982)\n",
      "  H 無改善 wait_H=2/150\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.009249 ---\n",
      "Epoch 36 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.024\n",
      "Epoch 36, loss_H: 0.009249, loss_Pcv: 0.024551\n",
      "Epoch 36 | Train Loss: 0.042257 | Val Loss: 0.009616 | Time: 24.48s\n",
      "(Best Epoch 33 | best H: 0.000134| best Pcv: 0.009551| val_loss : 0.009616)\n",
      "  H 無改善 wait_H=3/150\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.000566 ---\n",
      "Epoch 37 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.025\n",
      "Epoch 37, loss_H: 0.000566, loss_Pcv: 0.030525\n",
      "Epoch 37 | Train Loss: 0.003844 | Val Loss: 0.001305 | Time: 24.97s\n",
      "(Best Epoch 33 | best H: 0.000134| best Pcv: 0.009551| val_loss : 0.001305)\n",
      "  H 無改善 wait_H=4/150\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.000485 ---\n",
      "Epoch 38 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.025\n",
      "Epoch 38, loss_H: 0.000485, loss_Pcv: 0.013504\n",
      "Epoch 38 | Train Loss: 0.001038 | Val Loss: 0.000815 | Time: 25.52s\n",
      "(Best Epoch 33 | best H: 0.000134| best Pcv: 0.009551| val_loss : 0.000815)\n",
      "  H 無改善 wait_H=5/150\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.000346 ---\n",
      "Epoch 39 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.026\n",
      "Epoch 39, loss_H: 0.000346, loss_Pcv: 0.010441\n",
      "Epoch 39 | Train Loss: 0.000765 | Val Loss: 0.000608 | Time: 25.29s\n",
      "(Best Epoch 33 | best H: 0.000134| best Pcv: 0.009551| val_loss : 0.000608)\n",
      "  H 無改善 wait_H=6/150\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.000224 ---\n",
      "Epoch 40 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.027\n",
      "Epoch 40, loss_H: 0.000224, loss_Pcv: 0.015315\n",
      "Epoch 40 | Train Loss: 0.000668 | Val Loss: 0.000626 | Time: 25.19s\n",
      "(Best Epoch 33 | best H: 0.000134| best Pcv: 0.009551| val_loss : 0.000626)\n",
      "  H 無改善 wait_H=7/150\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.000236 ---\n",
      "Epoch 41 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.027\n",
      "Epoch 41, loss_H: 0.000236, loss_Pcv: 0.009054\n",
      "Epoch 41 | Train Loss: 0.000778 | Val Loss: 0.000477 | Time: 25.21s\n",
      "(Best Epoch 33 | best H: 0.000134| best Pcv: 0.009551| val_loss : 0.000477)\n",
      "  H 無改善 wait_H=8/150\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.000215 ---\n",
      "Epoch 42 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.028\n",
      "Epoch 42, loss_H: 0.000215, loss_Pcv: 0.009785\n",
      "Epoch 42 | Train Loss: 0.000555 | Val Loss: 0.000483 | Time: 25.28s\n",
      "(Best Epoch 33 | best H: 0.000134| best Pcv: 0.009551| val_loss : 0.000483)\n",
      "  H 無改善 wait_H=9/150\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.000180 ---\n",
      "Epoch 43 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.029\n",
      "Epoch 43, loss_H: 0.000180, loss_Pcv: 0.008467\n",
      "Epoch 43 | Train Loss: 0.000608 | Val Loss: 0.000418 | Time: 25.99s\n",
      "(Best Epoch 33 | best H: 0.000134| best Pcv: 0.009551| val_loss : 0.000418)\n",
      "  H 無改善 wait_H=10/150\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.000659 ---\n",
      "Epoch 44 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.029\n",
      "Epoch 44, loss_H: 0.000659, loss_Pcv: 0.032298\n",
      "Epoch 44 | Train Loss: 0.000973 | Val Loss: 0.001587 | Time: 25.28s\n",
      "(Best Epoch 33 | best H: 0.000134| best Pcv: 0.009551| val_loss : 0.001587)\n",
      "  H 無改善 wait_H=11/150\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.000389 ---\n",
      "Epoch 45 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.030\n",
      "Epoch 45, loss_H: 0.000389, loss_Pcv: 0.009768\n",
      "Epoch 45 | Train Loss: 0.001027 | Val Loss: 0.000671 | Time: 25.81s\n",
      "(Best Epoch 33 | best H: 0.000134| best Pcv: 0.009551| val_loss : 0.000671)\n",
      "  H 無改善 wait_H=12/150\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.000203 ---\n",
      "Epoch 46 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.031\n",
      "Epoch 46, loss_H: 0.000203, loss_Pcv: 0.009468\n",
      "Epoch 46 | Train Loss: 0.000591 | Val Loss: 0.000487 | Time: 25.26s\n",
      "(Best Epoch 33 | best H: 0.000134| best Pcv: 0.009551| val_loss : 0.000487)\n",
      "  H 無改善 wait_H=13/150\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.000194 ---\n",
      "Epoch 47 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.031\n",
      "Epoch 47, loss_H: 0.000194, loss_Pcv: 0.011157\n",
      "Epoch 47 | Train Loss: 0.000526 | Val Loss: 0.000537 | Time: 25.40s\n",
      "(Best Epoch 33 | best H: 0.000134| best Pcv: 0.009551| val_loss : 0.000537)\n",
      "  H 無改善 wait_H=14/150\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.000163 ---\n",
      "Epoch 48 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.032\n",
      "Epoch 48, loss_H: 0.000163, loss_Pcv: 0.008177\n",
      "Epoch 48 | Train Loss: 0.000667 | Val Loss: 0.000420 | Time: 25.28s\n",
      "(Best Epoch 33 | best H: 0.000134| best Pcv: 0.009551| val_loss : 0.000420)\n",
      "  H 無改善 wait_H=15/150\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.000201 ---\n",
      "Epoch 49 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.033\n",
      "Epoch 49, loss_H: 0.000201, loss_Pcv: 0.008273\n",
      "Epoch 49 | Train Loss: 0.000661 | Val Loss: 0.000465 | Time: 25.65s\n",
      "(Best Epoch 33 | best H: 0.000134| best Pcv: 0.009551| val_loss : 0.000465)\n",
      "  H 無改善 wait_H=16/150\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.000170 ---\n",
      "Epoch 50 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.033\n",
      "Epoch 50, loss_H: 0.000170, loss_Pcv: 0.009890\n",
      "Epoch 50 | Train Loss: 0.000636 | Val Loss: 0.000494 | Time: 25.45s\n",
      "(Best Epoch 33 | best H: 0.000134| best Pcv: 0.009551| val_loss : 0.000494)\n",
      "  H 無改善 wait_H=17/150\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.000206 ---\n",
      "Epoch 51 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.034\n",
      "Epoch 51, loss_H: 0.000206, loss_Pcv: 0.014198\n",
      "Epoch 51 | Train Loss: 0.000602 | Val Loss: 0.000682 | Time: 25.40s\n",
      "(Best Epoch 33 | best H: 0.000134| best Pcv: 0.009551| val_loss : 0.000682)\n",
      "  H 無改善 wait_H=18/150\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.000277 ---\n",
      "Epoch 52 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.035\n",
      "Epoch 52, loss_H: 0.000277, loss_Pcv: 0.009435\n",
      "Epoch 52 | Train Loss: 0.000690 | Val Loss: 0.000595 | Time: 25.30s\n",
      "(Best Epoch 33 | best H: 0.000134| best Pcv: 0.009551| val_loss : 0.000595)\n",
      "  H 無改善 wait_H=19/150\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.000456 ---\n",
      "Epoch 53 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.035\n",
      "Epoch 53, loss_H: 0.000456, loss_Pcv: 0.009220\n",
      "Epoch 53 | Train Loss: 0.001602 | Val Loss: 0.000766 | Time: 24.85s\n",
      "(Best Epoch 33 | best H: 0.000134| best Pcv: 0.009551| val_loss : 0.000766)\n",
      "  H 無改善 wait_H=20/150\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.000190 ---\n",
      "Epoch 54 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.036\n",
      "Epoch 54, loss_H: 0.000190, loss_Pcv: 0.007796\n",
      "Epoch 54 | Train Loss: 0.000670 | Val Loss: 0.000464 | Time: 24.52s\n",
      "(Best Epoch 33 | best H: 0.000134| best Pcv: 0.009551| val_loss : 0.000464)\n",
      "  H 無改善 wait_H=21/150\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.000122 ---\n",
      "Epoch 55 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.037\n",
      "Epoch 55, loss_H: 0.000122, loss_Pcv: 0.009989\n",
      "Epoch 55 | Train Loss: 0.000579 | Val Loss: 0.000484 | Time: 25.71s\n",
      "(Best Epoch 33 | best H: 0.000134| best Pcv: 0.009551| val_loss : 0.000484)\n",
      "✅ Save best H @ epoch 55\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.000274 ---\n",
      "Epoch 56 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.037\n",
      "Epoch 56, loss_H: 0.000274, loss_Pcv: 0.009171\n",
      "Epoch 56 | Train Loss: 0.000714 | Val Loss: 0.000606 | Time: 25.45s\n",
      "(Best Epoch 55 | best H: 0.000122| best Pcv: 0.009989| val_loss : 0.000606)\n",
      "  H 無改善 wait_H=1/150\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.000134 ---\n",
      "Epoch 57 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.038\n",
      "Epoch 57, loss_H: 0.000134, loss_Pcv: 0.008784\n",
      "Epoch 57 | Train Loss: 0.000626 | Val Loss: 0.000463 | Time: 25.43s\n",
      "(Best Epoch 55 | best H: 0.000122| best Pcv: 0.009989| val_loss : 0.000463)\n",
      "  H 無改善 wait_H=2/150\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.000183 ---\n",
      "Epoch 58 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.039\n",
      "Epoch 58, loss_H: 0.000183, loss_Pcv: 0.015500\n",
      "Epoch 58 | Train Loss: 0.000616 | Val Loss: 0.000775 | Time: 25.29s\n",
      "(Best Epoch 55 | best H: 0.000122| best Pcv: 0.009989| val_loss : 0.000775)\n",
      "  H 無改善 wait_H=3/150\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.000195 ---\n",
      "Epoch 59 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.039\n",
      "Epoch 59, loss_H: 0.000195, loss_Pcv: 0.010727\n",
      "Epoch 59 | Train Loss: 0.000712 | Val Loss: 0.000610 | Time: 25.22s\n",
      "(Best Epoch 55 | best H: 0.000122| best Pcv: 0.009989| val_loss : 0.000610)\n",
      "  H 無改善 wait_H=4/150\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.000119 ---\n",
      "Epoch 60 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.040\n",
      "Epoch 60, loss_H: 0.000119, loss_Pcv: 0.009825\n",
      "Epoch 60 | Train Loss: 0.000662 | Val Loss: 0.000508 | Time: 24.72s\n",
      "(Best Epoch 55 | best H: 0.000122| best Pcv: 0.009989| val_loss : 0.000508)\n",
      "✅ Save best H @ epoch 60\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.000470 ---\n",
      "Epoch 61 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.041\n",
      "Epoch 61, loss_H: 0.000470, loss_Pcv: 0.012113\n",
      "Epoch 61 | Train Loss: 0.001627 | Val Loss: 0.000944 | Time: 24.27s\n",
      "(Best Epoch 60 | best H: 0.000119| best Pcv: 0.009825| val_loss : 0.000944)\n",
      "  H 無改善 wait_H=1/150\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.000340 ---\n",
      "Epoch 62 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.041\n",
      "Epoch 62, loss_H: 0.000340, loss_Pcv: 0.007909\n",
      "Epoch 62 | Train Loss: 0.001209 | Val Loss: 0.000653 | Time: 23.99s\n",
      "(Best Epoch 60 | best H: 0.000119| best Pcv: 0.009825| val_loss : 0.000653)\n",
      "  H 無改善 wait_H=2/150\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.000227 ---\n",
      "Epoch 63 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.042\n",
      "Epoch 63, loss_H: 0.000227, loss_Pcv: 0.008322\n",
      "Epoch 63 | Train Loss: 0.000855 | Val Loss: 0.000567 | Time: 23.87s\n",
      "(Best Epoch 60 | best H: 0.000119| best Pcv: 0.009825| val_loss : 0.000567)\n",
      "  H 無改善 wait_H=3/150\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.000263 ---\n",
      "Epoch 64 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.043\n",
      "Epoch 64, loss_H: 0.000263, loss_Pcv: 0.007620\n",
      "Epoch 64 | Train Loss: 0.000715 | Val Loss: 0.000577 | Time: 24.12s\n",
      "(Best Epoch 60 | best H: 0.000119| best Pcv: 0.009825| val_loss : 0.000577)\n",
      "  H 無改善 wait_H=4/150\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.000520 ---\n",
      "Epoch 65 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.043\n",
      "Epoch 65, loss_H: 0.000520, loss_Pcv: 0.011264\n",
      "Epoch 65 | Train Loss: 0.001079 | Val Loss: 0.000985 | Time: 23.91s\n",
      "(Best Epoch 60 | best H: 0.000119| best Pcv: 0.009825| val_loss : 0.000985)\n",
      "  H 無改善 wait_H=5/150\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.000205 ---\n",
      "Epoch 66 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.044\n",
      "Epoch 66, loss_H: 0.000205, loss_Pcv: 0.007827\n",
      "Epoch 66 | Train Loss: 0.001007 | Val Loss: 0.000540 | Time: 24.09s\n",
      "(Best Epoch 60 | best H: 0.000119| best Pcv: 0.009825| val_loss : 0.000540)\n",
      "  H 無改善 wait_H=6/150\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.000205 ---\n",
      "Epoch 67 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.045\n",
      "Epoch 67, loss_H: 0.000205, loss_Pcv: 0.009227\n",
      "Epoch 67 | Train Loss: 0.000780 | Val Loss: 0.000608 | Time: 24.64s\n",
      "(Best Epoch 60 | best H: 0.000119| best Pcv: 0.009825| val_loss : 0.000608)\n",
      "  H 無改善 wait_H=7/150\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.000176 ---\n",
      "Epoch 68 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.045\n",
      "Epoch 68, loss_H: 0.000176, loss_Pcv: 0.008985\n",
      "Epoch 68 | Train Loss: 0.000826 | Val Loss: 0.000575 | Time: 26.06s\n",
      "(Best Epoch 60 | best H: 0.000119| best Pcv: 0.009825| val_loss : 0.000575)\n",
      "  H 無改善 wait_H=8/150\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.000320 ---\n",
      "Epoch 69 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.046\n",
      "Epoch 69, loss_H: 0.000320, loss_Pcv: 0.010100\n",
      "Epoch 69 | Train Loss: 0.000929 | Val Loss: 0.000770 | Time: 24.74s\n",
      "(Best Epoch 60 | best H: 0.000119| best Pcv: 0.009825| val_loss : 0.000770)\n",
      "  H 無改善 wait_H=9/150\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.000200 ---\n",
      "Epoch 70 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.047\n",
      "Epoch 70, loss_H: 0.000200, loss_Pcv: 0.009431\n",
      "Epoch 70 | Train Loss: 0.001089 | Val Loss: 0.000631 | Time: 24.67s\n",
      "(Best Epoch 60 | best H: 0.000119| best Pcv: 0.009825| val_loss : 0.000631)\n",
      "  H 無改善 wait_H=10/150\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.000150 ---\n",
      "Epoch 71 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.047\n",
      "Epoch 71, loss_H: 0.000150, loss_Pcv: 0.020219\n",
      "Epoch 71 | Train Loss: 0.000786 | Val Loss: 0.001100 | Time: 24.54s\n",
      "(Best Epoch 60 | best H: 0.000119| best Pcv: 0.009825| val_loss : 0.001100)\n",
      "  H 無改善 wait_H=11/150\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.000215 ---\n",
      "Epoch 72 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.048\n",
      "Epoch 72, loss_H: 0.000215, loss_Pcv: 0.008862\n",
      "Epoch 72 | Train Loss: 0.001413 | Val Loss: 0.000630 | Time: 24.81s\n",
      "(Best Epoch 60 | best H: 0.000119| best Pcv: 0.009825| val_loss : 0.000630)\n",
      "  H 無改善 wait_H=12/150\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.000575 ---\n",
      "Epoch 73 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.049\n",
      "Epoch 73, loss_H: 0.000575, loss_Pcv: 0.012418\n",
      "Epoch 73 | Train Loss: 0.000940 | Val Loss: 0.001152 | Time: 24.69s\n",
      "(Best Epoch 60 | best H: 0.000119| best Pcv: 0.009825| val_loss : 0.001152)\n",
      "  H 無改善 wait_H=13/150\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.000379 ---\n",
      "Epoch 74 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.049\n",
      "Epoch 74, loss_H: 0.000379, loss_Pcv: 0.008863\n",
      "Epoch 74 | Train Loss: 0.001188 | Val Loss: 0.000797 | Time: 24.54s\n",
      "(Best Epoch 60 | best H: 0.000119| best Pcv: 0.009825| val_loss : 0.000797)\n",
      "  H 無改善 wait_H=14/150\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.000255 ---\n",
      "Epoch 75 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.050\n",
      "Epoch 75, loss_H: 0.000255, loss_Pcv: 0.014696\n",
      "Epoch 75 | Train Loss: 0.000936 | Val Loss: 0.000977 | Time: 24.86s\n",
      "(Best Epoch 60 | best H: 0.000119| best Pcv: 0.009825| val_loss : 0.000977)\n",
      "  H 無改善 wait_H=15/150\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.000196 ---\n",
      "Epoch 76 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.051\n",
      "Epoch 76, loss_H: 0.000196, loss_Pcv: 0.008664\n",
      "Epoch 76 | Train Loss: 0.000809 | Val Loss: 0.000625 | Time: 24.60s\n",
      "(Best Epoch 60 | best H: 0.000119| best Pcv: 0.009825| val_loss : 0.000625)\n",
      "  H 無改善 wait_H=16/150\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.000238 ---\n",
      "Epoch 77 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.051\n",
      "Epoch 77, loss_H: 0.000238, loss_Pcv: 0.009370\n",
      "Epoch 77 | Train Loss: 0.000875 | Val Loss: 0.000707 | Time: 24.75s\n",
      "(Best Epoch 60 | best H: 0.000119| best Pcv: 0.009825| val_loss : 0.000707)\n",
      "  H 無改善 wait_H=17/150\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.000932 ---\n",
      "Epoch 78 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.052\n",
      "Epoch 78, loss_H: 0.000932, loss_Pcv: 0.017781\n",
      "Epoch 78 | Train Loss: 0.000940 | Val Loss: 0.001808 | Time: 24.73s\n",
      "(Best Epoch 60 | best H: 0.000119| best Pcv: 0.009825| val_loss : 0.001808)\n",
      "  H 無改善 wait_H=18/150\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.001003 ---\n",
      "Epoch 79 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.053\n",
      "Epoch 79, loss_H: 0.001003, loss_Pcv: 1.431328\n",
      "Epoch 79 | Train Loss: 0.001434 | Val Loss: 0.076334 | Time: 24.64s\n",
      "(Best Epoch 60 | best H: 0.000119| best Pcv: 0.009825| val_loss : 0.076334)\n",
      "  H 無改善 wait_H=19/150\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.171755 ---\n",
      "Epoch 80 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.053\n",
      "Epoch 80, loss_H: 0.171755, loss_Pcv: 3.809854\n",
      "Epoch 80 | Train Loss: 2.442997 | Val Loss: 0.365787 | Time: 24.53s\n",
      "(Best Epoch 60 | best H: 0.000119| best Pcv: 0.009825| val_loss : 0.365787)\n",
      "  H 無改善 wait_H=20/150\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.053699 ---\n",
      "Epoch 81 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.054\n",
      "Epoch 81, loss_H: 0.053699, loss_Pcv: 4.423989\n",
      "Epoch 81 | Train Loss: 0.337042 | Val Loss: 0.289695 | Time: 24.69s\n",
      "(Best Epoch 60 | best H: 0.000119| best Pcv: 0.009825| val_loss : 0.289695)\n",
      "  H 無改善 wait_H=21/150\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.036411 ---\n",
      "Epoch 82 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.055\n",
      "Epoch 82, loss_H: 0.036411, loss_Pcv: 0.383800\n",
      "Epoch 82 | Train Loss: 0.106433 | Val Loss: 0.055402 | Time: 25.08s\n",
      "(Best Epoch 60 | best H: 0.000119| best Pcv: 0.009825| val_loss : 0.055402)\n",
      "  H 無改善 wait_H=22/150\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.027887 ---\n",
      "Epoch 83 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.055\n",
      "Epoch 83, loss_H: 0.027887, loss_Pcv: 0.410090\n",
      "Epoch 83 | Train Loss: 0.050962 | Val Loss: 0.049035 | Time: 24.83s\n",
      "(Best Epoch 60 | best H: 0.000119| best Pcv: 0.009825| val_loss : 0.049035)\n",
      "  H 無改善 wait_H=23/150\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.023950 ---\n",
      "Epoch 84 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.056\n",
      "Epoch 84, loss_H: 0.023950, loss_Pcv: 0.286167\n",
      "Epoch 84 | Train Loss: 0.043799 | Val Loss: 0.038635 | Time: 24.52s\n",
      "(Best Epoch 60 | best H: 0.000119| best Pcv: 0.009825| val_loss : 0.038635)\n",
      "  H 無改善 wait_H=24/150\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.023562 ---\n",
      "Epoch 85 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.057\n",
      "Epoch 85, loss_H: 0.023562, loss_Pcv: 0.250848\n",
      "Epoch 85 | Train Loss: 0.037673 | Val Loss: 0.036442 | Time: 24.79s\n",
      "(Best Epoch 60 | best H: 0.000119| best Pcv: 0.009825| val_loss : 0.036442)\n",
      "  H 無改善 wait_H=25/150\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.021147 ---\n",
      "Epoch 86 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.057\n",
      "Epoch 86, loss_H: 0.021147, loss_Pcv: 0.212829\n",
      "Epoch 86 | Train Loss: 0.035667 | Val Loss: 0.032136 | Time: 24.74s\n",
      "(Best Epoch 60 | best H: 0.000119| best Pcv: 0.009825| val_loss : 0.032136)\n",
      "  H 無改善 wait_H=26/150\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.017955 ---\n",
      "Epoch 87 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.058\n",
      "Epoch 87, loss_H: 0.017955, loss_Pcv: 0.194211\n",
      "Epoch 87 | Train Loss: 0.031187 | Val Loss: 0.028178 | Time: 24.80s\n",
      "(Best Epoch 60 | best H: 0.000119| best Pcv: 0.009825| val_loss : 0.028178)\n",
      "  H 無改善 wait_H=27/150\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.015239 ---\n",
      "Epoch 88 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.059\n",
      "Epoch 88, loss_H: 0.015239, loss_Pcv: 0.217189\n",
      "Epoch 88 | Train Loss: 0.027698 | Val Loss: 0.027087 | Time: 24.93s\n",
      "(Best Epoch 60 | best H: 0.000119| best Pcv: 0.009825| val_loss : 0.027087)\n",
      "  H 無改善 wait_H=28/150\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.017020 ---\n",
      "Epoch 89 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.059\n",
      "Epoch 89, loss_H: 0.017020, loss_Pcv: 0.131546\n",
      "Epoch 89 | Train Loss: 0.026734 | Val Loss: 0.023815 | Time: 24.58s\n",
      "(Best Epoch 60 | best H: 0.000119| best Pcv: 0.009825| val_loss : 0.023815)\n",
      "  H 無改善 wait_H=29/150\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.013943 ---\n",
      "Epoch 90 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.060\n",
      "Epoch 90, loss_H: 0.013943, loss_Pcv: 0.099883\n",
      "Epoch 90 | Train Loss: 0.025019 | Val Loss: 0.019099 | Time: 24.83s\n",
      "(Best Epoch 60 | best H: 0.000119| best Pcv: 0.009825| val_loss : 0.019099)\n",
      "  H 無改善 wait_H=30/150\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.011811 ---\n",
      "Epoch 91 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.061\n",
      "Epoch 91, loss_H: 0.011811, loss_Pcv: 0.098438\n",
      "Epoch 91 | Train Loss: 0.018233 | Val Loss: 0.017066 | Time: 24.82s\n",
      "(Best Epoch 60 | best H: 0.000119| best Pcv: 0.009825| val_loss : 0.017066)\n",
      "  H 無改善 wait_H=31/150\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.010285 ---\n",
      "Epoch 92 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.061\n",
      "Epoch 92, loss_H: 0.010285, loss_Pcv: 0.093920\n",
      "Epoch 92 | Train Loss: 0.015734 | Val Loss: 0.015414 | Time: 24.87s\n",
      "(Best Epoch 60 | best H: 0.000119| best Pcv: 0.009825| val_loss : 0.015414)\n",
      "  H 無改善 wait_H=32/150\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.010076 ---\n",
      "Epoch 93 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.062\n",
      "Epoch 93, loss_H: 0.010076, loss_Pcv: 0.083242\n",
      "Epoch 93 | Train Loss: 0.020807 | Val Loss: 0.014612 | Time: 24.58s\n",
      "(Best Epoch 60 | best H: 0.000119| best Pcv: 0.009825| val_loss : 0.014612)\n",
      "  H 無改善 wait_H=33/150\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.009856 ---\n",
      "Epoch 94 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.063\n",
      "Epoch 94, loss_H: 0.009856, loss_Pcv: 0.037138\n",
      "Epoch 94 | Train Loss: 0.013178 | Val Loss: 0.011566 | Time: 24.56s\n",
      "(Best Epoch 60 | best H: 0.000119| best Pcv: 0.009825| val_loss : 0.011566)\n",
      "  H 無改善 wait_H=34/150\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.007671 ---\n",
      "Epoch 95 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.063\n",
      "Epoch 95, loss_H: 0.007671, loss_Pcv: 0.039611\n",
      "Epoch 95 | Train Loss: 0.010834 | Val Loss: 0.009694 | Time: 24.72s\n",
      "(Best Epoch 60 | best H: 0.000119| best Pcv: 0.009825| val_loss : 0.009694)\n",
      "  H 無改善 wait_H=35/150\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.006625 ---\n",
      "Epoch 96 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.064\n",
      "Epoch 96, loss_H: 0.006625, loss_Pcv: 0.036539\n",
      "Epoch 96 | Train Loss: 0.009399 | Val Loss: 0.008539 | Time: 24.84s\n",
      "(Best Epoch 60 | best H: 0.000119| best Pcv: 0.009825| val_loss : 0.008539)\n",
      "  H 無改善 wait_H=36/150\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.005611 ---\n",
      "Epoch 97 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.065\n",
      "Epoch 97, loss_H: 0.005611, loss_Pcv: 0.036160\n",
      "Epoch 97 | Train Loss: 0.008353 | Val Loss: 0.007586 | Time: 25.33s\n",
      "(Best Epoch 60 | best H: 0.000119| best Pcv: 0.009825| val_loss : 0.007586)\n",
      "  H 無改善 wait_H=37/150\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.005233 ---\n",
      "Epoch 98 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.065\n",
      "Epoch 98, loss_H: 0.005233, loss_Pcv: 0.036552\n",
      "Epoch 98 | Train Loss: 0.007467 | Val Loss: 0.007279 | Time: 25.59s\n",
      "(Best Epoch 60 | best H: 0.000119| best Pcv: 0.009825| val_loss : 0.007279)\n",
      "  H 無改善 wait_H=38/150\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.004289 ---\n",
      "Epoch 99 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.066\n",
      "Epoch 99, loss_H: 0.004289, loss_Pcv: 0.030703\n",
      "Epoch 99 | Train Loss: 0.006776 | Val Loss: 0.006033 | Time: 24.50s\n",
      "(Best Epoch 60 | best H: 0.000119| best Pcv: 0.009825| val_loss : 0.006033)\n",
      "  H 無改善 wait_H=39/150\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.003980 ---\n",
      "Epoch 100 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.067\n",
      "Epoch 100, loss_H: 0.003980, loss_Pcv: 0.028434\n",
      "Epoch 100 | Train Loss: 0.006225 | Val Loss: 0.005610 | Time: 23.98s\n",
      "(Best Epoch 60 | best H: 0.000119| best Pcv: 0.009825| val_loss : 0.005610)\n",
      "  H 無改善 wait_H=40/150\n",
      "--- [Phase 1: H-focus] Monitoring val_loss_H: 0.003273 ---\n",
      "Epoch 101 | Learning Rate: 0.010000\n",
      "---\n",
      "alpha: 0.067\n",
      "Epoch 101, loss_H: 0.003273, loss_Pcv: 0.026758\n",
      "Epoch 101 | Train Loss: 0.005577 | Val Loss: 0.004854 | Time: 24.34s\n",
      "(Best Epoch 60 | best H: 0.000119| best Pcv: 0.009825| val_loss : 0.004854)\n",
      "  H 無改善 wait_H=41/150\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_cu128_pre",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
